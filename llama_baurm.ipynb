{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yjgGjxJJwThw",
        "xxJ0uqPsahyr"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMx4x1DZaZeJQhkSFYLekt7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0642edee176147f4a68d622eeb6b64ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5f8ea8631254415b779f013482561d5",
              "IPY_MODEL_1a261823bf934f408fd8fa2d53cbd086",
              "IPY_MODEL_c4cec27bf9bd4f089c275a32fb156b95",
              "IPY_MODEL_d0e744fddcc646869db8f2aaa72eaaa6",
              "IPY_MODEL_8e4357b91d4243c99b9ffd0291224954"
            ],
            "layout": "IPY_MODEL_4ca551f195174d75ada226f2b45a5465"
          }
        },
        "a5f8ea8631254415b779f013482561d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c1909b3c2ff4f4daab1d32fd8259c29",
            "placeholder": "​",
            "style": "IPY_MODEL_5354943d53804704b83195ac5e52a6e4",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "1a261823bf934f408fd8fa2d53cbd086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_879fc3f49a044fe787cc4c2ca881a38a",
            "placeholder": "​",
            "style": "IPY_MODEL_8baff2df59144560b9cb1fd1289c79a9",
            "value": ""
          }
        },
        "c4cec27bf9bd4f089c275a32fb156b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7cf782cb05054a959e0fd445bb1e2fa8",
            "style": "IPY_MODEL_6da19a724e96450e9dba98fcbd2b1caf",
            "value": true
          }
        },
        "d0e744fddcc646869db8f2aaa72eaaa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b74f6fbce74e4d289a74f076eb15424d",
            "style": "IPY_MODEL_20e1eb0ede264360b873bdee25437e1f",
            "tooltip": ""
          }
        },
        "8e4357b91d4243c99b9ffd0291224954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a77d994740224fa180f2f22019b7ae2f",
            "placeholder": "​",
            "style": "IPY_MODEL_29751897f82a42279a0f7a6461e2900a",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "4ca551f195174d75ada226f2b45a5465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7c1909b3c2ff4f4daab1d32fd8259c29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5354943d53804704b83195ac5e52a6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "879fc3f49a044fe787cc4c2ca881a38a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8baff2df59144560b9cb1fd1289c79a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cf782cb05054a959e0fd445bb1e2fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6da19a724e96450e9dba98fcbd2b1caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b74f6fbce74e4d289a74f076eb15424d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e1eb0ede264360b873bdee25437e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a77d994740224fa180f2f22019b7ae2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29751897f82a42279a0f7a6461e2900a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be6c2448796840a3974c8b7051ac1dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_211bf88023414e6da73f75321e228857",
              "IPY_MODEL_a54d4ea4a2594b2eac368edfcf64d057",
              "IPY_MODEL_0eee3953ec3d421ebe9f2dd10a5cf7a0"
            ],
            "layout": "IPY_MODEL_3324b0c3a6594c4d9f80a8ecb2f3f19f"
          }
        },
        "211bf88023414e6da73f75321e228857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3220a36de7c48e28176b2a111bfdfcc",
            "placeholder": "​",
            "style": "IPY_MODEL_4ec9e23df252433c8746d701dc46abd9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a54d4ea4a2594b2eac368edfcf64d057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8976dbdf76ef466aa4ac33f9dd7c80fc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d485d6cc5e84a36bbfd0fc2d1b47e69",
            "value": 2
          }
        },
        "0eee3953ec3d421ebe9f2dd10a5cf7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14b4963f025e4f09b3e6d17d96bb837a",
            "placeholder": "​",
            "style": "IPY_MODEL_83f58c90359a4edfa7ecaff62a4a7672",
            "value": " 2/2 [00:08&lt;00:00,  3.97s/it]"
          }
        },
        "3324b0c3a6594c4d9f80a8ecb2f3f19f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3220a36de7c48e28176b2a111bfdfcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ec9e23df252433c8746d701dc46abd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8976dbdf76ef466aa4ac33f9dd7c80fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d485d6cc5e84a36bbfd0fc2d1b47e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14b4963f025e4f09b3e6d17d96bb837a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f58c90359a4edfa7ecaff62a4a7672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-meltzer/LLMs/blob/main/llama_baurm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependecies"
      ],
      "metadata": {
        "id": "50sCRlr2vbP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OSucbzqphf8",
        "outputId": "3245bf1d-ea46-487a-972e-1d84b4a187c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/SFT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yson3GOrpkhK",
        "outputId": "aa24f1e6-36b5-4650-9a15-cbab4975bf17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/SFT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juVsG32qpq_I",
        "outputId": "5ddf9f76-3e0a-42e3-c8f6-6fc992203c2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34martifacts\u001b[0m/  \u001b[01;34mfinetuned_models\u001b[0m/  llama.ipynb  requirements.txt  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqqU -r requirements.txt\n",
        "!pip install -qqqU datasets trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SZR6aYqCm9E",
        "outputId": "ef3f986b-7116-40ba-8f6a-217d1c25db59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, datasets, accelerate, transformers, evaluate, gc\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForCausalLM,\n",
        "                          GenerationConfig,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          BitsAndBytesConfig)\n",
        "\n",
        "from peft import (LoraConfig,\n",
        "                  get_peft_model,\n",
        "                  prepare_model_for_kbit_training)\n",
        "\n",
        "from trl import (SFTTrainer,\n",
        "                 DataCollatorForCompletionOnlyLM)\n",
        "\n",
        "from google.colab import runtime"
      ],
      "metadata": {
        "id": "uICeflp8gLwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb0ed2b-1a05-4a0e-af45-8eb983e5593d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('8013'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-vhm-3qmodx3es1ofd --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "def pp(x):\n",
        "  pprint(x, compact = True, width = 150)\n",
        "\n",
        "def cleanup(x):\n",
        "    if x == 'memory':\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    elif x == 'runtime':\n",
        "        runtime.unassign()"
      ],
      "metadata": {
        "id": "lFifz7ePgae9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331,
          "referenced_widgets": [
            "0642edee176147f4a68d622eeb6b64ab",
            "a5f8ea8631254415b779f013482561d5",
            "1a261823bf934f408fd8fa2d53cbd086",
            "c4cec27bf9bd4f089c275a32fb156b95",
            "d0e744fddcc646869db8f2aaa72eaaa6",
            "8e4357b91d4243c99b9ffd0291224954",
            "4ca551f195174d75ada226f2b45a5465",
            "7c1909b3c2ff4f4daab1d32fd8259c29",
            "5354943d53804704b83195ac5e52a6e4",
            "879fc3f49a044fe787cc4c2ca881a38a",
            "8baff2df59144560b9cb1fd1289c79a9",
            "7cf782cb05054a959e0fd445bb1e2fa8",
            "6da19a724e96450e9dba98fcbd2b1caf",
            "b74f6fbce74e4d289a74f076eb15424d",
            "20e1eb0ede264360b873bdee25437e1f",
            "a77d994740224fa180f2f22019b7ae2f",
            "29751897f82a42279a0f7a6461e2900a"
          ]
        },
        "id": "qOe5CYmxKNgw",
        "outputId": "15b0567d-6af9-43dd-dc19-02dcc25239c7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0642edee176147f4a68d622eeb6b64ab"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "B0SsQTz2p1Gu",
        "outputId": "d128d0a5-2f3f-4c11-f8a7-502ff81fbd2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "RPCmEWx9ifH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_api = wandb.Api()\n",
        "artifact = wandb_api.artifact('ft-llmmm/ELI5_analysis/combined_dataset:v5', type='dataset')\n",
        "artifact_dir = artifact.download()"
      ],
      "metadata": {
        "id": "6ZOy7y4giecw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artifact_dir = './artifacts/combined_dataset:v5'"
      ],
      "metadata": {
        "id": "9ctTUMl3qgjF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "\n",
        "data = load_from_disk(artifact_dir)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXf2mveKD0pI",
        "outputId": "9b6690ed-73f2-43f9-e51b-436c0acd29b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA'],\n",
              "        num_rows: 72214\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA'],\n",
              "        num_rows: 1964\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA'],\n",
              "        num_rows: 3301\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add QA number of words to the dataset\n",
        "\n",
        "def add_num_words(example):\n",
        "    example['num_words'] = len(example['QA'].split())\n",
        "    return example"
      ],
      "metadata": {
        "id": "qu_zpo_8ppjb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.map(add_num_words)\n",
        "data = data.filter(lambda x : x['num_words'] < 400)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toS6LC_fpuiV",
        "outputId": "fe55dd26-1cc6-4522-817a-1a35d195bde4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 68587\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 1890\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 3107\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wiki = data.filter(lambda example : example['source'] == 'simple_wiki')\n",
        "wiki"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTJWK8yHqXjK",
        "outputId": "d3bac403-d337-4725-b041-a284cd8d9e97"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 29987\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 1000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eli5 = data.filter(lambda example : example['source'] == 'ELI5')\n",
        "eli5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2baYh0oPZ6P",
        "outputId": "709dee3c-ef0b-442e-8285-dcca2c6dee1b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 38600\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 890\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'answer', 'source', 'QA', 'num_words'],\n",
              "        num_rows: 2107\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer, formatting, collator"
      ],
      "metadata": {
        "id": "BFqMYcFkT4XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'meta-llama/Llama-2-7b-hf'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "Jf9caNdpT5u4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the collator\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['question'])):\n",
        "        text = f\"### Human: {example['question'][i]}\\n ### Assistant: {example['answer'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "response_template = \"### Assistant:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "fzZMEyTaBMbP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_tokenized = data.map(lambda x : tokenizer(x['QA'], return_length = True))\n",
        "# max_seq_length = max(data_tokenized['train']['length'])[0]\n",
        "\n",
        "# wiki_tokenized = wiki.map(lambda x : tokenizer(x['QA'], return_length = True))\n",
        "# max_seq_length_wiki = max(wiki_tokenized['train']['length'])[0]\n",
        "\n",
        "# eli5_tokenized = wiki.map(lambda x : tokenizer(x['QA'], return_length = True))\n",
        "# max_seq_length_eli5 = max(eli5_tokenized['train']['length'])[0]"
      ],
      "metadata": {
        "id": "qzRIyp_eQFHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word length statistics"
      ],
      "metadata": {
        "id": "yjgGjxJJwThw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_sorted = data.sort('num_words')\n",
        "\n",
        "# import numpy as np\n",
        "# len_stat = np.zeros((3,400))\n",
        "# split_names = {'train' : 0, 'validation' : 1 , 'test' : 2}\n",
        "\n",
        "# for k, v in split_names.items():\n",
        "#     lengths = np.array(data_sorted[k]['num_words'])\n",
        "#     len_stat[v] = np.array([lengths[lengths ==n].shape[0] for n in range(400)] )\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, axs = plt.subplots(1,3, figsize=(10, 3))\n",
        "# fig.suptitle('Number of words distibution')\n",
        "\n",
        "# for k, v  in split_names.items():\n",
        "\n",
        "#   axs[v].plot(len_stat[v, :])\n",
        "#   axs[v].set_title(k + ' dataset')\n",
        "#   if v ==0:\n",
        "#     axs[v].set_xlabel('Number of words')\n",
        "#     axs[v].set_ylabel('Number of examples')\n",
        "\n",
        "# plt.tight_layout()"
      ],
      "metadata": {
        "id": "W2wLuAtoo16B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference analysis"
      ],
      "metadata": {
        "id": "xxJ0uqPsahyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from peft import PeftModel\n",
        "\n",
        "# model_base = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "# model_trained = PeftModel.from_pretrained(model = model_base,\n",
        "#                     model_id = 'finetuned_models/checkpoint-1170/',\n",
        "#                     torch_dtype = torch.bfloat16,\n",
        "#                     is_trainable = False)\n",
        "\n",
        "# model_trained_merged = model_trained.merge_and_unload()"
      ],
      "metadata": {
        "id": "8lUlBOev29wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# random.seed(123)\n",
        "\n",
        "# def generate_examples(models, n):\n",
        "#     \"\"\"\n",
        "#     Inputs:\n",
        "#         models -- list, a list of models\n",
        "#         n      -- int, number of examples to generate\n",
        "#     \"\"\"\n",
        "\n",
        "#     indices = [random.randint(0, len(wiki_qa['test']['question']) ) for _ in range(n)]\n",
        "\n",
        "\n",
        "\n",
        "#     dash_line = '-'*150\n",
        "\n",
        "#     #for model in models:\n",
        "#     #    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "#     generation_config = GenerationConfig(   num_beams = 3,\n",
        "#                                             #max_length = 100,\n",
        "#                                             max_new_tokens = 50,\n",
        "#                                             do_sample = True,\n",
        "#                                             temperature = 0.5,\n",
        "#                                             top_k = 50,\n",
        "#                                             top_p = 0.8,\n",
        "#                                             repetition_penalty = 1.0,\n",
        "#                                             #pad_token_id = model.config.eos_token_id\n",
        "#                                         )\n",
        "\n",
        "\n",
        "#     for idx in indices:\n",
        "#         question = wiki_qa['test'][idx]['question']\n",
        "#         answer = wiki_qa['test'][idx]['trunc_text']\n",
        "\n",
        "#         prompt = f\"### Question: {question}\\n### Answer:\"\n",
        "#         input = tokenizer(prompt, return_tensors = 'pt', padding = False)\n",
        "\n",
        "\n",
        "#         print(dash_line)\n",
        "#         print(f'Input prompt:\\n{prompt}')\n",
        "#         print(dash_line)\n",
        "#         print(f'Baseline:\\n{answer}')\n",
        "#         print(dash_line)\n",
        "\n",
        "#         for i in range(len(models)):\n",
        "#             model = models[i]\n",
        "#             output_ids = model.generate(input_ids = input['input_ids'],\n",
        "#                                         attention_mask = input['attention_mask'],\n",
        "#                                         generation_config = generation_config,\n",
        "#                                         pad_token_id = model.config.eos_token_id,\n",
        "#                                         #stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=100)])\n",
        "#                                         )\n",
        "\n",
        "#             output = tokenizer.decode(output_ids[0], skip_special_tokens = True)\n",
        "\n",
        "#             print(f'Model {i+1} output:\\n{output}')\n",
        "#             print(dash_line)\n",
        "\n",
        "#         print('\\n\\n')"
      ],
      "metadata": {
        "id": "d5E8NBAMbbyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_examples([model_base, model_trained, model_trained_merged],3)"
      ],
      "metadata": {
        "id": "_YwgBh7uPb3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# idx = 300\n",
        "# model = model_base\n",
        "# question = wiki_qa['test'][idx]['question']\n",
        "# prompt = f\"### Question: {question}\\n### Answer:\"\n",
        "# input = tokenizer(prompt, return_tensors = 'pt', padding = False)\n",
        "\n",
        "# output_ids = model.generate(input_ids = input['input_ids'],\n",
        "#                             #attention_mask = input['attention_mask'],\n",
        "#                             generation_config = GenerationConfig(   num_beams = 5,\n",
        "#                                                                     max_length = 100,\n",
        "#                                                                     max_new_tokens = 50,\n",
        "#                                                                     do_sample = True,\n",
        "#                                                                     temperature = 2.5,\n",
        "#                                                                     top_k = 40,\n",
        "#                                                                     top_p = 0.5,\n",
        "#                                                                     #repetition_penalty = 1.0,\n",
        "#                                                                     #pad_token_id = model.config.eos_token_id\n",
        "#                                                                 ),\n",
        "#                             pad_token_id = model.config.eos_token_id,\n",
        "#                             eos_token_id = model.config.eos_token_id\n",
        "#                             #stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=100)])\n",
        "#                             )\n",
        "# output = tokenizer.decode(output_ids[0], skip_special_tokens = True)\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "2f2Mj1OsGGZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model setup"
      ],
      "metadata": {
        "id": "jBQSWIRjTMH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "pjCxHokNQ-rF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_linear_names(model):\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, bnb.nn.Linear4bit):\n",
        "            names = name.split(\".\")\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "    if \"lm_head\" in lora_module_names:\n",
        "        lora_module_names.remove(\"lm_head\")\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "yoB7Q7Vkov6p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_quant_model(model_id = model_id):\n",
        "\n",
        "    nf4_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                    bnb_4bit_quant_type=\"nf4\",\n",
        "                                    bnb_4bit_use_double_quant=True,\n",
        "                                    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "                                    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                                quantization_config=nf4_config,\n",
        "                                                device_map='auto',\n",
        "                                                torch_dtype = torch.bfloat16\n",
        "                                                )\n",
        "\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    return model\n",
        "\n",
        "def get_lora_model(model, lora_rank = 32):\n",
        "\n",
        "    config = LoraConfig(r=lora_rank,\n",
        "                        lora_alpha=32,\n",
        "                        target_modules=find_all_linear_names(model),\n",
        "                        lora_dropout=0.1,\n",
        "                        bias=\"none\",\n",
        "                        task_type=\"CAUSAL_LM\")\n",
        "\n",
        "    model = get_peft_model(model, config)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "SaoFzPsGAbui"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "-aa8XyrATQ7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_PROJECT=sft_llama_bm\n",
        "WANDB_LOG_MODEL=\"checkpoint\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YCsXnMi9zzw",
        "outputId": "deff09c1-0605-4d72-a809-a74b986e22d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_PROJECT=sft_llama_bm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr_schedule = timm.scheduler.CosineLRScheduler()"
      ],
      "metadata": {
        "id": "RRFAeU85WhJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_args_config(lora_rank = 32,\n",
        "                         num_epochs = 1,\n",
        "                         lr = 1e-3,\n",
        "                         weight_decay = 0.1,\n",
        "                         batch_size = 8,\n",
        "                         gradient_accumulation_steps = 4,\n",
        "                         eval_steps = 500,\n",
        "                         find_batch_size = True,\n",
        "                         lr_schedule = 'linear',\n",
        "                         output_dir = 'finetuned_models/llama',\n",
        "                         run_name = None\n",
        "                         ):\n",
        "\n",
        "    params = {\n",
        "\n",
        "    #-----training----------------\n",
        "    'num_train_epochs' : num_epochs,\n",
        "    'learning_rate' : lr,\n",
        "    'weight_decay' : weight_decay,\n",
        "    'dataloader_num_workers' : 4,\n",
        "    'per_device_train_batch_size' : batch_size,\n",
        "    'per_device_eval_batch_size' : batch_size,\n",
        "    'group_by_length' : True,\n",
        "    'auto_find_batch_size' : find_batch_size,\n",
        "    'lr_scheduler_type' : lr_schedule,\n",
        "\n",
        "    #-----efficient training-----\n",
        "    'gradient_accumulation_steps' : gradient_accumulation_steps,\n",
        "    'gradient_checkpointing' : True,\n",
        "    'optim' : 'paged_adamw_8bit',\n",
        "    'bf16' : True,\n",
        "\n",
        "    #-----logging----------------\n",
        "    'report_to' : 'wandb',\n",
        "    'output_dir' : output_dir,\n",
        "\n",
        "    'evaluation_strategy' : \"steps\",\n",
        "    'eval_steps' : eval_steps,\n",
        "\n",
        "    'logging_strategy' : 'steps',\n",
        "    'logging_steps' : eval_steps,\n",
        "\n",
        "    'save_strategy' : \"steps\",\n",
        "    'save_steps' : eval_steps,\n",
        "\n",
        "    'save_total_limit' : 2,\n",
        "    'load_best_model_at_end' : True,\n",
        "    'metric_for_best_model' : \"eval_loss\",\n",
        "    'greater_is_better' : False,\n",
        "\n",
        "    'run_name' : f'r{lora_rank}_lr{lr:.0e}_wd{weight_decay}_bs{batch_size}' + ('_' + run_name if run_name is not None else ''),\n",
        "    }\n",
        "\n",
        "    params['output_dir'] = params['output_dir'] + '/' + params['run_name']\n",
        "\n",
        "    return params"
      ],
      "metadata": {
        "id": "Jh_gl1iRSvuw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_id = 'meta-llama/Llama-2-7b-hf'\n",
        "r = 16\n",
        "\n",
        "model= prepare_quant_model(model_id = model_id)\n",
        "model = get_lora_model(model, lora_rank = r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "be6c2448796840a3974c8b7051ac1dc6",
            "211bf88023414e6da73f75321e228857",
            "a54d4ea4a2594b2eac368edfcf64d057",
            "0eee3953ec3d421ebe9f2dd10a5cf7a0",
            "3324b0c3a6594c4d9f80a8ecb2f3f19f",
            "b3220a36de7c48e28176b2a111bfdfcc",
            "4ec9e23df252433c8746d701dc46abd9",
            "8976dbdf76ef466aa4ac33f9dd7c80fc",
            "7d485d6cc5e84a36bbfd0fc2d1b47e69",
            "14b4963f025e4f09b3e6d17d96bb837a",
            "83f58c90359a4edfa7ecaff62a4a7672"
          ]
        },
        "id": "W46Zk_n36Zic",
        "outputId": "d274fef7-f174-4718-99f1-c75091374cd4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be6c2448796840a3974c8b7051ac1dc6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tot_steps = wiki['train'].num_rows / (26 * 4)\n",
        "tot_steps/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ_f8M0tGTOf",
        "outputId": "75628eb1-5ffc-4eac-ee74-699f31cae823"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.11217948717949"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = training_args_config( lora_rank = r,\n",
        "                                    num_epochs = 3,\n",
        "                                    lr = 1e-5,\n",
        "                                    weight_decay = 0.1,\n",
        "                                    batch_size = 26,\n",
        "                                    gradient_accumulation_steps = 4,\n",
        "                                    eval_steps = 96,\n",
        "                                    find_batch_size = True,\n",
        "                                    lr_schedule = 'linear',\n",
        "                                    output_dir = 'finetuned_models/llama',\n",
        "                                    run_name = 'wiki'\n",
        "                                    )\n",
        "pp(hyperparams)\n",
        "\n",
        "trainer = SFTTrainer(   model,\n",
        "                        args = TrainingArguments(**hyperparams),\n",
        "                        max_seq_length=1024,\n",
        "                        train_dataset=wiki['train'],\n",
        "                        eval_dataset=wiki['validation'],\n",
        "                        formatting_func=formatting_prompts_func,\n",
        "                        data_collator=collator,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzRiMLBNTGjK",
        "outputId": "ac315501-e67d-4f1d-9690-399078bf16d0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'auto_find_batch_size': True,\n",
            " 'bf16': True,\n",
            " 'dataloader_num_workers': 4,\n",
            " 'eval_steps': 96,\n",
            " 'evaluation_strategy': 'steps',\n",
            " 'gradient_accumulation_steps': 4,\n",
            " 'gradient_checkpointing': True,\n",
            " 'greater_is_better': False,\n",
            " 'group_by_length': True,\n",
            " 'learning_rate': 1e-05,\n",
            " 'load_best_model_at_end': True,\n",
            " 'logging_steps': 96,\n",
            " 'logging_strategy': 'steps',\n",
            " 'lr_scheduler_type': 'linear',\n",
            " 'metric_for_best_model': 'eval_loss',\n",
            " 'num_train_epochs': 3,\n",
            " 'optim': 'paged_adamw_8bit',\n",
            " 'output_dir': 'finetuned_models/llama/r16_lr1e-05_wd10_bs26_wiki',\n",
            " 'per_device_eval_batch_size': 26,\n",
            " 'per_device_train_batch_size': 26,\n",
            " 'report_to': 'wandb',\n",
            " 'run_name': 'r16_lr1e-05_wd10_bs26_wiki',\n",
            " 'save_steps': 96,\n",
            " 'save_strategy': 'steps',\n",
            " 'save_total_limit': 2,\n",
            " 'weight_decay': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOWPlz6r1lZF",
        "outputId": "2a02234c-e785-47fb-f1a3-972b4fc97b44"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 39976960 || all params: 3540389888 || trainable%: 1.1291682911958425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fYJpgfypTiBS",
        "outputId": "92f5ef10-daf2-4a8e-a84b-443fe5edc17a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.15.8 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/SFT/wandb/run-20230821_145154-xmmu4keq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/sft_llama_bm/runs/xmmu4keq' target=\"_blank\">r16_lr1e-05_wd10_bs26_wiki</a></strong> to <a href='https://wandb.ai/ft-llmmm/sft_llama_bm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ft-llmmm/sft_llama_bm' target=\"_blank\">https://wandb.ai/ft-llmmm/sft_llama_bm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ft-llmmm/sft_llama_bm/runs/xmmu4keq' target=\"_blank\">https://wandb.ai/ft-llmmm/sft_llama_bm/runs/xmmu4keq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='864' max='864' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [864/864 2:01:31, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.303600</td>\n",
              "      <td>1.283455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>1.269600</td>\n",
              "      <td>1.267538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>1.255100</td>\n",
              "      <td>1.260830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>1.220300</td>\n",
              "      <td>1.259081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.216600</td>\n",
              "      <td>1.255147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>1.232400</td>\n",
              "      <td>1.253440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>1.183800</td>\n",
              "      <td>1.256996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>768</td>\n",
              "      <td>1.183100</td>\n",
              "      <td>1.255568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>864</td>\n",
              "      <td>1.188500</td>\n",
              "      <td>1.255063</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=864, training_loss=1.2281091919651739, metrics={'train_runtime': 7317.2121, 'train_samples_per_second': 12.294, 'train_steps_per_second': 0.118, 'total_flos': 2.787798350594949e+17, 'train_loss': 1.2281091919651739, 'epoch': 2.99})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "aIT-OVwf0bRb",
        "outputId": "e02ec235-2c9d-424c-9840-8021e0fe1a76"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▃▂▁▁▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁█▇▅▂█▇▇▄</td></tr><tr><td>eval/samples_per_second</td><td>█▁▂▄▇▁▂▂▅</td></tr><tr><td>eval/steps_per_second</td><td>█▁▃▆█▁▁▁▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▃▃▄▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.25506</td></tr><tr><td>eval/runtime</td><td>49.1842</td></tr><tr><td>eval/samples_per_second</td><td>20.332</td></tr><tr><td>eval/steps_per_second</td><td>0.793</td></tr><tr><td>train/epoch</td><td>2.99</td></tr><tr><td>train/global_step</td><td>864</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1885</td></tr><tr><td>train/total_flos</td><td>2.787798350594949e+17</td></tr><tr><td>train/train_loss</td><td>1.22811</td></tr><tr><td>train/train_runtime</td><td>7317.2121</td></tr><tr><td>train/train_samples_per_second</td><td>12.294</td></tr><tr><td>train/train_steps_per_second</td><td>0.118</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">r16_lr1e-05_wd10_bs26_wiki</strong> at: <a href='https://wandb.ai/ft-llmmm/sft_llama_bm/runs/xmmu4keq' target=\"_blank\">https://wandb.ai/ft-llmmm/sft_llama_bm/runs/xmmu4keq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230821_145154-xmmu4keq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "cleanup('memory')"
      ],
      "metadata": {
        "id": "Zdje6e61Wjz2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleanup('runtime')"
      ],
      "metadata": {
        "id": "tXls6qig7KqH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l__znkPK7goQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}