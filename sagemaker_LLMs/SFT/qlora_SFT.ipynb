{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.finish()\n",
    "#!pip install --upgrade pip\n",
    "#!pip install \"transformers==4.30.2\" \n",
    "#!pip install \"datasets[s3]==2.13.0\" \n",
    "#!pip install sagemaker --upgrade \n",
    "#!pip install \"transformers==4.30.2\" --upgrade\n",
    "#!pip3 install git+https://github.com/huggingface/transformers\n",
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sagemaker\n",
    "import boto3\n",
    "import torch\n",
    "##import wandb\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "import wandb\n",
    "\n",
    "hf_token=None\n",
    "wandb_token=None\n",
    "\n",
    "#wandb.login()\n",
    "\n",
    "#wandb.finish()\n",
    "#wandb.init(\n",
    "#    job_type = 'training',\n",
    "#    dir = './',\n",
    "#    project='sagemaker_training_test',\n",
    "#    entity ='ft-llmmm'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = getpass('input hf token')\n",
    "wandb_token = getpass('input wandb token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket='llms-hf'\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='DataScientist')['Role']['Arn']\n",
    "\n",
    "#sagemaker_session_bucket = sess.default_bucket()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "artifact_dir='artifacts/combined_dataset:v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging into the Hugging Face Hub with token hf_VLXemeC...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/davidmeltzer/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "print(f\"Logging into the Hugging Face Hub with token {hf_token[:10]}...\")\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/davidmeltzer/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=wandb_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now = datetime.now()\n",
    "#time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
    "#with wandb.init(project='SFT_training_DM',\n",
    "#                entity='ft-llmmm',\n",
    "#                job_type='download_data',\n",
    "#                name=f'download_combined_data_{time_stamp}') as run:\n",
    "#\n",
    "#    artifact = run.use_artifact('ft-llmmm/ELI5_analysis/combined_dataset:v1', type='dataset')\n",
    "#    artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "\n",
    "\n",
    "ds_QA_SFT = load_from_disk(f'../{artifact_dir}')\n",
    "ds_wiki = ds_QA_SFT.filter(lambda x:x['source']=='simple_wiki')\n",
    "ds_ELI5 = ds_QA_SFT.filter(lambda x:x['source']=='ELI5')\n",
    "\n",
    "ds_name='wiki'\n",
    "\n",
    "if ds_name == 'combined':\n",
    "    ds = ds_QA_SFT\n",
    "elif ds_name == 'wiki':\n",
    "    ds = ds_wiki\n",
    "elif ds_name == 'ELI5':\n",
    "    ds = ds_ELI5\n",
    "else:\n",
    "    raise ValueError(\"Input valid dataset name, 'combined','ELI5', or 'wiki'\")\n",
    "\n",
    "training_input_path = f's3://{sagemaker_session_bucket}/{ds_name}'\n",
    "#ds.save_to_disk(training_input_path)\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora'\n",
    "run_name += f'__{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,\n",
    "  'dataset_path': '/opt/ml/input/data/training',\n",
    "  'epochs': 1,\n",
    "  'per_device_train_batch_size': 8,\n",
    "  'lr': 2e-4,\n",
    "  'hf_token': hf_token,\n",
    "  'wandb_token': wandb_token,\n",
    "  'merge_weights': False,\n",
    "  'per_device_eval_batch_size': 8,\n",
    "  'gradient_accumulation_steps': 6,\n",
    "  'max_seq_length': 512,\n",
    "  'max_steps': 1,\n",
    "  'entity': 'ft-llmmm',\n",
    "  'project_name': 'SFT_training_dm',\n",
    "  'run_name': run_name\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = './scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-qlora-2023-08-10-15-46-59-2023-08-10-19-47-02-683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-10 19:47:03 Starting - Starting the training job...\n",
      "2023-08-10 19:47:19 Starting - Preparing the instances for training......\n",
      "2023-08-10 19:48:30 Downloading - Downloading input data...\n",
      "2023-08-10 19:48:51 Failed - Training job failed\n",
      ".."
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-qlora-2023-08-10-15-46-59-2023-08-10-19-47-02-683: Failed. Reason: ClientError: Data download failed:Failed to download data. 403 Forbidden (403): Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# starting the train job with our uploaded datasets as input\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m huggingface_estimator\u001b[39m.\u001b[39;49mfit(data, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sagemaker/estimator.py:1292\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1291\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 1292\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sagemaker/estimator.py:2474\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[39m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2473\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2474\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_type\u001b[39m=\u001b[39;49mlogs)\n\u001b[1;32m   2475\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2476\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sagemaker/session.py:4849\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4828\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs_for_job\u001b[39m(\u001b[39mself\u001b[39m, job_name, wait\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, poll\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, log_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   4829\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4830\u001b[0m \n\u001b[1;32m   4831\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4847\u001b[0m \u001b[39m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4848\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4849\u001b[0m     _logs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboto_session, job_name, wait, poll, log_type, timeout)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sagemaker/session.py:6760\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6757\u001b[0m             last_profiler_rule_statuses \u001b[39m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6759\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 6760\u001b[0m     _check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTrainingJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   6761\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   6762\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/sagemaker/session.py:6813\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6807\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   6808\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   6809\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6810\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6811\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6812\u001b[0m     )\n\u001b[0;32m-> 6813\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6814\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6815\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6816\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6817\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-qlora-2023-08-10-15-46-59-2023-08-10-19-47-02-683: Failed. Reason: ClientError: Data download failed:Failed to download data. 403 Forbidden (403): Forbidden"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
