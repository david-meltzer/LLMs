{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wandb.finish()\n",
    "!pip install --upgrade pip\n",
    "!pip install \"transformers==4.30.2\" \n",
    "!pip install \"datasets[s3]==2.13.0\" \n",
    "!pip install sagemaker --upgrade \n",
    "!pip install wandb\n",
    "#!pip install \"transformers==4.30.2\" --upgrade\n",
    "#!pip3 install git+https://github.com/huggingface/transformers\n",
    "!pip install torch \n",
    "!pip install \"accelerate[sagemaker]\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "import torch\n",
    "##import wandb\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "import wandb\n",
    "import accelerate\n",
    "\n",
    "hf_token=None\n",
    "wandb_token=None\n",
    "\n",
    "hf_token = getpass('input hf token')\n",
    "wandb_token = getpass('input wandb token')\n",
    "\n",
    "login(token=hf_token)\n",
    "wandb.login(key=wandb_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sess = sagemaker.Session()\n",
    "#sagemaker_session_bucket='llms-hf'\n",
    "#\n",
    "#iam = boto3.client('iam')\n",
    "#role = iam.get_role(RoleName='Sagemaker-DataScientist')['Role']['Arn']\n",
    "##role = iam.get_role(RoleName='sagemaker')['Role']['Arn']\n",
    "#\n",
    "##sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "#sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "#sagemaker_session_bucket = sess.default_bucket()\n",
    "#artifact_dir='artifacts/combined_dataset:v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "#sagemaker_session_bucket='llms-hf'\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "    \n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='Sagemaker-DataScientist')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now = datetime.now()\n",
    "#time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
    "#with wandb.init(project='SFT_training_DM',\n",
    "#                entity='ft-llmmm',\n",
    "##                job_type='download_data',\n",
    "#                name=f'download_combined_data_{time_stamp}') as run:\n",
    "#    \n",
    "#    artifact = run.use_artifact(f'ft-llmmm/ELI5_analysis/llama_QA_tokenized:latest', type='dataset')\n",
    "#    artifact_full_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#now = datetime.now()\n",
    "#time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
    "#with wandb.init(project='SFT_training_DM',\n",
    "#                entity='ft-llmmm',\n",
    "#                job_type='download_data',\n",
    "#                name=f'download_combined_data_{time_stamp}') as run:\n",
    "#    artifact_dir = {}\n",
    "#    \n",
    "#    \n",
    "#    for key in [1024,2048]:\n",
    "#    \n",
    "#        artifact = run.use_artifact(f'ft-llmmm/ELI5_analysis/llama_QA_tokenized_{key}:latest', type='dataset')\n",
    "#        artifact_dir[key] = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Provided instance_type ml.g5.12xlarge is not supported by smdataparallel.\nPlease specify one of the supported instance types:('ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p4de.24xlarge', 'local_gpu')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5502/3424645270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mhyperparameters\u001b[0m      \u001b[0;34m=\u001b[0m  \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# the hyperparameters passed to the training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0menvironment\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"HUGGINGFACE_HUB_CACHE\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"/tmp/.cache\"\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# set env variable to cache models in /tmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;31m#use_spot_instances = use_spot_instances,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m#max_wait = max_wait,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sagemaker/huggingface/estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, py_version, entry_point, transformers_version, tensorflow_version, pytorch_version, source_dir, hyperparameters, image_uri, distribution, compiler_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mpy_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0mimage_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             )\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sagemaker/fw_utils.py\u001b[0m in \u001b[0;36mvalidate_distribution\u001b[0;34m(distribution, instance_groups, framework_name, framework_version, py_version, image_uri, kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0mpy_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpy_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mdistribution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             \u001b[0mimage_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_uri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m         )\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframework_name\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mframework_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pytorch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sagemaker/fw_utils.py\u001b[0m in \u001b[0;36mvalidate_smdistributed\u001b[0;34m(instance_type, framework_name, framework_version, py_version, distribution, image_uri)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"dataparallel\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmdistributed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         _validate_smdataparallel_args(\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         )\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sagemaker/fw_utils.py\u001b[0m in \u001b[0;36m_validate_smdataparallel_args\u001b[0;34m(instance_type, framework_name, framework_version, py_version, distribution, image_uri)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Provided instance_type ml.g5.12xlarge is not supported by smdataparallel.\nPlease specify one of the supported instance types:('ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'ml.p4de.24xlarge', 'local_gpu')\n"
     ]
    }
   ],
   "source": [
    "model_id = \"distilgpt2\" # sharded weights\n",
    "model_name = model_id.split('/')[-1]\n",
    "ds_name = 'eli5_wiki_1024'\n",
    "training_input_path = f's3://{sagemaker_session_bucket}/{ds_name}'\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "time_now = f'_{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora'\n",
    "run_name += time_now\n",
    "\n",
    "# enables spot training\n",
    "use_spot_instances = False\n",
    "# max time including spot start + training time\n",
    "max_wait = 16*60*60\n",
    "# expected training time\n",
    "max_run = 10*60*60\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora'\n",
    "job_name = f'{model_name}_{ds_name}-qlora'.replace('_','-')\n",
    "\n",
    "if use_spot_instances:\n",
    "    job_name += '-spot'\n",
    "    run_name += '-spot'\n",
    "\n",
    "run_name += f'__{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "if ds_name[-4:] in ['1024','2048']:\n",
    "    max_seq_length = int(ds_name[-4:])\n",
    "    \n",
    "hyperparameters = {\n",
    "    'output_dir': '/opt/ml/checkpoints' if use_spot_instances else '/opt/ml/model',\n",
    "    'model_id': model_id,\n",
    "    'repo_id': f'{model_name}-{ds_name}',\n",
    "    'dataset_path': '/opt/ml/input/data/training',\n",
    "    'hf_token': hf_token,\n",
    "    'wandb_token': wandb_token,\n",
    "    'report_to_wandb': 1,\n",
    "    'epochs': 3,\n",
    "    'max_steps': -1,\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'lr': 2e-5,\n",
    "    'merge_weights': 0,\n",
    "    'entity': 'ft-llmmm',\n",
    "    'project_name': 'SFT_training_dm',\n",
    "    'hub_strategy': 'every_save',\n",
    "    'run_name': run_name,\n",
    "    'torch_compile': 0,\n",
    "    'gradient_checkpointing': 1,\n",
    "    'optim': 'paged_adamw_8bit',\n",
    "    'group_by_length': 1,\n",
    "    'lora_r': 64,\n",
    "    #'fsdp': '\"full_shard auto_wrap\"',\n",
    "    #'fsdp_transformer_layer_cls_to_wrap': \"LlamaDecoderLayer\",\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "\n",
    "instance_type = 'ml.g5.12xlarge'\n",
    "instance_count = 2\n",
    "volume_size = 250\n",
    "transformers_version = '4.28.1'\n",
    "pytorch_version='2.0.0'\n",
    "py_version='py310'\n",
    "\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "if use_spot_instances:\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point          = 'run_clm.py',      # train script\n",
    "        source_dir           = './',         # directory which includes all the files needed for training\n",
    "        instance_type        = instance_type,   # instances type used for the training job\n",
    "        instance_count       = instance_count,                 # the number of instances used for training\n",
    "        base_job_name        = job_name,          # the name of the training job\n",
    "        role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "        volume_size          = volume_size,               # the size of the EBS volume in GB\n",
    "        transformers_version = transformers_version,            # the transformers version used in the training job\n",
    "        #transformers_version = '4.30',\n",
    "        pytorch_version      = pytorch_version,             # the pytorch_version version used in the training job\n",
    "        py_version           = py_version,           # the python version used in the training job\n",
    "        hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "        environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "        use_spot_instances = use_spot_instances,\n",
    "        max_wait = max_wait,\n",
    "        max_run=max_run,\n",
    "        checkpoint_s3_uri = checkpoint_s3_uri,\n",
    "        distribution=distribution\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point          = 'run_clm.py',      # train script\n",
    "        source_dir           = './',         # directory which includes all the files needed for training\n",
    "        instance_type        = instance_type,   # instances type used for the training job\n",
    "        instance_count       = instance_count,                 # the number of instances used for training\n",
    "        base_job_name        = job_name,          # the name of the training job\n",
    "        role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "        volume_size          = volume_size,               # the size of the EBS volume in GB\n",
    "        transformers_version = transformers_version,            # the transformers version used in the training job\n",
    "        #transformers_version = '4.30',\n",
    "        pytorch_version      = pytorch_version,             # the pytorch_version version used in the training job\n",
    "        py_version           = py_version,           # the python version used in the training job\n",
    "        hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "        environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "        distribution = distribution,\n",
    "        #use_spot_instances = use_spot_instances,\n",
    "        #max_wait = max_wait,\n",
    "        #max_run=max_run,\n",
    "        #checkpoint_s3_uri = checkpoint_s3_uri\n",
    "    )\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## EC2 + FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project='SFT_Training_dm',\n",
    "                 entity='ft-llmmm',\n",
    "                 job_type='download_data',\n",
    "                 name='download_SFT_EC2') as run:\n",
    "    artifact = run.use_artifact('ft-llmmm/ELI5_analysis/llama_QA_tokenized:v2', type='dataset')\n",
    "    artifact_dir = artifact.download()\n",
    "    \n",
    "    print(f'artifact saved to {artifact_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_QA_SFT = datasets.load_from_disk(f'./{artifact_dir}')\n",
    "ds_wiki = ds_QA_SFT.filter(lambda x:x['source']=='simple_wiki')\n",
    "ds_ELI5 = ds_QA_SFT.filter(lambda x:x['source']=='ELI5')\n",
    "\n",
    "ds_wiki.save_to_disk('./data/ds_wiki')\n",
    "ds_ELI5.save_to_disk('./data/ds_ELI5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "model_name = model_id.split('/')[-1]\n",
    "ds_name = 'wiki'\n",
    "\n",
    "if ds_name == 'combined':\n",
    "    dataset_path = artifact_dir\n",
    "else:\n",
    "    dataset_path = f'./data/ds_{ds_name}'\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora_flash'\n",
    "run_name += f'__{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "output_dir = f'./{model_name}_{ds_name}/models'\n",
    "logging_dir = f'./{model_name}_{ds_name}/logs'\n",
    "repo_id = f'{model_name}-{ds_name}-flash'\n",
    "report_to_wandb = 1\n",
    "epochs = 3\n",
    "max_steps = -1\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "gradient_accumulation_steps = 16\n",
    "lr = 2e-4\n",
    "merge_weights = 0\n",
    "entity = 'ft-llmmm'\n",
    "project_name = 'SFT_training_dm'\n",
    "hub_strategy = 'every_save'\n",
    "torch_compile = 0\n",
    "gradient_checkpointing = 1\n",
    "optim = 'paged_adamw_8bit'\n",
    "group_by_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ['HUGGINGFACE_TOKEN']\n",
    "wandb_token = os.environ['WANDB_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python ./run_clm.py \\\n",
    "--output_dir './Llama-2-7b-hf_wiki/models' \\\n",
    "--logging_dir './Llama-2-7b-hf_wiki/logs' \\\n",
    "--model_id 'meta-llama/Llama-2-7b-hf' \\\n",
    "--dataset_path './data/ds_wiki' \\\n",
    "--run_name 'Llama-2-7b-hf_wiki_qlora_no_flash_test' \\\n",
    "--repo_id 'Llama-2-7b-hf-wiki-flash' \\\n",
    "--report_to_wandb 1 \\\n",
    "--epochs 3 \\\n",
    "--max_steps -1 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--per_device_eval_batch_size 8 \\\n",
    "--gradient_accumulation_steps 16 \\\n",
    "--lr 2e-4 \\\n",
    "--merge_weights 0 \\\n",
    "--entity 'ft-llmmm' \\\n",
    "--project_name 'SFT_training_dm' \\\n",
    "--hub_strategy 'every_save' \\\n",
    "--torch_compile 0 \\\n",
    "--gradient_checkpointing 1 \\\n",
    "--optim 'paged_adamw_8bit' \\\n",
    "--group_by_length 1 \\\n",
    "--hf_token hf_token \\\n",
    "--wandb_token wandb_token \\\n",
    "--use_flash_attention 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
