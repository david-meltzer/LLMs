{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wandb.finish()\n",
    "!pip install --upgrade pip\n",
    "!pip install \"transformers==4.30.2\" \n",
    "!pip install \"datasets[s3]==2.13.0\" \n",
    "!pip install sagemaker --upgrade \n",
    "!pip install wandb\n",
    "#!pip install \"transformers==4.30.2\" --upgrade\n",
    "#!pip3 install git+https://github.com/huggingface/transformers\n",
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "import torch\n",
    "##import wandb\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "import wandb\n",
    "\n",
    "hf_token=None\n",
    "wandb_token=None\n",
    "\n",
    "hf_token = getpass('input hf token')\n",
    "wandb_token = getpass('input wandb token')\n",
    "\n",
    "login(token=hf_token)\n",
    "wandb.login(key=wandb_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sess = sagemaker.Session()\n",
    "#sagemaker_session_bucket='llms-hf'\n",
    "#\n",
    "#iam = boto3.client('iam')\n",
    "#role = iam.get_role(RoleName='Sagemaker-DataScientist')['Role']['Arn']\n",
    "##role = iam.get_role(RoleName='sagemaker')['Role']['Arn']\n",
    "#\n",
    "##sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "#sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "#sagemaker_session_bucket = sess.default_bucket()\n",
    "#artifact_dir='artifacts/combined_dataset:v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "#sagemaker_session_bucket='llms-hf'\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "    \n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='Sagemaker-DataScientist')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/LLMs/training/aws/SFT/no_flash/wandb/run-20230819_182033-dl1lcnpq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/SFT_training_dm/runs/dl1lcnpq' target=\"_blank\">download_combined_data_08.19.23-18.20.33</a></strong> to <a href='https://wandb.ai/ft-llmmm/SFT_training_dm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ft-llmmm/SFT_training_dm' target=\"_blank\">https://wandb.ai/ft-llmmm/SFT_training_dm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ft-llmmm/SFT_training_dm/runs/dl1lcnpq' target=\"_blank\">https://wandb.ai/ft-llmmm/SFT_training_dm/runs/dl1lcnpq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact llama_QA_tokenized:latest, 206.21MB. 10 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   10 of 10 files downloaded.  \n",
      "Done. 0:0:7.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">download_combined_data_08.19.23-18.20.33</strong> at: <a href='https://wandb.ai/ft-llmmm/SFT_training_dm/runs/dl1lcnpq' target=\"_blank\">https://wandb.ai/ft-llmmm/SFT_training_dm/runs/dl1lcnpq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230819_182033-dl1lcnpq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
    "with wandb.init(project='SFT_training_DM',\n",
    "                entity='ft-llmmm',\n",
    "                job_type='download_data',\n",
    "                name=f'download_combined_data_{time_stamp}') as run:\n",
    "    \n",
    "    artifact = run.use_artifact(f'ft-llmmm/ELI5_analysis/llama_QA_tokenized:latest', type='dataset')\n",
    "    artifact_full_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
    "with wandb.init(project='SFT_training_DM',\n",
    "                entity='ft-llmmm',\n",
    "                job_type='download_data',\n",
    "                name=f'download_combined_data_{time_stamp}') as run:\n",
    "    artifact_dir = {}\n",
    "    \n",
    "    \n",
    "    for key in [1024,2048]:\n",
    "    \n",
    "        artifact = run.use_artifact(f'ft-llmmm/ELI5_analysis/llama_QA_tokenized_{key}:latest', type='dataset')\n",
    "        artifact_dir[key] = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-2-7b-hf_ELI5_qlora-spot__2023-08-19-18-34-43'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "model_name = model_id.split('/')[-1]\n",
    "ds_name = 'llama1024'\n",
    "training_input_path = f's3://{sagemaker_session_bucket}/{ds_name}'\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora'\n",
    "run_name += f'_{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# enables spot training\n",
    "use_spot_instances = True\n",
    "# max time including spot start + training time\n",
    "max_wait = 86400\n",
    "# expected training time\n",
    "max_run = 64800\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora'\n",
    "job_name = f'{model_name}_{ds_name}-qlora'.replace('_','-')\n",
    "\n",
    "if use_spot_instances:\n",
    "    job_name += '-spot'\n",
    "    run_name += '-spot'\n",
    "\n",
    "run_name += f'__{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'\n",
    "    \n",
    "max_seq_length = 4096\n",
    "\n",
    "if int(ds_name[-4:]) in [1024,2048]:\n",
    "    max_seq_length = int(ds_name[-4:])\n",
    "    \n",
    "hyperparameters = {\n",
    "    'model_id': model_id,\n",
    "    'repo_id': f'{model_name}-{ds_name}',\n",
    "    'dataset_path': '/opt/ml/input/data/training',\n",
    "    'hf_token': hf_token,\n",
    "    'wandb_token': wandb_token,\n",
    "    'report_to_wandb': 1,\n",
    "    'epochs': 3,\n",
    "    'max_steps': -1,\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'lr': 2e-4,\n",
    "    'merge_weights': 0,\n",
    "    'entity': 'ft-llmmm',\n",
    "    'project_name': 'SFT_training_dm',\n",
    "    'hub_strategy': 'every_save',\n",
    "    'run_name': run_name,\n",
    "    'torch_compile': 0,\n",
    "    'gradient_checkpointing': 1,\n",
    "    'optim': 'paged_adamw_8bit',\n",
    "    'group_by_length': 1\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = './',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.4xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    #transformers_version = '4.30',\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    "    use_spot_instances = use_spot_instances,\n",
    "    max_wait = max_wait,\n",
    "    max_run=max_run,\n",
    "    checkpoint_s3_uri = checkpoint_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: Llama-2-7b-hf-llama1024-qlora-spot-2023-08-19-20-08-16-512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-19 20:11:16 Starting - Starting the training job...\n",
      "2023-08-19 20:11:33 Starting - Preparing the instances for training......\n",
      "2023-08-19 20:12:29 Downloading - Downloading input data...\n",
      "2023-08-19 20:12:54 Training - Downloading the training image........................\n",
      "2023-08-19 20:17:11 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:05,551 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:05,564 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:05,573 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:05,574 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:11,705 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 60.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.31.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 88.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.13.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.13.0-py3-none-any.whl (485 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 485.6/485.6 kB 59.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft==0.4.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.4.0-py3-none-any.whl (72 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 21.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.21.0 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 244.2/244.2 kB 50.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.41.1 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 25.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting trl==0.4.7 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.4.7-py3-none-any.whl (77 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.4/77.4 kB 24.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (0.14.1)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 89.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.31.0->-r requirements.txt (line 2)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 3)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 3)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 3)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 3)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 3)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->-r requirements.txt (line 3)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 4)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0->-r requirements.txt (line 4)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (8.1.3)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.32-py3-none-any.whl (188 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.5/188.5 kB 43.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 215.6/215.6 kB 42.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 8)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 3)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 3)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 3)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 3)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 3)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->-r requirements.txt (line 3)) (1.3.1)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 20.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 2)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 4)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 4)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 4)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->-r requirements.txt (line 3)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->-r requirements.txt (line 3)) (2023.3)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 4)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0->-r requirements.txt (line 4)) (1.3.0)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: pathtools\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=c0d010412697793f97b1331229e708b354412736851f04637dd3fa686a21851b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\u001b[0m\n",
      "\u001b[34mSuccessfully built pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, pathtools, bitsandbytes, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, transformers, GitPython, accelerate, wandb, peft, datasets, trl\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.12.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.12.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.12.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.32 accelerate-0.21.0 bitsandbytes-0.41.1 datasets-2.13.0 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 peft-0.4.0 safetensors-0.3.2 sentry-sdk-1.29.2 setproctitle-1.3.2 smmap-5.0.0 transformers-4.31.0 trl-0.4.7 wandb-0.15.8\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.2.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:25,960 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:25,960 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:25,974 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:25,998 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:26,021 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:26,030 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataset_path\": \"/opt/ml/input/data/training\",\n",
      "        \"entity\": \"ft-llmmm\",\n",
      "        \"epochs\": 3,\n",
      "        \"gradient_accumulation_steps\": 16,\n",
      "        \"gradient_checkpointing\": 1,\n",
      "        \"group_by_length\": 1,\n",
      "        \"hf_token\": \"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\",\n",
      "        \"hub_strategy\": \"every_save\",\n",
      "        \"lr\": 0.0002,\n",
      "        \"max_steps\": -1,\n",
      "        \"merge_weights\": 0,\n",
      "        \"model_id\": \"meta-llama/Llama-2-7b-hf\",\n",
      "        \"optim\": \"paged_adamw_8bit\",\n",
      "        \"per_device_eval_batch_size\": 8,\n",
      "        \"per_device_train_batch_size\": 8,\n",
      "        \"project_name\": \"SFT_training_dm\",\n",
      "        \"repo_id\": \"Llama-2-7b-hf-llama1024\",\n",
      "        \"report_to_wandb\": 1,\n",
      "        \"run_name\": \"Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15\",\n",
      "        \"torch_compile\": 0,\n",
      "        \"wandb_token\": \"93b4fb1b729b939f257d7db15130b3710cad2ebb\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.4xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"Llama-2-7b-hf-llama1024-qlora-spot-2023-08-19-20-08-16-512\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-338563806291/Llama-2-7b-hf-llama1024-qlora-spot-2023-08-19-20-08-16-512/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_clm\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_clm.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_path\":\"/opt/ml/input/data/training\",\"entity\":\"ft-llmmm\",\"epochs\":3,\"gradient_accumulation_steps\":16,\"gradient_checkpointing\":1,\"group_by_length\":1,\"hf_token\":\"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\",\"hub_strategy\":\"every_save\",\"lr\":0.0002,\"max_steps\":-1,\"merge_weights\":0,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"optim\":\"paged_adamw_8bit\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"project_name\":\"SFT_training_dm\",\"repo_id\":\"Llama-2-7b-hf-llama1024\",\"report_to_wandb\":1,\"run_name\":\"Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15\",\"torch_compile\":0,\"wandb_token\":\"93b4fb1b729b939f257d7db15130b3710cad2ebb\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_clm.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.4xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_clm\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-338563806291/Llama-2-7b-hf-llama1024-qlora-spot-2023-08-19-20-08-16-512/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.4xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataset_path\":\"/opt/ml/input/data/training\",\"entity\":\"ft-llmmm\",\"epochs\":3,\"gradient_accumulation_steps\":16,\"gradient_checkpointing\":1,\"group_by_length\":1,\"hf_token\":\"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\",\"hub_strategy\":\"every_save\",\"lr\":0.0002,\"max_steps\":-1,\"merge_weights\":0,\"model_id\":\"meta-llama/Llama-2-7b-hf\",\"optim\":\"paged_adamw_8bit\",\"per_device_eval_batch_size\":8,\"per_device_train_batch_size\":8,\"project_name\":\"SFT_training_dm\",\"repo_id\":\"Llama-2-7b-hf-llama1024\",\"report_to_wandb\":1,\"run_name\":\"Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15\",\"torch_compile\":0,\"wandb_token\":\"93b4fb1b729b939f257d7db15130b3710cad2ebb\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"Llama-2-7b-hf-llama1024-qlora-spot-2023-08-19-20-08-16-512\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-338563806291/Llama-2-7b-hf-llama1024-qlora-spot-2023-08-19-20-08-16-512/source/sourcedir.tar.gz\",\"module_name\":\"run_clm\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_clm.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_path\",\"/opt/ml/input/data/training\",\"--entity\",\"ft-llmmm\",\"--epochs\",\"3\",\"--gradient_accumulation_steps\",\"16\",\"--gradient_checkpointing\",\"1\",\"--group_by_length\",\"1\",\"--hf_token\",\"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\",\"--hub_strategy\",\"every_save\",\"--lr\",\"0.0002\",\"--max_steps\",\"-1\",\"--merge_weights\",\"0\",\"--model_id\",\"meta-llama/Llama-2-7b-hf\",\"--optim\",\"paged_adamw_8bit\",\"--per_device_eval_batch_size\",\"8\",\"--per_device_train_batch_size\",\"8\",\"--project_name\",\"SFT_training_dm\",\"--repo_id\",\"Llama-2-7b-hf-llama1024\",\"--report_to_wandb\",\"1\",\"--run_name\",\"Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15\",\"--torch_compile\",\"0\",\"--wandb_token\",\"93b4fb1b729b939f257d7db15130b3710cad2ebb\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_PATH=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ENTITY=ft-llmmm\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=16\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_CHECKPOINTING=1\u001b[0m\n",
      "\u001b[34mSM_HP_GROUP_BY_LENGTH=1\u001b[0m\n",
      "\u001b[34mSM_HP_HF_TOKEN=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\u001b[0m\n",
      "\u001b[34mSM_HP_HUB_STRATEGY=every_save\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0002\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MERGE_WEIGHTS=0\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_ID=meta-llama/Llama-2-7b-hf\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIM=paged_adamw_8bit\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_PROJECT_NAME=SFT_training_dm\u001b[0m\n",
      "\u001b[34mSM_HP_REPO_ID=Llama-2-7b-hf-llama1024\u001b[0m\n",
      "\u001b[34mSM_HP_REPORT_TO_WANDB=1\u001b[0m\n",
      "\u001b[34mSM_HP_RUN_NAME=Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15\u001b[0m\n",
      "\u001b[34mSM_HP_TORCH_COMPILE=0\u001b[0m\n",
      "\u001b[34mSM_HP_WANDB_TOKEN=93b4fb1b729b939f257d7db15130b3710cad2ebb\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 run_clm.py --dataset_path /opt/ml/input/data/training --entity ft-llmmm --epochs 3 --gradient_accumulation_steps 16 --gradient_checkpointing 1 --group_by_length 1 --hf_token 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 --hub_strategy every_save --lr 0.0002 --max_steps -1 --merge_weights 0 --model_id meta-llama/Llama-2-7b-hf --optim paged_adamw_8bit --per_device_eval_batch_size 8 --per_device_train_batch_size 8 --project_name SFT_training_dm --repo_id Llama-2-7b-hf-llama1024 --report_to_wandb 1 --run_name Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15 --torch_compile 0 --wandb_token 93b4fb1b729b939f257d7db15130b3710cad2ebb\u001b[0m\n",
      "\u001b[34m2023-08-19 20:18:26,059 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34margs is Namespace(model_id='meta-llama/Llama-2-7b-hf', repo_id='Llama-2-7b-hf-llama1024', hub_strategy='every_save', output_dir='/opt/ml/model', output_data_dir='/opt/ml/output/data', dataset_path='/opt/ml/input/data/training', hf_token='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', report_to_wandb=1, wandb_token='93b4fb1b729b939f257d7db15130b3710cad2ebb', epochs=3, max_steps=-1, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=16, max_seq_length=4096, logging_steps=20, optim='paged_adamw_8bit', lr=0.0002, lora_r=64, lora_alpha=16, weight_decay=0.1, lora_dropout=0.1, load_in_4bit=1, load_in_8bit=0, use_peft=1, gradient_checkpointing=1, bf16=1, group_by_length=1, merge_weights=0, seed=42, warmup_ratio=0.03, project_name='SFT_training_dm', entity='ft-llmmm', run_name='Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15', load_best_model_at_end=1, use_sagemaker=1, torch_compile=0)\u001b[0m\n",
      "\u001b[34mextra is []\u001b[0m\n",
      "\u001b[34mLogging into the Hugging Face Hub with token hf_dZJsCiE...\u001b[0m\n",
      "\u001b[34mToken will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\u001b[0m\n",
      "\u001b[34mToken is valid.\u001b[0m\n",
      "\u001b[34mYour token has been saved to /root/.cache/huggingface/token\u001b[0m\n",
      "\u001b[34mLogin successful\u001b[0m\n",
      "\u001b[34mNamespace(model_id='meta-llama/Llama-2-7b-hf', repo_id='Llama-2-7b-hf-llama1024', hub_strategy='every_save', output_dir='/opt/ml/model', output_data_dir='/opt/ml/output/data', dataset_path='/opt/ml/input/data/training', hf_token='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', report_to_wandb=1, wandb_token='93b4fb1b729b939f257d7db15130b3710cad2ebb', epochs=3, max_steps=-1, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=16, max_seq_length=4096, logging_steps=20, optim='paged_adamw_8bit', lr=0.0002, lora_r=64, lora_alpha=16, weight_decay=0.1, lora_dropout=0.1, load_in_4bit=1, load_in_8bit=0, use_peft=1, gradient_checkpointing=1, bf16=1, group_by_length=1, merge_weights=0, seed=42, warmup_ratio=0.03, project_name='SFT_training_dm', entity='ft-llmmm', run_name='Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15', load_best_model_at_end=1, use_sagemaker=1, torch_compile=0)\u001b[0m\n",
      "\u001b[34mwandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: dmeltzer (ft-llmmm). Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.15.8\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20230819_201830-Llama-2-7b-hf-llama1024-qlora-spot-2023-08-19-20-08-16-512-7l5m2y-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run Llama-2-7b-hf_llama1024_qlora-spot__2023-08-19-20-08-15_r_64_alpha_16\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/ft-llmmm/SFT_training_dm\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/ft-llmmm/SFT_training_dm/runs/Llama-2-7b-hf-llama1024-qlora-spot-2023-08-19-20-08-16-512-7l5m2y-algo-1\u001b[0m\n",
      "\u001b[34mloading from /opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 609/609 [00:00<00:00, 4.01MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)fetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 89.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 41.9M/9.98G [00:00<00:25, 391MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|          | 83.9M/9.98G [00:00<00:25, 393MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|▏         | 126M/9.98G [00:00<00:24, 398MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 178M/9.98G [00:00<00:23, 414MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   2%|▏         | 220M/9.98G [00:00<00:23, 411MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 273M/9.98G [00:00<00:23, 421MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 325M/9.98G [00:00<00:22, 430MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 377M/9.98G [00:00<00:21, 441MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 430M/9.98G [00:01<00:21, 451MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▍         | 482M/9.98G [00:01<00:21, 448MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   5%|▌         | 535M/9.98G [00:01<00:20, 453MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 587M/9.98G [00:01<00:20, 448MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▋         | 640M/9.98G [00:01<00:22, 421MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 692M/9.98G [00:01<00:22, 410MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 734M/9.98G [00:01<00:22, 405MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 776M/9.98G [00:01<00:23, 395MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   8%|▊         | 818M/9.98G [00:01<00:23, 386MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▊         | 860M/9.98G [00:02<00:23, 388MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 902M/9.98G [00:02<00:25, 362MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 944M/9.98G [00:02<00:27, 326MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|▉         | 986M/9.98G [00:02<00:28, 314MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  10%|█         | 1.03G/9.98G [00:02<00:36, 248MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.07G/9.98G [00:02<00:33, 268MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 1.11G/9.98G [00:03<00:30, 288MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.15G/9.98G [00:03<00:29, 304MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.20G/9.98G [00:03<00:26, 331MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 1.24G/9.98G [00:03<00:25, 347MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.28G/9.98G [00:03<00:23, 363MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  13%|█▎        | 1.32G/9.98G [00:03<00:23, 375MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▎        | 1.36G/9.98G [00:03<00:22, 382MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 1.41G/9.98G [00:03<00:22, 389MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 1.45G/9.98G [00:03<00:21, 392MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▍        | 1.49G/9.98G [00:03<00:21, 391MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 1.53G/9.98G [00:04<00:21, 396MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.57G/9.98G [00:04<00:20, 401MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  16%|█▌        | 1.61G/9.98G [00:04<00:20, 402MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.66G/9.98G [00:04<00:20, 402MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.70G/9.98G [00:04<00:20, 403MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 1.74G/9.98G [00:04<00:20, 405MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.78G/9.98G [00:04<00:20, 405MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 1.82G/9.98G [00:04<00:20, 402MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▊        | 1.87G/9.98G [00:04<00:19, 406MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  19%|█▉        | 1.91G/9.98G [00:05<00:19, 405MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 1.95G/9.98G [00:05<00:19, 405MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 1.99G/9.98G [00:05<00:19, 405MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|██        | 2.03G/9.98G [00:05<00:19, 406MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.08G/9.98G [00:05<00:19, 407MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██        | 2.12G/9.98G [00:05<00:19, 406MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.16G/9.98G [00:05<00:19, 408MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  22%|██▏       | 2.21G/9.98G [00:05<00:18, 413MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.26G/9.98G [00:05<00:18, 415MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 2.31G/9.98G [00:05<00:18, 414MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▎       | 2.36G/9.98G [00:06<00:18, 417MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.40G/9.98G [00:06<00:18, 417MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 2.44G/9.98G [00:06<00:18, 417MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 2.50G/9.98G [00:06<00:17, 417MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.55G/9.98G [00:06<00:17, 420MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  26%|██▌       | 2.60G/9.98G [00:06<00:17, 420MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.65G/9.98G [00:06<00:17, 425MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 2.71G/9.98G [00:06<00:16, 433MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.76G/9.98G [00:07<00:16, 444MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 2.81G/9.98G [00:07<00:15, 449MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▊       | 2.86G/9.98G [00:07<00:15, 453MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 2.92G/9.98G [00:07<00:15, 451MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|██▉       | 2.97G/9.98G [00:07<00:15, 456MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  30%|███       | 3.02G/9.98G [00:07<00:15, 460MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 3.07G/9.98G [00:07<00:14, 468MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███▏      | 3.12G/9.98G [00:07<00:14, 471MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.18G/9.98G [00:07<00:14, 468MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 3.23G/9.98G [00:08<00:13, 483MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 3.29G/9.98G [00:08<00:13, 506MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▎      | 3.36G/9.98G [00:08<00:12, 516MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 3.42G/9.98G [00:08<00:12, 521MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  35%|███▍      | 3.48G/9.98G [00:08<00:12, 525MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.54G/9.98G [00:08<00:12, 495MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 3.61G/9.98G [00:08<00:12, 509MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.67G/9.98G [00:08<00:12, 520MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 3.72G/9.98G [00:08<00:13, 476MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.77G/9.98G [00:09<00:13, 458MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 3.83G/9.98G [00:09<00:13, 451MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.88G/9.98G [00:09<00:13, 437MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 3.93G/9.98G [00:09<00:13, 438MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|███▉      | 3.98G/9.98G [00:09<00:13, 441MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  40%|████      | 4.04G/9.98G [00:09<00:13, 445MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 4.09G/9.98G [00:09<00:13, 447MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.14G/9.98G [00:09<00:12, 459MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  42%|████▏     | 4.19G/9.98G [00:10<00:19, 302MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.25G/9.98G [00:10<00:16, 341MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 4.30G/9.98G [00:10<00:15, 374MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▎     | 4.35G/9.98G [00:10<00:13, 403MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 4.40G/9.98G [00:10<00:12, 430MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▍     | 4.46G/9.98G [00:10<00:12, 451MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  45%|████▌     | 4.51G/9.98G [00:10<00:11, 466MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 4.56G/9.98G [00:11<00:12, 446MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▋     | 4.62G/9.98G [00:11<00:11, 472MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.68G/9.98G [00:11<00:11, 478MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 4.73G/9.98G [00:11<00:10, 487MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.78G/9.98G [00:11<00:10, 491MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  48%|████▊     | 4.83G/9.98G [00:11<00:10, 495MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.89G/9.98G [00:11<00:13, 364MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 4.93G/9.98G [00:11<00:13, 375MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|████▉     | 4.98G/9.98G [00:11<00:12, 410MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 5.03G/9.98G [00:12<00:11, 436MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  51%|█████     | 5.09G/9.98G [00:12<00:11, 435MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.14G/9.98G [00:12<00:12, 400MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 5.19G/9.98G [00:12<00:11, 430MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.24G/9.98G [00:12<00:10, 450MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 5.30G/9.98G [00:12<00:10, 451MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▎    | 5.36G/9.98G [00:12<00:09, 478MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 5.41G/9.98G [00:12<00:09, 489MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▍    | 5.46G/9.98G [00:13<00:09, 494MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  55%|█████▌    | 5.53G/9.98G [00:13<00:08, 505MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 5.58G/9.98G [00:13<00:08, 510MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.64G/9.98G [00:13<00:08, 517MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 5.69G/9.98G [00:13<00:08, 517MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.75G/9.98G [00:13<00:08, 504MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  58%|█████▊    | 5.81G/9.98G [00:13<00:08, 517MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.87G/9.98G [00:13<00:07, 528MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 5.93G/9.98G [00:13<00:07, 536MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  60%|██████    | 6.00G/9.98G [00:14<00:09, 439MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.05G/9.98G [00:14<00:08, 458MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 6.10G/9.98G [00:14<00:08, 468MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.17G/9.98G [00:14<00:07, 482MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  62%|██████▏   | 6.23G/9.98G [00:14<00:07, 496MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 6.29G/9.98G [00:14<00:07, 511MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▎   | 6.35G/9.98G [00:14<00:06, 519MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 6.41G/9.98G [00:14<00:07, 455MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▍   | 6.47G/9.98G [00:15<00:07, 482MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  65%|██████▌   | 6.53G/9.98G [00:15<00:06, 502MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 6.60G/9.98G [00:15<00:06, 521MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.66G/9.98G [00:15<00:06, 525MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  67%|██████▋   | 6.72G/9.98G [00:15<00:06, 527MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 6.78G/9.98G [00:15<00:06, 523MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▊   | 6.85G/9.98G [00:15<00:05, 535MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  69%|██████▉   | 6.91G/9.98G [00:15<00:05, 539MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 6.97G/9.98G [00:15<00:05, 533MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.04G/9.98G [00:16<00:05, 536MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  71%|███████   | 7.10G/9.98G [00:16<00:05, 542MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.16G/9.98G [00:16<00:05, 523MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 7.22G/9.98G [00:16<00:05, 524MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 7.29G/9.98G [00:16<00:05, 521MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▎  | 7.35G/9.98G [00:16<00:04, 531MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  74%|███████▍  | 7.41G/9.98G [00:16<00:04, 531MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▍  | 7.48G/9.98G [00:16<00:04, 521MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 7.53G/9.98G [00:17<00:05, 458MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  76%|███████▌  | 7.58G/9.98G [00:17<00:05, 468MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.63G/9.98G [00:17<00:04, 476MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 7.69G/9.98G [00:17<00:04, 483MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.75G/9.98G [00:17<00:04, 501MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  78%|███████▊  | 7.81G/9.98G [00:17<00:04, 519MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 7.87G/9.98G [00:17<00:03, 530MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|███████▉  | 7.94G/9.98G [00:17<00:03, 532MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  80%|████████  | 8.00G/9.98G [00:17<00:03, 537MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 8.06G/9.98G [00:18<00:03, 541MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████▏ | 8.13G/9.98G [00:18<00:03, 545MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 8.19G/9.98G [00:18<00:03, 538MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.25G/9.98G [00:18<00:03, 529MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  83%|████████▎ | 8.32G/9.98G [00:18<00:03, 473MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.37G/9.98G [00:18<00:03, 481MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 8.42G/9.98G [00:18<00:03, 487MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▍ | 8.47G/9.98G [00:18<00:03, 434MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  85%|████████▌ | 8.52G/9.98G [00:19<00:03, 451MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 8.58G/9.98G [00:19<00:03, 457MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.63G/9.98G [00:19<00:02, 460MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  87%|████████▋ | 8.68G/9.98G [00:19<00:02, 433MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.73G/9.98G [00:19<00:02, 450MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 8.79G/9.98G [00:19<00:02, 451MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▊ | 8.84G/9.98G [00:19<00:02, 454MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  89%|████████▉ | 8.89G/9.98G [00:19<00:02, 449MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 8.95G/9.98G [00:20<00:02, 477MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|█████████ | 9.01G/9.98G [00:20<00:01, 488MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████ | 9.07G/9.98G [00:20<00:01, 508MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.13G/9.98G [00:20<00:01, 519MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  92%|█████████▏| 9.20G/9.98G [00:20<00:01, 529MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.26G/9.98G [00:20<00:01, 540MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 9.32G/9.98G [00:20<00:01, 535MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  94%|█████████▍| 9.38G/9.98G [00:20<00:01, 523MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 9.45G/9.98G [00:20<00:01, 525MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▌| 9.51G/9.98G [00:21<00:00, 537MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▌| 9.57G/9.98G [00:21<00:00, 497MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  96%|█████████▋| 9.63G/9.98G [00:21<00:00, 490MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 9.68G/9.98G [00:21<00:00, 480MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.73G/9.98G [00:21<00:00, 475MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  98%|█████████▊| 9.78G/9.98G [00:21<00:00, 457MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▊| 9.84G/9.98G [00:21<00:00, 306MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▉| 9.89G/9.98G [00:22<00:00, 344MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|█████████▉| 9.94G/9.98G [00:22<00:00, 382MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 9.98G/9.98G [00:22<00:00, 448MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 1/2 [00:22<00:22, 22.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   1%|▏         | 52.4M/3.50G [00:00<00:07, 456MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   3%|▎         | 105M/3.50G [00:00<00:06, 489MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   4%|▍         | 157M/3.50G [00:00<00:06, 503MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   6%|▌         | 210M/3.50G [00:00<00:06, 507MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   7%|▋         | 262M/3.50G [00:00<00:06, 507MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:   9%|▉         | 325M/3.50G [00:00<00:06, 519MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  11%|█         | 377M/3.50G [00:00<00:06, 511MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  12%|█▏        | 430M/3.50G [00:00<00:06, 504MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  14%|█▍        | 482M/3.50G [00:00<00:06, 461MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  15%|█▌        | 535M/3.50G [00:01<00:06, 460MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  17%|█▋        | 587M/3.50G [00:01<00:06, 469MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  18%|█▊        | 640M/3.50G [00:01<00:06, 438MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  20%|█▉        | 692M/3.50G [00:01<00:06, 436MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  21%|██▏       | 744M/3.50G [00:01<00:06, 458MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  23%|██▎       | 797M/3.50G [00:01<00:06, 409MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  24%|██▍       | 849M/3.50G [00:01<00:07, 359MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  25%|██▌       | 891M/3.50G [00:02<00:07, 366MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  27%|██▋       | 933M/3.50G [00:02<00:07, 321MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  28%|██▊       | 975M/3.50G [00:02<00:07, 334MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  29%|██▉       | 1.03G/3.50G [00:02<00:06, 360MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  31%|███       | 1.08G/3.50G [00:02<00:06, 381MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  32%|███▏      | 1.12G/3.50G [00:02<00:06, 383MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  33%|███▎      | 1.16G/3.50G [00:02<00:05, 390MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  34%|███▍      | 1.21G/3.50G [00:02<00:05, 395MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  36%|███▌      | 1.25G/3.50G [00:02<00:05, 401MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  37%|███▋      | 1.29G/3.50G [00:03<00:05, 405MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  38%|███▊      | 1.33G/3.50G [00:03<00:05, 406MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  39%|███▉      | 1.37G/3.50G [00:03<00:05, 409MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  41%|████      | 1.43G/3.50G [00:03<00:04, 436MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  43%|████▎     | 1.49G/3.50G [00:03<00:04, 470MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  44%|████▍     | 1.55G/3.50G [00:03<00:03, 495MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  46%|████▌     | 1.60G/3.50G [00:03<00:04, 464MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  47%|████▋     | 1.66G/3.50G [00:03<00:04, 397MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  49%|████▉     | 1.71G/3.50G [00:04<00:04, 402MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  50%|█████     | 1.76G/3.50G [00:04<00:04, 410MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  52%|█████▏    | 1.80G/3.50G [00:04<00:04, 394MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  53%|█████▎    | 1.85G/3.50G [00:04<00:04, 390MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  54%|█████▍    | 1.89G/3.50G [00:04<00:04, 393MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  56%|█████▌    | 1.95G/3.50G [00:04<00:03, 437MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  57%|█████▋    | 2.00G/3.50G [00:04<00:03, 440MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  59%|█████▉    | 2.07G/3.50G [00:04<00:03, 476MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  61%|██████    | 2.13G/3.50G [00:04<00:02, 501MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  63%|██████▎   | 2.19G/3.50G [00:05<00:02, 520MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  64%|██████▍   | 2.25G/3.50G [00:05<00:02, 534MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  66%|██████▌   | 2.32G/3.50G [00:05<00:02, 544MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  68%|██████▊   | 2.38G/3.50G [00:05<00:02, 554MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  70%|██████▉   | 2.44G/3.50G [00:05<00:01, 552MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  72%|███████▏  | 2.51G/3.50G [00:05<00:02, 472MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  73%|███████▎  | 2.57G/3.50G [00:05<00:01, 495MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  75%|███████▌  | 2.63G/3.50G [00:05<00:01, 510MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  77%|███████▋  | 2.69G/3.50G [00:06<00:01, 518MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  79%|███████▉  | 2.76G/3.50G [00:06<00:01, 529MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  81%|████████  | 2.82G/3.50G [00:06<00:01, 536MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  82%|████████▏ | 2.88G/3.50G [00:06<00:01, 531MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  84%|████████▍ | 2.95G/3.50G [00:06<00:01, 537MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  86%|████████▌ | 3.01G/3.50G [00:06<00:00, 536MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  88%|████████▊ | 3.07G/3.50G [00:06<00:00, 516MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  90%|████████▉ | 3.14G/3.50G [00:06<00:00, 520MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  91%|█████████▏| 3.20G/3.50G [00:06<00:00, 525MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  93%|█████████▎| 3.26G/3.50G [00:07<00:00, 535MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  95%|█████████▍| 3.32G/3.50G [00:07<00:00, 540MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  97%|█████████▋| 3.39G/3.50G [00:07<00:00, 541MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors:  99%|█████████▊| 3.45G/3.50G [00:07<00:00, 521MB/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mDownloading (…)of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:07<00:00, 464MB/s]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:29<00:00, 13.64s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 2/2 [00:29<00:00, 14.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json: 100%|██████████| 188/188 [00:00<00:00, 782kB/s]\u001b[0m\n",
      "\u001b[34mFound 7 modules to quantize: ['o_proj', 'up_proj', 'v_proj', 'k_proj', 'q_proj', 'down_proj', 'gate_proj']\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 3.79MB/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 454MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)/main/tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 39.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 2.18MB/s]\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mUsing pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/71305 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 1000/71305 [00:00<00:10, 6937.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 3000/71305 [00:00<00:07, 9212.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 5000/71305 [00:00<00:06, 9870.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 7000/71305 [00:00<00:06, 10315.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 9000/71305 [00:00<00:06, 10334.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 11000/71305 [00:01<00:05, 10505.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 13000/71305 [00:01<00:05, 10582.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 15000/71305 [00:01<00:05, 10737.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 17000/71305 [00:01<00:05, 10757.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 19000/71305 [00:01<00:04, 10790.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 21000/71305 [00:02<00:04, 10745.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 23000/71305 [00:02<00:04, 10746.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 25000/71305 [00:02<00:05, 8889.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 27000/71305 [00:02<00:04, 9456.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 29000/71305 [00:02<00:04, 9679.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 31000/71305 [00:03<00:04, 9896.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▋     | 33000/71305 [00:03<00:03, 10077.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 35000/71305 [00:03<00:03, 10165.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 37000/71305 [00:03<00:03, 10264.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▍    | 39000/71305 [00:03<00:03, 10326.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 41000/71305 [00:04<00:02, 10414.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 43000/71305 [00:04<00:02, 10492.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 45000/71305 [00:04<00:02, 10532.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 47000/71305 [00:04<00:02, 10533.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▊   | 49000/71305 [00:04<00:02, 10659.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 51000/71305 [00:04<00:01, 10603.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 53000/71305 [00:05<00:01, 10665.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 55000/71305 [00:05<00:01, 10688.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 57000/71305 [00:05<00:01, 10739.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 59000/71305 [00:05<00:01, 8939.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 61000/71305 [00:06<00:01, 9437.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 63000/71305 [00:06<00:00, 9764.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 65000/71305 [00:06<00:00, 10104.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 67000/71305 [00:06<00:00, 10243.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 69000/71305 [00:06<00:00, 10369.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 71000/71305 [00:06<00:00, 10490.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1948 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1948/1948 [00:00<00:00, 11513.09 examples/s]\u001b[0m\n",
      "\u001b[34mCloning https://huggingface.co/dhmeltzer/Llama-2-7b-hf-llama1024_r_64_alpha_16 into local empty directory.\u001b[0m\n",
      "\u001b[34mTo https://huggingface.co/dhmeltzer/Llama-2-7b-hf-llama1024_r_64_alpha_16\n",
      "   a27e12c..e4440b9  main -> main\u001b[0m\n",
      "\u001b[34m0%|          | 0/1671 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\u001b[0m\n",
      "\u001b[34mTo disable this warning, you can either:\u001b[0m\n",
      "\u001b[34m#011- Avoid using `tokenizers` before the fork if possible\u001b[0m\n",
      "\u001b[34m#011- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\u001b[0m\n",
      "\u001b[34m0%|          | 1/1671 [02:51<79:27:32, 171.29s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/1671 [04:52<65:51:02, 142.04s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker-us-east-1-338563806291\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# EC2 + FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project='SFT_Training_dm',\n",
    "                 entity='ft-llmmm',\n",
    "                 job_type='download_data',\n",
    "                 name='download_SFT_EC2') as run:\n",
    "    artifact = run.use_artifact('ft-llmmm/ELI5_analysis/llama_QA_tokenized:v2', type='dataset')\n",
    "    artifact_dir = artifact.download()\n",
    "    \n",
    "    print(f'artifact saved to {artifact_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_QA_SFT = datasets.load_from_disk(f'./{artifact_dir}')\n",
    "ds_wiki = ds_QA_SFT.filter(lambda x:x['source']=='simple_wiki')\n",
    "ds_ELI5 = ds_QA_SFT.filter(lambda x:x['source']=='ELI5')\n",
    "\n",
    "ds_wiki.save_to_disk('./data/ds_wiki')\n",
    "ds_ELI5.save_to_disk('./data/ds_ELI5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "model_name = model_id.split('/')[-1]\n",
    "ds_name = 'wiki'\n",
    "\n",
    "if ds_name == 'combined':\n",
    "    dataset_path = artifact_dir\n",
    "else:\n",
    "    dataset_path = f'./data/ds_{ds_name}'\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora_flash'\n",
    "run_name += f'__{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "output_dir = f'./{model_name}_{ds_name}/models'\n",
    "logging_dir = f'./{model_name}_{ds_name}/logs'\n",
    "repo_id = f'{model_name}-{ds_name}-flash'\n",
    "report_to_wandb = 1\n",
    "epochs = 3\n",
    "max_steps = -1\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "gradient_accumulation_steps = 16\n",
    "lr = 2e-4\n",
    "merge_weights = 0\n",
    "entity = 'ft-llmmm'\n",
    "project_name = 'SFT_training_dm'\n",
    "hub_strategy = 'every_save'\n",
    "torch_compile = 0\n",
    "gradient_checkpointing = 1\n",
    "optim = 'paged_adamw_8bit'\n",
    "group_by_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ['HUGGINGFACE_TOKEN']\n",
    "wandb_token = os.environ['WANDB_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python ./run_clm.py \\\n",
    "--output_dir './Llama-2-7b-hf_wiki/models' \\\n",
    "--logging_dir './Llama-2-7b-hf_wiki/logs' \\\n",
    "--model_id 'meta-llama/Llama-2-7b-hf' \\\n",
    "--dataset_path './data/ds_wiki' \\\n",
    "--run_name 'Llama-2-7b-hf_wiki_qlora_no_flash_test' \\\n",
    "--repo_id 'Llama-2-7b-hf-wiki-flash' \\\n",
    "--report_to_wandb 1 \\\n",
    "--epochs 3 \\\n",
    "--max_steps -1 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--per_device_eval_batch_size 8 \\\n",
    "--gradient_accumulation_steps 16 \\\n",
    "--lr 2e-4 \\\n",
    "--merge_weights 0 \\\n",
    "--entity 'ft-llmmm' \\\n",
    "--project_name 'SFT_training_dm' \\\n",
    "--hub_strategy 'every_save' \\\n",
    "--torch_compile 0 \\\n",
    "--gradient_checkpointing 1 \\\n",
    "--optim 'paged_adamw_8bit' \\\n",
    "--group_by_length 1 \\\n",
    "--hf_token hf_token \\\n",
    "--wandb_token wandb_token \\\n",
    "--use_flash_attention 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
