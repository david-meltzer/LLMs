{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#wandb.finish()\n",
    "!pip install --upgrade pip\n",
    "!pip install \"transformers==4.30.2\" \n",
    "!pip install \"datasets[s3]==2.13.0\" \n",
    "!pip install sagemaker --upgrade \n",
    "!pip install wandb\n",
    "#!pip install \"transformers==4.30.2\" --upgrade\n",
    "#!pip3 install git+https://github.com/huggingface/transformers\n",
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "import torch\n",
    "##import wandb\n",
    "from huggingface_hub import login\n",
    "from transformers import LlamaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "import wandb\n",
    "\n",
    "hf_token=None\n",
    "wandb_token=None\n",
    "\n",
    "hf_token = getpass('input hf token')\n",
    "wandb_token = getpass('input wandb token')\n",
    "\n",
    "login(token=hf_token)\n",
    "wandb.login(key=wandb_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket='llms-hf'\n",
    "\n",
    "iam = boto3.client('iam')\n",
    "role = iam.get_role(RoleName='Sagemaker-DataScientist')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "#artifact_dir='artifacts/combined_dataset:v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
    "with wandb.init(project='SFT_training_DM',\n",
    "                entity='ft-llmmm',\n",
    "                job_type='download_data',\n",
    "                name=f'download_combined_data_{time_stamp}') as run:\n",
    "\n",
    "    artifact = run.use_artifact('ft-llmmm/ELI5_analysis/combined_dataset:v2', type='dataset')\n",
    "    artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artifact_dir='./artifacts/combined_dataset:v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "ds_QA_SFT = load_from_disk(f'./{artifact_dir}')\n",
    "ds_wiki = ds_QA_SFT.filter(lambda x:x['source']=='simple_wiki')\n",
    "ds_ELI5 = ds_QA_SFT.filter(lambda x:x['source']=='ELI5')\n",
    "\n",
    "ds_name='wiki'\n",
    "\n",
    "if ds_name == 'combined':\n",
    "    ds = ds_QA_SFT\n",
    "elif ds_name == 'wiki':\n",
    "    ds = ds_wiki\n",
    "elif ds_name == 'ELI5':\n",
    "    ds = ds_ELI5\n",
    "else:\n",
    "    raise ValueError(\"Input valid dataset name, 'combined','ELI5', or 'wiki'\")\n",
    "\n",
    "training_input_path = f's3://{sagemaker_session_bucket}/{ds_name}'\n",
    "#ds.save_to_disk(training_input_path)\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora'\n",
    "run_name += f'__{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name = f'{model_name}_{ds_name}-qlora'.replace('_','-')\n",
    "\n",
    "hyperparameters ={\n",
    "    'model_id': model_id,\n",
    "    'repo_id': f'{model_name}-{ds_name}-no-group-by-length',\n",
    "    'dataset_path': '/opt/ml/input/data/training',\n",
    "    'hf_token': hf_token,\n",
    "    'wandb_token': wandb_token,\n",
    "    'report_to_wandb': 1,\n",
    "    'epochs': 3,\n",
    "    'max_steps': -1,\n",
    "    'per_device_train_batch_size': 8,\n",
    "    'per_device_eval_batch_size': 8,\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'lr': 2e-4,\n",
    "    'merge_weights': 0,\n",
    "    'entity': 'ft-llmmm',\n",
    "    'project_name': 'SFT_training_dm',\n",
    "    'hub_strategy': 'every_save',\n",
    "    'run_name': run_name,\n",
    "    'torch_compile': 0,\n",
    "    'gradient_checkpointing': 1,\n",
    "    'optim': 'paged_adamw_8bit',\n",
    "    'group_by_length':0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = './scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    #transformers_version = '4.30',\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC2 + FlashAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.31.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: datasets==2.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: peft==0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: accelerate==0.21.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes==0.40.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.40.2)\n",
      "Requirement already satisfied: trl==0.4.7 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.4.7)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers==4.31.0) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.13.0) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.13.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.13.0) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.13.0) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.13.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.13.0) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets==2.13.0) (3.8.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.4.0) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from peft==0.4.0) (2.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.31.0) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.31.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers==4.31.0) (2023.7.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets==2.13.0) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.13.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.31.0\" \"datasets==2.13.0\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.15.8)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (8.1.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (3.1.32)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (1.29.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from wandb) (4.24.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/qlora_SFT_new/wandb/run-20230816_194231-rhvfjme0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/SFT_training_dm/runs/rhvfjme0' target=\"_blank\">download_SFT_EC2</a></strong> to <a href='https://wandb.ai/ft-llmmm/SFT_training_dm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ft-llmmm/SFT_training_dm' target=\"_blank\">https://wandb.ai/ft-llmmm/SFT_training_dm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ft-llmmm/SFT_training_dm/runs/rhvfjme0' target=\"_blank\">https://wandb.ai/ft-llmmm/SFT_training_dm/runs/rhvfjme0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact llama_QA_tokenized:v2, 206.21MB. 10 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   10 of 10 files downloaded.  \n",
      "Done. 0:0:0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact saved to ./artifacts/llama_QA_tokenized:v2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">download_SFT_EC2</strong> at: <a href='https://wandb.ai/ft-llmmm/SFT_training_dm/runs/rhvfjme0' target=\"_blank\">https://wandb.ai/ft-llmmm/SFT_training_dm/runs/rhvfjme0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230816_194231-rhvfjme0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project='SFT_Training_dm',\n",
    "                 entity='ft-llmmm',\n",
    "                 job_type='download_data',\n",
    "                 name='download_SFT_EC2') as run:\n",
    "    artifact = run.use_artifact('ft-llmmm/ELI5_analysis/llama_QA_tokenized:v2', type='dataset')\n",
    "    artifact_dir = artifact.download()\n",
    "    \n",
    "    print(f'artifact saved to {artifact_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/72214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3301 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/72214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3301 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/42214 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/964 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2301 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_QA_SFT = datasets.load_from_disk(f'./{artifact_dir}')\n",
    "ds_wiki = ds_QA_SFT.filter(lambda x:x['source']=='simple_wiki')\n",
    "ds_ELI5 = ds_QA_SFT.filter(lambda x:x['source']=='ELI5')\n",
    "\n",
    "ds_wiki.save_to_disk('./data/ds_wiki')\n",
    "ds_ELI5.save_to_disk('./data/ds_ELI5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-hf\" # sharded weights\n",
    "model_name = model_id.split('/')[-1]\n",
    "ds_name = 'wiki'\n",
    "\n",
    "if ds_name == 'combined':\n",
    "    dataset_path = artifact_dir\n",
    "else:\n",
    "    dataset_path = f'./data/ds_{ds_name}'\n",
    "\n",
    "run_name = f'{model_name}_{ds_name}_qlora_flash'\n",
    "run_name += f'__{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "output_dir = f'./{model_name}_{ds_name}/models'\n",
    "logging_dir = f'./{model_name}_{ds_name}/logs'\n",
    "repo_id = f'{model_name}-{ds_name}-flash'\n",
    "report_to_wandb = 1\n",
    "epochs = 3\n",
    "max_steps = -1\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "gradient_accumulation_steps = 16\n",
    "lr = 2e-4\n",
    "merge_weights = 0\n",
    "entity = 'ft-llmmm'\n",
    "project_name = 'SFT_training_dm'\n",
    "hub_strategy = 'every_save'\n",
    "torch_compile = 0\n",
    "gradient_checkpointing = 1\n",
    "optim = 'paged_adamw_8bit'\n",
    "group_by_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = os.environ['HUGGINGFACE_TOKEN']\n",
    "wandb_token = os.environ['WANDB_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args is Namespace(model_id='meta-llama/Llama-2-7b-hf', repo_id='Llama-2-7b-hf-wiki-flash', hub_strategy='every_save', output_dir='./Llama-2-7b-hf_wiki/models', output_data_dir=None, dataset_path='./data/ds_wiki', hf_token='hf_token', report_to_wandb=1, wandb_token='802aabee3bd60e8c7f1a3e849e7496c6b43e0886', epochs=3, max_steps=-1, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=16, max_seq_length=4096, logging_steps=20, optim='paged_adamw_8bit', lr=0.0002, lora_r=64, lora_alpha=16, weight_decay=0.1, lora_dropout=0.1, load_in_4bit=1, load_in_8bit=0, use_peft=1, gradient_checkpointing=1, bf16=1, group_by_length=1, merge_weights=0, seed=42, warmup_ratio=0.03, project_name='SFT_training_dm', entity='ft-llmmm', run_name='Llama-2-7b-hf_wiki_qlora_flash_test', load_best_model_at_end=1, use_sagemaker=1, torch_compile=0, use_flash_attention=1)\n",
      "extra is ['--logging_dir', './Llama-2-7b-hf_wiki/logs']\n",
      "Logging into the Hugging Face Hub with token hf_token...\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid token passed!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/qlora_SFT_new/run_clm.py:601\u001b[0m\n\u001b[1;32m    597\u001b[0m     training_function(args)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 601\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m#        model.push_to_hub(args.repo_id)\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m#        tokenizer.push_to_hub(args.repo_id)\u001b[39;00m\n\u001b[1;32m    606\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;66;03m#                                          use_auth_token=args.hf_token)\u001b[39;00m\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;66;03m#tokenizer.save_pretrained(save_dir)\u001b[39;00m\n",
      "File \u001b[0;32m~/qlora_SFT_new/run_clm.py:594\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 594\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mparse_arge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[1;32m    597\u001b[0m     training_function(args)\n",
      "File \u001b[0;32m~/qlora_SFT_new/run_clm.py:306\u001b[0m, in \u001b[0;36mparse_arge\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mhf_token:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogging into the Hugging Face Hub with token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mhf_token[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 306\u001b[0m     \u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mload_in_8bit \u001b[38;5;129;01mand\u001b[39;00m args\u001b[38;5;241m.\u001b[39mload_in_4bit:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the model in 8 bits and 4 bits at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/_login.py:96\u001b[0m, in \u001b[0;36mlogin\u001b[0;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m add_to_git_credential:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken will not been saved to git credential helper. Pass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `add_to_git_credential=True` if you want to set the git\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m credential as well.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m         )\n\u001b[0;32m---> 96\u001b[0m     \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_git_credential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_to_git_credential\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_permission\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_permission\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_notebook():\n\u001b[1;32m     98\u001b[0m     notebook_login(new_session\u001b[38;5;241m=\u001b[39mnew_session, write_permission\u001b[38;5;241m=\u001b[39mwrite_permission)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/_login.py:275\u001b[0m, in \u001b[0;36m_login\u001b[0;34m(token, add_to_git_credential, write_permission)\u001b[0m\n\u001b[1;32m    273\u001b[0m permission \u001b[38;5;241m=\u001b[39m get_token_permission(token)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m permission \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid token passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m write_permission \u001b[38;5;129;01mand\u001b[39;00m permission \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is valid but is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread-only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m token is required.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease provide a new token with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m correct permission.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid token passed!"
     ]
    }
   ],
   "source": [
    "%python ./run_clm.py \\\n",
    "--output_dir './Llama-2-7b-hf_wiki/models' \\\n",
    "--logging_dir './Llama-2-7b-hf_wiki/logs' \\\n",
    "--model_id 'meta-llama/Llama-2-7b-hf' \\\n",
    "--dataset_path './data/ds_wiki' \\\n",
    "--run_name 'Llama-2-7b-hf_wiki_qlora_flash_test' \\\n",
    "--repo_id 'Llama-2-7b-hf-wiki-flash' \\\n",
    "--report_to_wandb 1 \\\n",
    "--epochs 3 \\\n",
    "--max_steps -1 \\\n",
    "--per_device_train_batch_size 8 \\\n",
    "--per_device_eval_batch_size 8 \\\n",
    "--gradient_accumulation_steps 16 \\\n",
    "--lr 2e-4 \\\n",
    "--merge_weights 0 \\\n",
    "--entity 'ft-llmmm' \\\n",
    "--project_name 'SFT_training_dm' \\\n",
    "--hub_strategy 'every_save' \\\n",
    "--torch_compile 0 \\\n",
    "--gradient_checkpointing 1 \\\n",
    "--optim 'paged_adamw_8bit' \\\n",
    "--group_by_length 1 \\\n",
    "--hf_token hf_token \\\n",
    "--wandb_token wandb_token \\\n",
    "--use_flash_attention 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
