{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-meltzer/LLMs/blob/main/training/david/SFT/colab_SFT_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlRh0dzyTRo0"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-TmbqMbxyLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b477620-7f91-42fd-c11b-5307a7deecc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/LLMs/Fine-tuning/SFT\n",
            "Requirement already satisfied: peft==0.4.0 in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (4.33.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (0.21.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers->peft==0.4.0) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes==0.41.1 in /usr/local/lib/python3.10/dist-packages (0.41.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.33.1)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.23.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.21.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.14.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->trl) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->trl) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (4.66.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.8.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.9)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.34)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.30.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: accelerate==0.21.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.21.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.23.5)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.33.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.3.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2023.7.22)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=3.0.0->bert_score) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert_score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.1)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.0.1+cu118)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from flash-attn) (23.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from flash-attn) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->flash-attn) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/LLMs/Fine-tuning/SFT\n",
        "\n",
        "# installations\n",
        "#!pip install detoxify\n",
        "\n",
        "!pip install peft==0.4.0\n",
        "!pip install bitsandbytes==0.41.1\n",
        "!pip install safetensors>=0.3.1\n",
        "!pip install trl\n",
        "!pip install wandb\n",
        "!pip install tokenizers>=0.13.3\n",
        "!pip install -U transformers\n",
        "!pip install accelerate==0.21.0\n",
        "!pip install datasets\n",
        "!pip install -U torch\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install nltk\n",
        "!pip install bert_score\n",
        "\n",
        "!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
        "!pip install ninja packaging\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SetPqPtmTEYf"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from google.colab import runtime\n",
        "import pandas as pd\n",
        "\n",
        "import datasets\n",
        "import accelerate\n",
        "import transformers\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForCausalLM,\n",
        "                          Trainer,\n",
        "                          TrainingArguments,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainerCallback)\n",
        "import bitsandbytes as bnb\n",
        "import wandb\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from datetime import datetime\n",
        "from huggingface_hub import login\n",
        "\n",
        "from peft.tuners.lora import LoraLayer\n",
        "import evaluate\n",
        "\n",
        "#from getpass import getpass\n",
        "#hf_token = getpass()\n",
        "#wandb_token = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "83HFBdzV0yfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a367a1-6000-493c-cf80-31e04fe0b88a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "··········\n",
            "··········\n",
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmeltzer\u001b[0m (\u001b[33mft-llmmm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from getpass import getpass\n",
        "hf_token = getpass()\n",
        "wandb_token = getpass()\n",
        "\n",
        "login(hf_token)\n",
        "wandb.login(key=wandb_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjygy89vQITv"
      },
      "source": [
        "# Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5dtefRJQcWe"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nE0Vqvn1QiX3"
      },
      "outputs": [],
      "source": [
        "# setup collator\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['question'])):\n",
        "        text = f\"### Human: {example['question'][i]}\\n ### Assistant: {example['answer'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "def sft_collator(tokenizer, response_template = \" ### Assistant:\"):\n",
        "\n",
        "    return DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)\n",
        "\n",
        "def combine_question_answer(ds,formatting_func):\n",
        "\n",
        "    if 'QA' not in ds['train']:\n",
        "        ds = ds.map(lambda x: {'QA':formatting_func(x)},\n",
        "                    batched=True)\n",
        "    return ds\n",
        "\n",
        "def prepare_dataset(ds,\n",
        "                    tokenizer,\n",
        "                    formatting_func,\n",
        "                    max_seq_length='auto'):\n",
        "\n",
        "    if max_seq_length == 'auto':\n",
        "        max_seq_length = tokenizer.model_max_length\n",
        "\n",
        "    ds = combine_question_answer(ds,formatting_func)\n",
        "\n",
        "    ds = ds.map(lambda x: {'tokens':tokenizer(x['QA'],\n",
        "                                              return_length=False)})\n",
        "\n",
        "    ds = ds.filter(lambda x: len(x['tokens']['input_ids'])<=max_seq_length)\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVQPqQvfiP_j"
      },
      "source": [
        "# Form Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvhKLq9bXygs"
      },
      "source": [
        "## Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "PuVdhm5MigZU",
        "outputId": "3b192f17-6413-48d1-c387-59b13c1e8eee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmeltzer\u001b[0m (\u001b[33mft-llmmm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230822_162613-ud17dzo1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/ud17dzo1' target=\"_blank\">SFT_training</a></strong> to <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/ud17dzo1' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/ud17dzo1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \\ 1 of 3 files downloaded...\r\u001b[34m\u001b[1mwandb\u001b[0m:   3 of 3 files downloaded.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact ELI5_cleaned:latest, 1379.04MB. 24 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   24 of 24 files downloaded.  \n",
            "Done. 0:0:32.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed2ec66d503e4170a775520147d07b65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.010 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.164206…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">SFT_training</strong> at: <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/ud17dzo1' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/ud17dzo1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230822_162613-ud17dzo1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with wandb.init(project='ELI5_analysis',\n",
        "                 entity='ft-llmmm',\n",
        "                 job_type='training',\n",
        "                 name='SFT_training') as run:\n",
        "\n",
        "    artifact_wiki_QA = run.use_artifact('ft-llmmm/ELI5_analysis/simple_wiki_QA:latest', type='dataset')\n",
        "    artifact_dir_wiki_QA = artifact_wiki_QA.download()\n",
        "\n",
        "    artifact_ELI5 = run.use_artifact('ft-llmmm/ELI5_analysis/ELI5_cleaned:latest', type='dataset')\n",
        "    artifact_dir_ELI5 = artifact_ELI5.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSJNQ4UEqzmh"
      },
      "outputs": [],
      "source": [
        "artifact_dir_wiki_QA='./artifacts/simple_wiki_QA:v4'\n",
        "artifact_dir_ELI5='./artifacts/ELI5_cleaned:v5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NygRQl1mbCa"
      },
      "outputs": [],
      "source": [
        "simplewiki_QA_ds = datasets.load_dataset(\"csv\",\n",
        "                                         data_files={\"train\": artifact_dir_wiki_QA + '/simple_wiki_QA_combined_train.csv',\n",
        "                                                    \"test\": artifact_dir_wiki_QA +  '/simple_wiki_QA_combined_test.csv',\n",
        "                                                    \"val\": artifact_dir_wiki_QA + '/simple_wiki_QA_combined_validation.csv'\n",
        "                                        }\n",
        ")\n",
        "simplewiki_QA_ds = simplewiki_QA_ds.remove_columns(['id','system_message','prompt_template'])\n",
        "simplewiki_QA_ds = simplewiki_QA_ds.rename_columns({'trunc_text':'answer'})\n",
        "\n",
        "simplewiki_QA_ds['validation'] = simplewiki_QA_ds['val']\n",
        "del simplewiki_QA_ds['val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cva9r9Q0R__o",
        "outputId": "5054b4b0-9572-42aa-d00c-36bb35a75ab8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Unnamed: 0': 0,\n",
              " 'question': 'What was the impact of the 2005 Kashmir Earthquake?',\n",
              " 'answer': 'The 2005 Kashmir Earthquake (also known as the Great Pakistan earthquake) was a major earthquake centered in Pakistan-administered Kashmir and in Khyber Pakhtunkhwa near the city of Muzaffarabad. It occurred at 08:52:37 Pakistan Standard Time (03:52:37 UTC) on 8 October 2002 87,351 peoples died, 75,266 peoples injuried, and 2.4 million people were left homeless. Kashmir, Pakistan, and Southern part of India were all affected.',\n",
              " 'source': 'simple_wiki'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "simplewiki_QA_ds['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0urp2yuZ8DU"
      },
      "outputs": [],
      "source": [
        "for split in simplewiki_QA_ds:\n",
        "    dset_source = datasets.Dataset.from_dict({'source':['simple_wiki']*len(simplewiki_QA_ds[split])})\n",
        "    simplewiki_QA_ds[split] = datasets.concatenate_datasets([simplewiki_QA_ds[split],dset_source],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3cTYfeZqsfh"
      },
      "outputs": [],
      "source": [
        "ELI5_ds = datasets.load_from_disk(f'{artifact_dir_ELI5}/ds_SFT')\n",
        "ELI5_ds = ELI5_ds.flatten()\n",
        "ELI5_ds = ELI5_ds.remove_columns(['document','q_id','title','selftext','subreddit','url','title_urls','selftext_urls','answers_urls','pref_idxs','dupl_scores_idxs','qu_emb',\n",
        "                                  'answers.a_id','answers.fkg','answers.fre','answers.score'])\n",
        "ELI5_ds = ELI5_ds.map(lambda x: {'answers.text':list(x['answers.text'])})\n",
        "\n",
        "ELI5_ds = ELI5_ds.with_format(\"pandas\").map(lambda df:\n",
        "                                                df.explode(\"answers.text\"),\n",
        "                                                batched=True)\n",
        "\n",
        "ELI5_ds = ELI5_ds.with_format(None)\n",
        "\n",
        "ELI5_ds = ELI5_ds.remove_columns(['__index_level_0__'])\n",
        "ELI5_ds = ELI5_ds.rename_columns({'answers.text':'answer',\n",
        "                                  'title_body':'question'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7-r3wefZhuv"
      },
      "outputs": [],
      "source": [
        "for split in ELI5_ds:\n",
        "    dset_source = datasets.Dataset.from_dict({'source':['ELI5']*len(ELI5_ds[split])})\n",
        "    ELI5_ds[split] = datasets.concatenate_datasets([ELI5_ds[split],dset_source],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP1GQbTiT1P1"
      },
      "source": [
        "## Detoxify ELI5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p4CWtJrWIUE"
      },
      "outputs": [],
      "source": [
        "!pip install detoxify\n",
        "!pip install -U torch\n",
        "!pip install -U transformers\n",
        "from detoxify import Detoxify\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "detoxify_model = Detoxify('unbiased')\n",
        "detoxify_model.model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEHaDfoPLSFQ"
      },
      "outputs": [],
      "source": [
        "ELI5_ds = ELI5_ds.map(lambda x: detoxify_model.predict(x['answer']),\n",
        "                                                  batched=True,batch_size=64\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXQlyANTmAGc"
      },
      "outputs": [],
      "source": [
        "ELI5_ds.save_to_disk('../data/ELI5_toxic_scores')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74D7vn2jsD_h"
      },
      "outputs": [],
      "source": [
        "ELI5_ds = datasets.load_from_disk('../data/ELI5_toxic_scores')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqkXUojcmGed"
      },
      "outputs": [],
      "source": [
        "metrics=['toxicity', 'severe_toxicity',\n",
        "         'obscene', 'identity_attack',\n",
        "         'insult', 'threat', 'sexual_explicit']\n",
        "\n",
        "ELI5_non_toxic = ELI5_ds.filter(lambda x: all(x[metric]<=.1\n",
        "                                              for metric in metrics))\n",
        "\n",
        "ELI5_non_toxic = ELI5_non_toxic.remove_columns([col for col in ELI5_non_toxic['train'].features if\n",
        "                                                col not in ['answer','question']])\n",
        "\n",
        "ELI5_non_toxic.save_to_disk('../data/ELI5_non_toxic')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XD5fUFsT21h"
      },
      "source": [
        "## Combine Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLRl5bK_xGms"
      },
      "outputs": [],
      "source": [
        "SFT_QA_dataset = datasets.DatasetDict()\n",
        "ELI5_non_toxic = datasets.load_from_disk('../data/ELI5_non_toxic')\n",
        "\n",
        "for split in ['train','validation','test']:\n",
        "\n",
        "    SFT_QA_dataset[split] = datasets.concatenate_datasets([simplewiki_QA_ds[split],\n",
        "                                                ELI5_non_toxic[split]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADDaYMxLx0Ge"
      },
      "outputs": [],
      "source": [
        "SFT_QA_dataset = SFT_QA_dataset.shuffle(seed=12321)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "47f54682ed024e63bd920400ccb82276",
            "ad0d35feac88419191807f3cd97acfdd",
            "8c849837041748b492d898d8774945b5"
          ]
        },
        "id": "KnyY6bR_qVhK",
        "outputId": "1a7e4eba-278c-4dc0-f7c8-5d7f19dbb4a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47f54682ed024e63bd920400ccb82276",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad0d35feac88419191807f3cd97acfdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c849837041748b492d898d8774945b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "SFT_QA_dataset = combine_question_answer(SFT_QA_dataset,\n",
        "                                         formatting_prompts_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iwBG2gKSHm_"
      },
      "outputs": [],
      "source": [
        "SFT_QA_dataset = SFT_QA_dataset.remove_columns('Unnamed: 0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "1dc8d511e5384bb284d7c7d0d566807c",
            "15854f31261240e6b7d420c5ac7b7810",
            "4d5528dc89e142258afa5570d3c27166"
          ]
        },
        "id": "2yOpZHqoyUJR",
        "outputId": "6e024628-7f7a-4a2c-8912-ac5a96f3760a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dc8d511e5384bb284d7c7d0d566807c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "15854f31261240e6b7d420c5ac7b7810",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d5528dc89e142258afa5570d3c27166",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "SFT_QA_dataset.save_to_disk('../data/SFT_QA_ds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "OW_fuNgXEEwj",
        "outputId": "c2cacd9d-87c7-49ee-9d51-d4331e00310b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230822_172714-dulxkzv3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/dulxkzv3' target=\"_blank\">SFT_QA_dataset_08.22.23-17.27.14</a></strong> to <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/dulxkzv3' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/dulxkzv3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../data/SFT_QA_ds)... Done. 2.4s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">SFT_QA_dataset_08.22.23-17.27.14</strong> at: <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/dulxkzv3' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/dulxkzv3</a><br/>Synced 5 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230822_172714-dulxkzv3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "now = datetime.now()\n",
        "time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "with wandb.init(project='ELI5_analysis',\n",
        "                entity='ft-llmmm',\n",
        "                job_type='upload_data',\n",
        "                name=f'SFT_QA_dataset_{time_stamp}') as run:\n",
        "\n",
        "    clean_data_art = wandb.Artifact('combined_dataset', 'dataset')\n",
        "    clean_data_art.add_dir('../data/SFT_QA_ds')\n",
        "    run.log_artifact(clean_data_art)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = datasets.load_from_disk('../data/SFT_QA_ds')"
      ],
      "metadata": {
        "id": "yItRz860u7hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "107468+5955+7298"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ydReHxYwGjI",
        "outputId": "08a89027-4940-4f4d-d0c5-f087782d2063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "120721"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uElMmvz9X1Px"
      },
      "source": [
        "## Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgn-pwyoQKlt"
      },
      "source": [
        "### GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_5GI9bESeDr"
      },
      "outputs": [],
      "source": [
        "SFT_QA_dataset = datasets.load_from_disk('../data/SFT_QA_ds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740,
          "referenced_widgets": [
            "6d6b3df3bab241ca8bb91b3c056a74b6",
            "a4a37e4ebba84246917a506fcf54cf61",
            "47b5d4ab01fc4baa944e89635ab3849b",
            "c209b7c06e5a487a94797d6165295ec2",
            "7a817049e27e4bbca02dffce1f460fee",
            "1e4f76e3616f4f60aafcfb59fea9d8d1",
            "f3fa8d2aebb54e419d10b2a3012ada65",
            "64996a2253f245549d288f00ddbfe7fd",
            "7d7c49892539478384bec41d0eacb4cb",
            "c05ee580418440f09c5d4381c07abed4",
            "2736ccde6ff5413e9209ffcd927f458e",
            "caed1e699cff45f48a15dd30bddb25a9",
            "3e5a56efd2a942fb9d675ce99ae4e65a",
            "70945e3320b741b585de7161e34dee36",
            "b4bbd40c95ed4c26943481fc0181670e",
            "06dac56a8ee1401d9780e0454bce8b9c"
          ]
        },
        "id": "BNmO8W7TcT5p",
        "outputId": "646b5b75-91a0-41fc-9e44-77b5bcedee57"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d6b3df3bab241ca8bb91b3c056a74b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4a37e4ebba84246917a506fcf54cf61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47b5d4ab01fc4baa944e89635ab3849b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c209b7c06e5a487a94797d6165295ec2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a817049e27e4bbca02dffce1f460fee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e4f76e3616f4f60aafcfb59fea9d8d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3fa8d2aebb54e419d10b2a3012ada65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64996a2253f245549d288f00ddbfe7fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1791 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d7c49892539478384bec41d0eacb4cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c05ee580418440f09c5d4381c07abed4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2736ccde6ff5413e9209ffcd927f458e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caed1e699cff45f48a15dd30bddb25a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e5a56efd2a942fb9d675ce99ae4e65a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70945e3320b741b585de7161e34dee36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/106806 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4bbd40c95ed4c26943481fc0181670e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/5942 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06dac56a8ee1401d9780e0454bce8b9c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/7259 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230822_173122-jjdhqkg0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/jjdhqkg0' target=\"_blank\">GPT2_QA_tokenized_dataset_08.22.23-17.31.22</a></strong> to <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/jjdhqkg0' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/jjdhqkg0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/GPT2_QA_tokenized)... Done. 2.3s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">GPT2_QA_tokenized_dataset_08.22.23-17.31.22</strong> at: <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/jjdhqkg0' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/jjdhqkg0</a><br/>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230822_173122-jjdhqkg0/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tok = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "GPT2_QA_tokenized = prepare_dataset(SFT_QA_dataset,tok,formatting_prompts_func)\n",
        "GPT2_QA_tokenized.save_to_disk('./data/GPT2_QA_tokenized')\n",
        "\n",
        "now = datetime.now()\n",
        "time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "with wandb.init(project='ELI5_analysis',\n",
        "                entity='ft-llmmm',\n",
        "                job_type='upload_data',\n",
        "                name=f'GPT2_QA_tokenized_dataset_{time_stamp}') as run:\n",
        "\n",
        "    clean_data_art = wandb.Artifact('GPT2_QA_tokenized', 'dataset')\n",
        "    clean_data_art.add_dir('./data/GPT2_QA_tokenized')\n",
        "    run.log_artifact(clean_data_art)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbNAxtQqQMN2"
      },
      "source": [
        "### Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxx2vudp86HE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxu3k7V0FIKE"
      },
      "outputs": [],
      "source": [
        "SFT_QA_dataset = datasets.load_from_disk('../data/SFT_QA_ds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ktiiddITYCT",
        "outputId": "150adca7-91d2-4025-af58-8efb020182dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2RNwUIAGZcs"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "model_name = model_id.split('/')[-1]\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305,
          "referenced_widgets": [
            "3a47a04f8bca44b882dee013ef7106fb",
            "2afc328836474a53ba98220e74a1b0b9",
            "169f31c3ef2945cabe70a7228bc9d6d9",
            "dbf4ae71184049478b78e6c0bb4112dd",
            "eb9a759d3e9c44a6a91cb4d821797e10",
            "843b9b5e341746e28d012b86505bdaea",
            "3f600ea525ed4b8da5f1660bf7045521",
            "ed4d4f08047b4be4b30f10789eab3646",
            "d6e8aae5ce674d5e97261d2714c06f58"
          ]
        },
        "id": "YF8h7y5pISkV",
        "outputId": "6af20570-166c-43f8-854b-9f0fca65e358"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a47a04f8bca44b882dee013ef7106fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2afc328836474a53ba98220e74a1b0b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "169f31c3ef2945cabe70a7228bc9d6d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbf4ae71184049478b78e6c0bb4112dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb9a759d3e9c44a6a91cb4d821797e10",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "843b9b5e341746e28d012b86505bdaea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f600ea525ed4b8da5f1660bf7045521",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed4d4f08047b4be4b30f10789eab3646",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6e8aae5ce674d5e97261d2714c06f58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "SFT_QA_dataset_llama = SFT_QA_dataset.map(lambda x :\n",
        "                                    llama_tokenizer(x['QA']))\n",
        "\n",
        "SFT_QA_dataset_llama = SFT_QA_dataset_llama.map(lambda x: {'length':len(x['input_ids'])})\n",
        "\n",
        "SFT_QA_dataset_llama.save_to_disk('../data/SFT_QA_dataset_llama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "UHVX0FnUHKzy",
        "outputId": "4e05ff30-693f-4888-9881-0baf169ae1b5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230822_173943-yt6bgw9y</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/yt6bgw9y' target=\"_blank\">llama_QA_tokenized_dataset_clean</a></strong> to <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/yt6bgw9y' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/yt6bgw9y</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../data/SFT_QA_dataset_llama)... Done. 1.8s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">llama_QA_tokenized_dataset_clean</strong> at: <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/yt6bgw9y' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/yt6bgw9y</a><br/>Synced 5 W&B file(s), 0 media file(s), 37 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230822_173943-yt6bgw9y/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "with wandb.init(project='ELI5_analysis',\n",
        "                entity='ft-llmmm',\n",
        "                job_type='upload_data',\n",
        "                name=f'llama_QA_tokenized_dataset_clean') as run:\n",
        "\n",
        "    clean_data_art = wandb.Artifact('llama_QA_tokenized', 'dataset')\n",
        "    clean_data_art.add_dir('../data/SFT_QA_dataset_llama')\n",
        "    run.log_artifact(clean_data_art)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMXjVr4KJ_ou"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ6vzUA6ypCg"
      },
      "outputs": [],
      "source": [
        "ds_llama = datasets.load_from_disk('../data/SFT_QA_dataset_llama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "7deea55270ed46829c713aa299dcfa21",
            "a567081154244f3da0d3e10b11bb468b",
            "78d652171b1b4cf9a57f4c2838232507",
            "e0c16e87e01742318ebe4210f8eab4b0",
            "48c814a4a33c4e09b4c8eecf0097af93",
            "eedbbbe7e362481b868bb5cee605567b"
          ]
        },
        "id": "CcnD_Xg2zNRx",
        "outputId": "1efd7c9e-20f5-47d6-92a7-d1d5409a5938"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7deea55270ed46829c713aa299dcfa21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/72214 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a567081154244f3da0d3e10b11bb468b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1964 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78d652171b1b4cf9a57f4c2838232507",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/3301 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0c16e87e01742318ebe4210f8eab4b0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/72214 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48c814a4a33c4e09b4c8eecf0097af93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/1964 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eedbbbe7e362481b868bb5cee605567b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/3301 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds_llama_wiki = ds_llama.filter(lambda x: x['source']=='simple_wiki')\n",
        "ds_llama_eli5 = ds_llama.filter(lambda x: x['source']=='ELI5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qDhzQeMyvkK",
        "outputId": "b8ff6746-b251-437b-89d7-0b0f794a7ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max length in split train for ELI5 is: 3250\n",
            "max length in split train for wiki is: 937\n",
            "max length in split validation for ELI5 is: 2434\n",
            "max length in split validation for wiki is: 645\n",
            "max length in split test for ELI5 is: 3975\n",
            "max length in split test for wiki is: 550\n"
          ]
        }
      ],
      "source": [
        "for key in ['train','validation','test']:\n",
        "    print(f\"max length in split {key} for ELI5 is: {max(ds_llama_eli5[key]['length'])}\")\n",
        "    print(f\"max length in split {key} for wiki is: {max(ds_llama_wiki[key]['length'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PojQd7918ndf"
      },
      "outputs": [],
      "source": [
        "SFT_QA_dataset_llama = datasets.load_from_disk('../data/SFT_QA_dataset_llama')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "8c37e921ade347e5948efed11af980a4",
            "285f06eda6cc49f7ae9ba59b6d0048b6",
            "33c4e1c485e6494aa8a13e3b384af119",
            "017306213bdd4f8d84d3cec6ebef3000",
            "ddaf5deda7fa479daf59605aa635c9f0",
            "57f0b7f1b5374011bd8072a38a8ff942"
          ]
        },
        "id": "Z1qzCafR8qFi",
        "outputId": "e9edbf43-7876-4958-ec88-bbd73739c6ab"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c37e921ade347e5948efed11af980a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "285f06eda6cc49f7ae9ba59b6d0048b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33c4e1c485e6494aa8a13e3b384af119",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "017306213bdd4f8d84d3cec6ebef3000",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/107468 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddaf5deda7fa479daf59605aa635c9f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/5955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57f0b7f1b5374011bd8072a38a8ff942",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/7298 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "SFT_QA_dataset_llama_1024 = SFT_QA_dataset_llama.filter(lambda x:x['length']<=1024)\n",
        "SFT_QA_dataset_llama_2048 = SFT_QA_dataset_llama.filter(lambda x:x['length']<=2048)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "e54a8c86a24f4fe2a0df4e085321dcd4",
            "168732e4430e43c39e02c9ffe70950c1",
            "f9ad621366ca40919a48cdd5f9b0d441",
            "aac2b49045424b64b84934b084f6eda8",
            "7015f696c14441518c58ee0917eb9d26",
            "311762d6158e460b8038cbb60116a07f"
          ]
        },
        "id": "PNUMnUEB8yhH",
        "outputId": "f0402fad-34f6-4229-dd0b-8c6286e362c6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e54a8c86a24f4fe2a0df4e085321dcd4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/106557 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "168732e4430e43c39e02c9ffe70950c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/5939 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9ad621366ca40919a48cdd5f9b0d441",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/7247 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aac2b49045424b64b84934b084f6eda8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/107388 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7015f696c14441518c58ee0917eb9d26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/5954 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "311762d6158e460b8038cbb60116a07f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/7287 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "SFT_QA_dataset_llama_1024.save_to_disk('../data/llama_tokenized_1024')\n",
        "SFT_QA_dataset_llama_2048.save_to_disk('../data/llama_tokenized_2048')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coKr2VM48_GY"
      },
      "outputs": [],
      "source": [
        "with wandb.init(project='ELI5_analysis',\n",
        "                entity='ft-llmmm',\n",
        "                job_type='upload_data',\n",
        "                name=f'llama_QA_tokenized_dataset_clean_short') as run:\n",
        "\n",
        "    clean_data_art_1024 = wandb.Artifact('llama_QA_tokenized_1024', 'dataset')\n",
        "    clean_data_art_1024.add_dir('../data/llama_tokenized_1024')\n",
        "    run.log_artifact(clean_data_art_1024)\n",
        "\n",
        "    clean_data_art_2048 = wandb.Artifact('llama_QA_tokenized_2048', 'dataset')\n",
        "    clean_data_art_2048.add_dir('../data/llama_tokenized_2048')\n",
        "    run.log_artifact(clean_data_art_2048)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds=datasets.load_from_disk('../data/llama_tokenized_1024')"
      ],
      "metadata": {
        "id": "t70sjSbJg0xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_wiki = ds.filter(lambda x:x['source']=='simple_wiki')\n",
        "ds_eli5 = ds.filter(lambda x:x['source']!='simple_wiki')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "9ec95d5a6cf24809b6ae7fb85e5a3f99",
            "42a3e3570c384ba6a91ea56d8a99b751",
            "bc5eabe736534126909b208a693f1a10",
            "c80f5f234a2c4a37b2f9b430dd02fdb7",
            "9b7133276d1e4dbfb1803bfbd83fe5e4",
            "b05afb6f0b15442fac736afa9410da88",
            "666c21662ae94824b6242d5af12d44c1",
            "68ea2cbad2db4dfd9528a4f317cdec01",
            "606732276e90460fabf45d4ec54613b0",
            "3b720af7d88d46e5ad406b547759eba2",
            "df95bb6035e94182935681acf9a93be3",
            "75023618498a44329e7a38eaa9772bfd",
            "f3e9078cdad44d2a8a7f8a6cef5c4458",
            "eb7e7af87c2b4c0f91ec6bc95632b6f3",
            "ff3a8a39a5c34daa929e669162fa4d65",
            "13386ffd2b4e4d38803536d66d7d6b3d",
            "8abea99cbac644c3876b45ad50b5f11c",
            "cabdab99c5ed417c846b595a5c1094d6",
            "c28490aa7ceb40a8a936d54453e600ce",
            "8f74e1901adb4bf28130214976670467",
            "e252551667f44ae580364a9a7d4ace32",
            "60391b6e40d4428f9c460a4fade75ea1",
            "288fac6a7d034e76a5da608797d0fb6d",
            "aa9164f1428049b5b9bc03ed4a29eb95",
            "5874e02e0c7b4d23a56c1ffc6e72fbcf",
            "56888d59d0174cf9a8b2122effa52bcc",
            "ec94041036ee4c409fcdccfe9ad79884",
            "88165582cf674f7893c9cc78b0ce0401",
            "58d710e8f7d74ac0bc175af2a00dc698",
            "1b3e03685ae54ddab2757f760dfb76e2",
            "6e6a791e7c0b4db38cf3b0b6779a7ab9",
            "8c10f27cef72444f9ddc9d1eec135cd2",
            "be39eae6a08e45afbebbdc8d56cdcc06",
            "42489c6d5e3c428eac79ad465d0dd970",
            "5bdb9b1ea66643498c5cfe76864b8c1e",
            "03a49488cd684ccaa0244031a110f2d3",
            "df512da945fa4e1d87f1c7c7c08ceddc",
            "c022cd1feeda480db3055be21f2671d1",
            "f0fbbe5c878d4e89a66269a1572fc899",
            "10fcf3a0aa63491a90023d63d6bced19",
            "27d1eb662b25400d9b89027bbe49f38f",
            "bf2771a8882f45d0b239ac79b582c49b",
            "364a5b1389bd4649a63f8f6ae2d7c297",
            "f49e4ee8209f4b0f86022f262fff27a9",
            "2190899e0f4c48ae8bcb2674a93e66c1",
            "dce546672faf4e218518fba3d02cf321",
            "d48eb69797f84e9185e54b3bb7022cdd",
            "e2a3f56aa92343dcb52f3d073ede3d47",
            "d5546eb8d76f4573a6921e4148e42c1b",
            "0ce796f7c9cd4a9ca1f9c4c353fc64c0",
            "27669cf20b484194aa5220b37613b0dc",
            "2a9c3c12236c4b73b2859c02268a010a",
            "717f7e2573aa42e787a366091d3350e0",
            "44e834ca788248778048b46df0e0b95e",
            "620757e2cd0f4be4a67019bd625886d5",
            "e793cefdd32d4564aaa3b91c9519c9c3",
            "43aa35559c34432fb0d9e89eb0982ab5",
            "b26d952ba80b4bb9a6fbdbea8643dc13",
            "99603bda2be7404982ebb3d8fab91fc6",
            "a62baebee6c04deaa0c1b6910b6a5c13",
            "e86bc23ebd724ec1ab63aaffb1ee4ade",
            "22e8157835994bf5ab79beada908ef0a",
            "b72e4ad90ed14b23882e17e3c5ea0e19",
            "8ef56e1c98f647fc9ba384f46420b963",
            "c33056afcac74799a427aa0181d6ba4f",
            "564509a103f8467d9ac8237c9b546165"
          ]
        },
        "id": "n_3kZpdehoQr",
        "outputId": "05a14554-f140-4e18-a213-0917ee03ef75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/106557 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ec95d5a6cf24809b6ae7fb85e5a3f99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/5939 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75023618498a44329e7a38eaa9772bfd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/7247 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "288fac6a7d034e76a5da608797d0fb6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/106557 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42489c6d5e3c428eac79ad465d0dd970"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/5939 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2190899e0f4c48ae8bcb2674a93e66c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/7247 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e793cefdd32d4564aaa3b91c9519c9c3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "nmiT7IXRiS19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_wiki['train']"
      ],
      "metadata": {
        "id": "cBXvSf90ih4c",
        "outputId": "f4401da7-f9d7-4d9c-c0cb-de808823510e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'source', 'QA', 'input_ids', 'attention_mask', 'length'],\n",
              "    num_rows: 65252\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_eli5['train']"
      ],
      "metadata": {
        "id": "DyyZQ0DUijzR",
        "outputId": "fcee6e66-c9f4-4825-9656-302b5bce3347",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['question', 'answer', 'source', 'QA', 'input_ids', 'attention_mask', 'length'],\n",
              "    num_rows: 41305\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(ds_wiki['train']['length'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvMRlmW7iNNS",
        "outputId": "64ebca57-8358-4956-8c2c-762950056d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9748885"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(ds_eli5['train']['length'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0vpTemriaji",
        "outputId": "556c57ab-a588-43b8-ef90-739ad6a813c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9909595"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTapsiF6W_wZ"
      },
      "source": [
        "# Training Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pfeQ-UAgR5x"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "run = wandb.init(project='SFT_training_dm',\n",
        "                 entity='ft-llmmm')\n",
        "\n",
        "artifact = run.use_artifact(\n",
        "    'ft-llmmm/ELI5_analysis/llama_QA_tokenized_1024:latest',\n",
        "    type='dataset')\n",
        "artifact_dir = artifact.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVuLh7XqgldR"
      },
      "outputs": [],
      "source": [
        "ds_combined_1024 = datasets.load_from_disk(\n",
        "    './artifacts/llama_QA_tokenized_1024:v0')\n",
        "ds_wiki_1024 = ds_combined_1024.filter(lambda x:\n",
        "                                       x['source']=='simple_wiki')\n",
        "\n",
        "ds_wiki_1024.save_to_disk('./data/ds_wiki_1024')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "e9f7c216ddff49609c41c777ece24742",
            "4aa1d7e657f1483f990e5116c3b0b8a2",
            "a71d6e6c71694cbba52dfb3cb5d9f00b",
            "edc366d9a09d45688bbe4bf176db7f12",
            "e45cfe61e04545c69218d7dda297944b",
            "dff5b5617dd3426e98743ab828517a39",
            "b2ebdfde6f8e4045959377101c65f3d6",
            "812f198b05104ad28ae9d29399b4d77d",
            "e9d442ee43e647169cbf4f862ebfeb9d",
            "8d61ae0b338048a2979ce5ba42613528",
            "a5fc78d74ac946a6bd85f14cdff3fd16",
            "4b3f4137f26f4ebc891cd8cfbe3362ee",
            "3c4923ea45184b8f843af340b8c5103e",
            "f7d02bc9e0e74cb49e9ec3e390e1814a",
            "2872be6249e742ecaaa7528434584a58",
            "8986504bfec64133a9392319fe68e2a3",
            "0fff54b74b3740dd91a93914b77d414e",
            "558728111107485d8a356f726f0ede23",
            "2d932962f1df4cb78560ff2909a7e53d",
            "3334eb1e16604ff0a8c28c6ed38dfd9a",
            "24eeaa6ad3a24b34a75ef63c23770e15",
            "13e5289477db410cb394a8a3fc46ef95",
            "ceb1da06817d4adeae0c0b777596706f",
            "8a7fb3f1296542bcb34e51dbedca929c",
            "5746d68c6e324cfcbcd02c4b26d44138",
            "da90c785ffce4a8c90b47ec8b2367c7d",
            "66db49cef7644066ac851f42b3d855f8",
            "bc7c44726b514729a3b91c8734dc6e94",
            "8ac4e9af3bfb478192e33ecc429ead4c",
            "6d3d425474244d5ba1774c33ac49a6a1",
            "b45b4f9f716f42ebb5d2f78bca8d8020",
            "a0ccb486b2604e24a718eeb7e7551467",
            "a3d1a2bd3fde49139935cdde49bcf6a2"
          ]
        },
        "id": "KMPR-ZOQYGD_",
        "outputId": "e75d9e09-beb5-4e66-93d0-5a3c5bcf090c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9f7c216ddff49609c41c777ece24742",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/106557 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b3f4137f26f4ebc891cd8cfbe3362ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/5939 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ceb1da06817d4adeae0c0b777596706f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/7247 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds_full = datasets.load_from_disk(\n",
        "    './artifacts/llama_QA_tokenized_1024:v1')\n",
        "\n",
        "ds_wiki_1024_full = ds_full.filter(\n",
        "    lambda x: x['source']=='simple_wiki')\n",
        "\n",
        "ds_eli5_1024 = ds_full.filter(\n",
        "    lambda x: x['source']!='simple_wiki')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejqVA1snCOwi"
      },
      "outputs": [],
      "source": [
        "ds_wiki_1024_full.save_to_disk('./data/ds_wiki_1024_full')\n",
        "ds_eli5_1024.save_to_disk('./data/ds_eli5_1024')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82goyqsdizuU"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights\n",
        "dataset_path = './artifacts/llama_QA_tokenized_1024:v1'\n",
        "ds_name = 'eli5-wiki-1024'\n",
        "\n",
        "now = datetime.now()\n",
        "time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "\n",
        "#model_name = model_id.replace('/','-')\n",
        "model_name = model_id.split('/')[-1]\n",
        "#ds_name = dataset_path.split('/')[-1].replace('llama','combined_large').replace(':','-')\n",
        "\n",
        "#ds_name = dataset_path.split('/')[-1]\n",
        "output_dir = f'./{model_name}_{ds_name}/models'\n",
        "logging_dir = f'{output_dir}/logs'\n",
        "\n",
        "run_name = f'resumed_{ds_name}_{time_stamp}'\n",
        "optim = 'paged_adamw_8bit'\n",
        "\n",
        "from pathlib import Path\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(logging_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_id = f'{model_name}-{ds_name}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxjGFF11XBT3"
      },
      "outputs": [],
      "source": [
        "!python ./run_clm.py \\\n",
        "--output_dir {output_dir} \\\n",
        "--logging_dir {logging_dir} \\\n",
        "--model_id {model_id} \\\n",
        "--dataset_path {dataset_path} \\\n",
        "--run_name {run_name} \\\n",
        "--repo_id {repo_id} \\\n",
        "--report_to_wandb 0 \\\n",
        "--epochs 1 \\\n",
        "--max_steps -1 \\\n",
        "--per_device_train_batch_size 16 \\\n",
        "--per_device_eval_batch_size 16 \\\n",
        "--gradient_accumulation_steps 8 \\\n",
        "--lr 2e-4 \\\n",
        "--entity 'ft-llmmm' \\\n",
        "--project_name 'SFT_training_dm' \\\n",
        "--hub_strategy 'every_save' \\\n",
        "--torch_compile 0 \\\n",
        "--gradient_checkpointing 1 \\\n",
        "--optim 'paged_adamw_8bit' \\\n",
        "--group_by_length 1 \\\n",
        "--hf_token {hf_token} \\\n",
        "--wandb_token {wandb_token} \\\n",
        "--use_flash_attention 1 \\\n",
        "--logging_steps 10 \\\n",
        "--resume_from_checkpoint 1 \\\n",
        "--auto_find_batch_size 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights\n",
        "model_name = model_id.split('/')[-1]\n",
        "\n",
        "dataset_path = './data/ds_wiki_1024_full'\n",
        "ds_name = dataset_path.split('/')[-1]\n",
        "\n",
        "now = datetime.now()\n",
        "time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "\n",
        "#model_name = model_id.replace('/','-')\n",
        "#ds_name = dataset_path.split('/')[-1].replace('llama','combined_large').replace(':','-')\n",
        "\n",
        "output_dir = f'./{model_name}_{ds_name}/models'\n",
        "logging_dir = f'{output_dir}/logs'\n",
        "\n",
        "run_name = f'{ds_name}_{time_stamp}'\n",
        "optim = 'paged_adamw_8bit'\n",
        "\n",
        "from pathlib import Path\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(logging_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_id = f'{model_name}-{ds_name}'\n",
        "\n",
        "!python ./run_clm.py \\\n",
        "--output_dir {output_dir} \\\n",
        "--logging_dir {logging_dir} \\\n",
        "--model_id {model_id} \\\n",
        "--dataset_path {dataset_path} \\\n",
        "--run_name {run_name} \\\n",
        "--repo_id {repo_id} \\\n",
        "--report_to_wandb 1 \\\n",
        "--epochs 1 \\\n",
        "--max_steps -1 \\\n",
        "--per_device_train_batch_size 16 \\\n",
        "--per_device_eval_batch_size 16 \\\n",
        "--gradient_accumulation_steps 8 \\\n",
        "--lr 2e-4 \\\n",
        "--entity 'ft-llmmm' \\\n",
        "--project_name 'SFT_training_dm' \\\n",
        "--hub_strategy 'every_save' \\\n",
        "--torch_compile 0 \\\n",
        "--gradient_checkpointing 1 \\\n",
        "--optim 'paged_adamw_8bit' \\\n",
        "--group_by_length 1 \\\n",
        "--hf_token {hf_token} \\\n",
        "--wandb_token {wandb_token} \\\n",
        "--use_flash_attention 1 \\\n",
        "--logging_steps 10 \\\n",
        "--resume_from_checkpoint 0 \\\n",
        "--auto_find_batch_size 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwy6XMu2nN3f",
        "outputId": "47fe9dde-e481-4c2f-9ef5-fb6a3d76ed80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-05 18:50:26.892529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "args is Namespace(model_id='meta-llama/Llama-2-13b-hf', repo_id='Llama-2-13b-hf-ds_wiki_1024_full', hub_strategy='every_save', output_dir='./Llama-2-13b-hf_ds_wiki_1024_full/models', output_data_dir=None, dataset_path='./data/ds_wiki_1024_full', hf_token='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', report_to_wandb=1, wandb_token='93b4fb1b729b939f257d7db15130b3710cad2ebb', epochs=1, max_steps=-1, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=8, max_seq_length=4096, logging_steps=10, optim='paged_adamw_8bit', lr=0.0002, lora_r=64, lora_alpha=16, weight_decay=0.1, lora_dropout=0.1, load_in_4bit=1, load_in_8bit=0, use_peft=1, gradient_checkpointing=1, bf16=1, group_by_length=1, merge_weights=0, seed=42, warmup_ratio=0.03, project_name='SFT_training_dm', entity='ft-llmmm', run_name='ds_wiki_1024_full_09.05.23-18.50.22', load_best_model_at_end=1, use_sagemaker=1, torch_compile=0, use_flash_attention=1, resume_from_checkpoint=0, auto_find_batch_size=0)\n",
            "extra is ['--logging_dir', './Llama-2-13b-hf_ds_wiki_1024_full/models/logs']\n",
            "Logging into the Hugging Face Hub with token hf_dZJsCiE...\n",
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Namespace(model_id='meta-llama/Llama-2-13b-hf', repo_id='Llama-2-13b-hf-ds_wiki_1024_full', hub_strategy='every_save', output_dir='./Llama-2-13b-hf_ds_wiki_1024_full/models', output_data_dir=None, dataset_path='./data/ds_wiki_1024_full', hf_token='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', report_to_wandb=1, wandb_token='93b4fb1b729b939f257d7db15130b3710cad2ebb', epochs=1, max_steps=-1, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=8, max_seq_length=4096, logging_steps=10, optim='paged_adamw_8bit', lr=0.0002, lora_r=64, lora_alpha=16, weight_decay=0.1, lora_dropout=0.1, load_in_4bit=1, load_in_8bit=0, use_peft=1, gradient_checkpointing=1, bf16=1, group_by_length=1, merge_weights=0, seed=42, warmup_ratio=0.03, project_name='SFT_training_dm', entity='ft-llmmm', run_name='ds_wiki_1024_full_09.05.23-18.50.22', load_best_model_at_end=1, use_sagemaker=1, torch_compile=0, use_flash_attention=1, resume_from_checkpoint=0, auto_find_batch_size=0)\n",
            "using flash attention\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmeltzer\u001b[0m (\u001b[33mft-llmmm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230905_185034-tszv476e\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mds_wiki_1024_full_09.05.23-18.50.22_r_64_alpha_16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ft-llmmm/SFT_training_dm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ft-llmmm/SFT_training_dm/runs/tszv476e\u001b[0m\n",
            "loading from ./data/ds_wiki_1024_full\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Downloading (…)lve/main/config.json: 100% 610/610 [00:00<00:00, 1.95MB/s]\n",
            "Downloading (…)fetensors.index.json: 100% 33.4k/33.4k [00:00<00:00, 75.3MB/s]\n",
            "Downloading shards:   0% 0/3 [00:00<?, ?it/s]\n",
            "Downloading (…)of-00003.safetensors:   0% 0.00/9.95G [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   0% 41.9M/9.95G [00:00<00:23, 419MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   1% 105M/9.95G [00:00<00:20, 488MB/s] \u001b[A\n",
            "Downloading (…)of-00003.safetensors:   2% 157M/9.95G [00:00<00:19, 503MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   2% 220M/9.95G [00:00<00:18, 514MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   3% 283M/9.95G [00:00<00:18, 522MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   3% 346M/9.95G [00:00<00:18, 525MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   4% 409M/9.95G [00:00<00:18, 526MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   5% 472M/9.95G [00:00<00:17, 528MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   5% 535M/9.95G [00:01<00:17, 529MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   6% 598M/9.95G [00:01<00:17, 530MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   7% 661M/9.95G [00:01<00:17, 528MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   7% 724M/9.95G [00:01<00:17, 527MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   8% 786M/9.95G [00:01<00:17, 522MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   8% 839M/9.95G [00:01<00:17, 514MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   9% 891M/9.95G [00:01<00:17, 510MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   9% 944M/9.95G [00:01<00:17, 504MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  10% 996M/9.95G [00:01<00:17, 500MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  11% 1.05G/9.95G [00:02<00:17, 496MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  11% 1.10G/9.95G [00:02<00:19, 451MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  12% 1.15G/9.95G [00:02<00:20, 421MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  12% 1.22G/9.95G [00:02<00:19, 452MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  13% 1.27G/9.95G [00:02<00:18, 470MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  13% 1.32G/9.95G [00:02<00:17, 482MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  14% 1.37G/9.95G [00:02<00:17, 490MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  14% 1.43G/9.95G [00:02<00:17, 500MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  15% 1.48G/9.95G [00:02<00:16, 506MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  15% 1.53G/9.95G [00:03<00:16, 511MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  16% 1.59G/9.95G [00:03<00:16, 517MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  17% 1.66G/9.95G [00:03<00:15, 520MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  17% 1.71G/9.95G [00:03<00:20, 396MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  18% 1.76G/9.95G [00:03<00:27, 295MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  18% 1.80G/9.95G [00:04<00:30, 263MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  18% 1.84G/9.95G [00:04<00:32, 247MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  19% 1.87G/9.95G [00:04<00:33, 243MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  19% 1.90G/9.95G [00:04<00:33, 242MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  19% 1.93G/9.95G [00:04<00:32, 249MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  20% 1.97G/9.95G [00:04<00:29, 266MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  20% 2.01G/9.95G [00:04<00:26, 295MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  21% 2.04G/9.95G [00:04<00:26, 299MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  21% 2.08G/9.95G [00:04<00:26, 302MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  21% 2.12G/9.95G [00:05<00:24, 325MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  22% 2.16G/9.95G [00:05<00:22, 344MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  22% 2.20G/9.95G [00:05<00:24, 320MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  23% 2.24G/9.95G [00:05<00:23, 334MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  23% 2.29G/9.95G [00:05<00:23, 332MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  23% 2.33G/9.95G [00:05<00:22, 335MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  24% 2.37G/9.95G [00:05<00:23, 325MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  24% 2.41G/9.95G [00:06<00:26, 280MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  25% 2.46G/9.95G [00:06<00:23, 322MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  25% 2.51G/9.95G [00:06<00:21, 344MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  26% 2.55G/9.95G [00:06<00:21, 342MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  26% 2.59G/9.95G [00:06<00:21, 338MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  26% 2.63G/9.95G [00:06<00:20, 349MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  27% 2.67G/9.95G [00:06<00:20, 348MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  27% 2.73G/9.95G [00:06<00:19, 378MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  28% 2.78G/9.95G [00:06<00:17, 408MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  28% 2.83G/9.95G [00:07<00:16, 424MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  29% 2.88G/9.95G [00:07<00:16, 428MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  30% 2.94G/9.95G [00:07<00:16, 435MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  30% 2.99G/9.95G [00:07<00:15, 458MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  31% 3.04G/9.95G [00:07<00:14, 474MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  31% 3.09G/9.95G [00:07<00:14, 485MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  32% 3.15G/9.95G [00:07<00:13, 493MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  32% 3.20G/9.95G [00:07<00:13, 500MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  33% 3.25G/9.95G [00:07<00:13, 504MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  33% 3.30G/9.95G [00:08<00:13, 508MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  34% 3.36G/9.95G [00:08<00:12, 511MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  34% 3.41G/9.95G [00:08<00:12, 514MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  35% 3.46G/9.95G [00:08<00:12, 513MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  35% 3.51G/9.95G [00:08<00:12, 515MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  36% 3.57G/9.95G [00:08<00:17, 374MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  36% 3.62G/9.95G [00:08<00:22, 286MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  37% 3.66G/9.95G [00:09<00:25, 248MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  37% 3.69G/9.95G [00:09<00:27, 231MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  37% 3.72G/9.95G [00:09<00:28, 218MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  38% 3.75G/9.95G [00:09<00:29, 210MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  38% 3.79G/9.95G [00:09<00:29, 208MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  38% 3.82G/9.95G [00:10<00:28, 214MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  39% 3.85G/9.95G [00:10<00:28, 217MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  39% 3.88G/9.95G [00:10<00:27, 225MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  39% 3.91G/9.95G [00:10<00:25, 238MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  40% 3.94G/9.95G [00:10<00:23, 255MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  40% 3.97G/9.95G [00:10<00:22, 266MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  40% 4.01G/9.95G [00:10<00:21, 276MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  41% 4.05G/9.95G [00:10<00:18, 312MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  41% 4.09G/9.95G [00:10<00:18, 316MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 4.13G/9.95G [00:11<00:18, 310MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 4.17G/9.95G [00:11<00:17, 327MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 4.22G/9.95G [00:11<00:17, 322MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  43% 4.26G/9.95G [00:11<00:17, 328MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  43% 4.30G/9.95G [00:11<00:17, 329MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  44% 4.34G/9.95G [00:11<00:16, 339MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  44% 4.38G/9.95G [00:11<00:16, 341MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  44% 4.42G/9.95G [00:11<00:16, 331MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  45% 4.47G/9.95G [00:12<00:19, 285MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  45% 4.50G/9.95G [00:12<00:19, 276MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  46% 4.53G/9.95G [00:12<00:20, 269MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  46% 4.56G/9.95G [00:12<00:19, 275MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  46% 4.59G/9.95G [00:12<00:19, 281MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  47% 4.63G/9.95G [00:12<00:18, 287MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  47% 4.67G/9.95G [00:12<00:18, 285MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  47% 4.70G/9.95G [00:12<00:18, 287MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  48% 4.73G/9.95G [00:13<00:17, 292MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  48% 4.76G/9.95G [00:13<00:18, 288MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  48% 4.79G/9.95G [00:13<00:17, 289MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  48% 4.82G/9.95G [00:13<00:17, 293MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  49% 4.85G/9.95G [00:13<00:17, 289MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  49% 4.89G/9.95G [00:13<00:17, 293MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  49% 4.92G/9.95G [00:13<00:17, 291MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  50% 4.95G/9.95G [00:13<00:17, 283MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  50% 4.98G/9.95G [00:13<00:17, 287MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  50% 5.02G/9.95G [00:14<00:16, 302MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  51% 5.06G/9.95G [00:14<00:15, 319MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  51% 5.11G/9.95G [00:14<00:15, 319MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  52% 5.15G/9.95G [00:14<00:14, 326MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  52% 5.19G/9.95G [00:14<00:14, 328MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  53% 5.23G/9.95G [00:14<00:15, 308MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  53% 5.28G/9.95G [00:14<00:13, 344MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  54% 5.33G/9.95G [00:15<00:19, 240MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  54% 5.38G/9.95G [00:15<00:15, 287MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  55% 5.43G/9.95G [00:15<00:13, 333MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  55% 5.48G/9.95G [00:15<00:12, 371MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  56% 5.54G/9.95G [00:15<00:11, 384MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  56% 5.58G/9.95G [00:15<00:11, 383MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  56% 5.62G/9.95G [00:15<00:11, 379MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  57% 5.66G/9.95G [00:15<00:11, 373MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  57% 5.70G/9.95G [00:16<00:11, 372MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  58% 5.75G/9.95G [00:16<00:11, 380MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  58% 5.79G/9.95G [00:16<00:10, 385MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  59% 5.84G/9.95G [00:16<00:10, 408MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  59% 5.89G/9.95G [00:16<00:09, 419MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  60% 5.95G/9.95G [00:16<00:09, 435MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  60% 6.00G/9.95G [00:16<00:08, 449MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  61% 6.05G/9.95G [00:16<00:08, 451MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  61% 6.10G/9.95G [00:16<00:08, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  62% 6.16G/9.95G [00:17<00:08, 449MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  62% 6.21G/9.95G [00:17<00:08, 451MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  63% 6.26G/9.95G [00:17<00:08, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  63% 6.31G/9.95G [00:17<00:07, 458MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  64% 6.36G/9.95G [00:17<00:07, 474MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  65% 6.42G/9.95G [00:17<00:07, 485MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  65% 6.47G/9.95G [00:17<00:07, 491MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  66% 6.52G/9.95G [00:17<00:06, 498MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  66% 6.57G/9.95G [00:17<00:06, 503MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  67% 6.63G/9.95G [00:18<00:06, 496MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  67% 6.68G/9.95G [00:18<00:06, 504MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  68% 6.73G/9.95G [00:18<00:06, 507MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  68% 6.78G/9.95G [00:18<00:06, 510MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  69% 6.84G/9.95G [00:18<00:06, 511MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  69% 6.89G/9.95G [00:18<00:05, 515MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  70% 6.94G/9.95G [00:18<00:06, 498MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  70% 6.99G/9.95G [00:18<00:07, 389MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  71% 7.05G/9.95G [00:19<00:08, 342MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  71% 7.09G/9.95G [00:19<00:09, 316MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  72% 7.13G/9.95G [00:19<00:09, 301MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  72% 7.17G/9.95G [00:19<00:09, 287MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  72% 7.20G/9.95G [00:19<00:10, 269MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  73% 7.24G/9.95G [00:19<00:09, 272MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  73% 7.27G/9.95G [00:19<00:10, 264MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  73% 7.30G/9.95G [00:20<00:10, 250MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  74% 7.33G/9.95G [00:20<00:10, 241MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  74% 7.36G/9.95G [00:20<00:11, 225MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  74% 7.39G/9.95G [00:20<00:11, 217MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.42G/9.95G [00:20<00:12, 203MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.44G/9.95G [00:20<00:12, 200MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.47G/9.95G [00:20<00:12, 192MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.49G/9.95G [00:21<00:13, 186MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.51G/9.95G [00:21<00:13, 187MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  76% 7.53G/9.95G [00:21<00:13, 183MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  76% 7.55G/9.95G [00:21<00:12, 188MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  76% 7.57G/9.95G [00:21<00:12, 185MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  76% 7.60G/9.95G [00:21<00:11, 213MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  77% 7.64G/9.95G [00:21<00:09, 254MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  77% 7.70G/9.95G [00:21<00:07, 311MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  78% 7.75G/9.95G [00:21<00:06, 355MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  78% 7.80G/9.95G [00:22<00:05, 381MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  79% 7.85G/9.95G [00:22<00:05, 405MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  79% 7.91G/9.95G [00:22<00:04, 426MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  80% 7.96G/9.95G [00:22<00:04, 446MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  81% 8.01G/9.95G [00:22<00:04, 461MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  81% 8.06G/9.95G [00:22<00:03, 472MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  82% 8.12G/9.95G [00:22<00:03, 483MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  82% 8.17G/9.95G [00:22<00:03, 491MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  83% 8.22G/9.95G [00:22<00:03, 432MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  83% 8.27G/9.95G [00:23<00:04, 398MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  84% 8.33G/9.95G [00:23<00:03, 425MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  84% 8.38G/9.95G [00:23<00:03, 446MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  85% 8.43G/9.95G [00:23<00:03, 409MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  85% 8.48G/9.95G [00:23<00:04, 362MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  86% 8.52G/9.95G [00:23<00:04, 336MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  86% 8.57G/9.95G [00:23<00:04, 313MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  87% 8.61G/9.95G [00:24<00:04, 287MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  87% 8.64G/9.95G [00:24<00:04, 278MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  87% 8.67G/9.95G [00:24<00:04, 261MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  87% 8.70G/9.95G [00:24<00:04, 256MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  88% 8.73G/9.95G [00:24<00:04, 243MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  88% 8.77G/9.95G [00:24<00:04, 242MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  88% 8.80G/9.95G [00:24<00:04, 241MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  89% 8.83G/9.95G [00:25<00:04, 234MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  89% 8.86G/9.95G [00:25<00:04, 235MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  89% 8.89G/9.95G [00:25<00:04, 232MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  90% 8.92G/9.95G [00:25<00:04, 234MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  90% 8.95G/9.95G [00:25<00:04, 246MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  90% 9.00G/9.95G [00:25<00:03, 271MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  91% 9.04G/9.95G [00:25<00:03, 301MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  91% 9.08G/9.95G [00:25<00:02, 323MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  92% 9.13G/9.95G [00:26<00:02, 369MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  92% 9.19G/9.95G [00:26<00:01, 398MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  93% 9.24G/9.95G [00:26<00:01, 413MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  93% 9.29G/9.95G [00:26<00:01, 430MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  94% 9.34G/9.95G [00:26<00:01, 443MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  94% 9.40G/9.95G [00:26<00:01, 449MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  95% 9.45G/9.95G [00:26<00:01, 455MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  95% 9.50G/9.95G [00:26<00:00, 456MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  96% 9.55G/9.95G [00:26<00:00, 460MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  97% 9.60G/9.95G [00:27<00:00, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  97% 9.66G/9.95G [00:27<00:00, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  98% 9.71G/9.95G [00:27<00:00, 456MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  98% 9.76G/9.95G [00:27<00:00, 465MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  99% 9.81G/9.95G [00:27<00:00, 474MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  99% 9.87G/9.95G [00:27<00:00, 479MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors: 100% 9.95G/9.95G [00:27<00:00, 357MB/s]\n",
            "Downloading shards:  33% 1/3 [00:28<00:56, 28.19s/it]\n",
            "Downloading (…)of-00003.safetensors:   0% 0.00/9.90G [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   0% 10.5M/9.90G [00:00<03:37, 45.6MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   1% 62.9M/9.90G [00:00<00:45, 218MB/s] \u001b[A\n",
            "Downloading (…)of-00003.safetensors:   1% 115M/9.90G [00:00<00:30, 319MB/s] \u001b[A\n",
            "Downloading (…)of-00003.safetensors:   2% 168M/9.90G [00:00<00:25, 380MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   2% 220M/9.90G [00:00<00:23, 416MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   3% 273M/9.90G [00:00<00:22, 425MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   3% 325M/9.90G [00:00<00:21, 447MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   4% 377M/9.90G [00:01<00:21, 443MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   4% 430M/9.90G [00:01<00:20, 453MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   5% 482M/9.90G [00:01<00:20, 466MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   5% 535M/9.90G [00:01<00:19, 475MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   6% 587M/9.90G [00:01<00:19, 483MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   6% 640M/9.90G [00:01<00:19, 470MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   7% 692M/9.90G [00:01<00:19, 477MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   8% 744M/9.90G [00:01<00:19, 479MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   8% 797M/9.90G [00:01<00:23, 383MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   8% 839M/9.90G [00:02<00:28, 316MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   9% 881M/9.90G [00:02<00:32, 278MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   9% 912M/9.90G [00:02<00:36, 247MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  10% 944M/9.90G [00:02<00:39, 228MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  10% 975M/9.90G [00:02<00:42, 210MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  10% 1.01G/9.90G [00:03<00:46, 192MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  10% 1.03G/9.90G [00:03<00:47, 189MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  11% 1.05G/9.90G [00:03<00:49, 178MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  11% 1.07G/9.90G [00:03<00:49, 180MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  11% 1.10G/9.90G [00:03<00:44, 196MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  11% 1.13G/9.90G [00:03<00:39, 221MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  12% 1.17G/9.90G [00:03<00:33, 258MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  12% 1.22G/9.90G [00:03<00:29, 291MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  13% 1.26G/9.90G [00:04<00:27, 319MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  13% 1.31G/9.90G [00:04<00:23, 368MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  14% 1.35G/9.90G [00:04<00:22, 378MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  14% 1.41G/9.90G [00:04<00:20, 412MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  15% 1.46G/9.90G [00:04<00:19, 435MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  15% 1.51G/9.90G [00:04<00:18, 460MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  16% 1.56G/9.90G [00:04<00:17, 477MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  16% 1.61G/9.90G [00:04<00:16, 489MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  17% 1.67G/9.90G [00:04<00:17, 482MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  17% 1.72G/9.90G [00:05<00:18, 451MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  18% 1.77G/9.90G [00:05<00:20, 395MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  18% 1.82G/9.90G [00:05<00:19, 411MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  19% 1.88G/9.90G [00:05<00:18, 424MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  19% 1.93G/9.90G [00:05<00:18, 442MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  20% 1.98G/9.90G [00:05<00:17, 453MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  21% 2.03G/9.90G [00:05<00:18, 428MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  21% 2.09G/9.90G [00:05<00:21, 360MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  21% 2.13G/9.90G [00:06<00:23, 335MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  22% 2.17G/9.90G [00:06<00:25, 308MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  22% 2.21G/9.90G [00:06<00:26, 291MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  23% 2.24G/9.90G [00:06<00:27, 278MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  23% 2.28G/9.90G [00:06<00:29, 261MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  23% 2.31G/9.90G [00:06<00:30, 247MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  24% 2.34G/9.90G [00:07<00:31, 239MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  24% 2.37G/9.90G [00:07<00:33, 227MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  24% 2.40G/9.90G [00:07<00:35, 212MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  25% 2.43G/9.90G [00:07<00:36, 205MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  25% 2.45G/9.90G [00:07<00:37, 196MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  25% 2.47G/9.90G [00:07<00:37, 197MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  25% 2.50G/9.90G [00:07<00:37, 197MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  25% 2.52G/9.90G [00:07<00:37, 199MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  26% 2.55G/9.90G [00:08<00:34, 211MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  26% 2.58G/9.90G [00:08<00:31, 234MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  26% 2.61G/9.90G [00:08<00:28, 252MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  27% 2.65G/9.90G [00:08<00:26, 276MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  27% 2.69G/9.90G [00:08<00:23, 309MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  28% 2.74G/9.90G [00:08<00:21, 336MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  28% 2.79G/9.90G [00:08<00:18, 377MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  29% 2.84G/9.90G [00:08<00:17, 400MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  29% 2.89G/9.90G [00:08<00:16, 430MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  30% 2.95G/9.90G [00:09<00:15, 441MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  30% 3.00G/9.90G [00:09<00:15, 447MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  31% 3.05G/9.90G [00:09<00:15, 451MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  31% 3.10G/9.90G [00:09<00:14, 465MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  32% 3.16G/9.90G [00:09<00:14, 475MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  33% 3.22G/9.90G [00:09<00:13, 488MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  33% 3.27G/9.90G [00:09<00:13, 490MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  34% 3.33G/9.90G [00:09<00:13, 503MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  34% 3.40G/9.90G [00:09<00:12, 511MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  35% 3.45G/9.90G [00:10<00:12, 514MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  35% 3.51G/9.90G [00:10<00:12, 516MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  36% 3.57G/9.90G [00:10<00:16, 379MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  37% 3.62G/9.90G [00:10<00:15, 409MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  37% 3.67G/9.90G [00:10<00:14, 428MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  38% 3.72G/9.90G [00:10<00:17, 349MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  38% 3.76G/9.90G [00:11<00:20, 296MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  38% 3.81G/9.90G [00:11<00:22, 272MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  39% 3.84G/9.90G [00:11<00:23, 263MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  39% 3.87G/9.90G [00:11<00:23, 254MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  39% 3.90G/9.90G [00:11<00:24, 246MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  40% 3.93G/9.90G [00:11<00:24, 240MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  40% 3.96G/9.90G [00:11<00:25, 233MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  40% 4.00G/9.90G [00:12<00:26, 224MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  41% 4.03G/9.90G [00:12<00:27, 216MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  41% 4.06G/9.90G [00:12<00:28, 204MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  41% 4.09G/9.90G [00:12<00:29, 198MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 4.11G/9.90G [00:12<00:29, 197MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 4.13G/9.90G [00:12<00:30, 189MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 4.15G/9.90G [00:12<00:29, 192MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 4.17G/9.90G [00:13<00:31, 184MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 4.19G/9.90G [00:13<00:31, 183MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  43% 4.22G/9.90G [00:13<00:31, 183MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  43% 4.24G/9.90G [00:13<00:30, 187MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  43% 4.27G/9.90G [00:13<00:25, 218MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  44% 4.31G/9.90G [00:13<00:22, 254MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  44% 4.36G/9.90G [00:13<00:18, 305MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  45% 4.41G/9.90G [00:13<00:16, 342MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  45% 4.47G/9.90G [00:14<00:14, 370MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  46% 4.52G/9.90G [00:14<00:13, 400MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  46% 4.57G/9.90G [00:14<00:12, 429MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  47% 4.62G/9.90G [00:14<00:12, 433MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  47% 4.68G/9.90G [00:14<00:11, 446MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  48% 4.73G/9.90G [00:14<00:11, 455MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  48% 4.78G/9.90G [00:14<00:10, 467MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  49% 4.84G/9.90G [00:14<00:10, 486MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  49% 4.90G/9.90G [00:14<00:10, 495MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  50% 4.95G/9.90G [00:15<00:11, 421MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  51% 5.00G/9.90G [00:15<00:13, 368MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  51% 5.04G/9.90G [00:15<00:14, 345MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  51% 5.09G/9.90G [00:15<00:14, 327MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  52% 5.13G/9.90G [00:15<00:15, 312MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  52% 5.17G/9.90G [00:15<00:15, 300MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  53% 5.20G/9.90G [00:15<00:15, 295MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  53% 5.23G/9.90G [00:16<00:16, 282MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  53% 5.26G/9.90G [00:16<00:16, 276MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  53% 5.30G/9.90G [00:16<00:16, 272MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  54% 5.33G/9.90G [00:16<00:16, 269MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  54% 5.36G/9.90G [00:16<00:16, 272MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  54% 5.39G/9.90G [00:16<00:17, 264MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  55% 5.42G/9.90G [00:16<00:24, 182MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  55% 5.47G/9.90G [00:17<00:18, 242MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  56% 5.53G/9.90G [00:17<00:15, 292MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  56% 5.57G/9.90G [00:17<00:13, 317MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  57% 5.61G/9.90G [00:17<00:13, 322MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  57% 5.65G/9.90G [00:17<00:13, 317MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  57% 5.69G/9.90G [00:17<00:13, 320MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  58% 5.74G/9.90G [00:17<00:12, 326MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  58% 5.78G/9.90G [00:17<00:12, 334MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  59% 5.83G/9.90G [00:18<00:11, 369MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  59% 5.88G/9.90G [00:18<00:10, 387MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  60% 5.93G/9.90G [00:18<00:09, 400MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  60% 5.99G/9.90G [00:18<00:09, 408MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  61% 6.03G/9.90G [00:18<00:09, 408MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  61% 6.08G/9.90G [00:18<00:09, 412MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  62% 6.13G/9.90G [00:18<00:09, 418MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  62% 6.18G/9.90G [00:18<00:08, 416MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  63% 6.23G/9.90G [00:18<00:08, 426MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  63% 6.28G/9.90G [00:19<00:08, 413MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  64% 6.33G/9.90G [00:19<00:08, 416MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  64% 6.39G/9.90G [00:19<00:07, 441MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  65% 6.44G/9.90G [00:19<00:07, 460MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  66% 6.49G/9.90G [00:19<00:07, 474MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  66% 6.54G/9.90G [00:19<00:06, 486MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  67% 6.60G/9.90G [00:19<00:06, 473MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  67% 6.65G/9.90G [00:19<00:06, 478MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  68% 6.70G/9.90G [00:19<00:06, 489MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  68% 6.75G/9.90G [00:20<00:06, 484MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  69% 6.81G/9.90G [00:20<00:06, 493MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  69% 6.86G/9.90G [00:20<00:06, 499MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  70% 6.91G/9.90G [00:20<00:07, 382MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  70% 6.96G/9.90G [00:20<00:09, 324MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  71% 7.00G/9.90G [00:20<00:09, 303MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  71% 7.05G/9.90G [00:21<00:09, 286MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  71% 7.08G/9.90G [00:21<00:10, 281MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  72% 7.11G/9.90G [00:21<00:10, 274MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  72% 7.14G/9.90G [00:21<00:10, 264MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  72% 7.17G/9.90G [00:21<00:10, 259MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  73% 7.20G/9.90G [00:21<00:10, 251MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  73% 7.24G/9.90G [00:21<00:10, 243MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  73% 7.27G/9.90G [00:22<00:11, 232MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  74% 7.30G/9.90G [00:22<00:11, 222MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  74% 7.33G/9.90G [00:22<00:11, 221MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  74% 7.36G/9.90G [00:22<00:12, 210MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.39G/9.90G [00:22<00:12, 202MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.41G/9.90G [00:22<00:12, 195MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.44G/9.90G [00:22<00:12, 194MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 7.48G/9.90G [00:23<00:12, 195MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  76% 7.50G/9.90G [00:23<00:12, 198MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  76% 7.53G/9.90G [00:23<00:11, 210MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  76% 7.57G/9.90G [00:23<00:10, 215MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  77% 7.60G/9.90G [00:23<00:09, 234MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  77% 7.65G/9.90G [00:23<00:07, 297MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  78% 7.70G/9.90G [00:23<00:06, 325MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  78% 7.75G/9.90G [00:23<00:05, 371MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  79% 7.80G/9.90G [00:24<00:05, 399MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  79% 7.85G/9.90G [00:24<00:04, 416MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  80% 7.91G/9.90G [00:24<00:04, 442MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  80% 7.96G/9.90G [00:24<00:04, 459MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  81% 8.01G/9.90G [00:24<00:04, 471MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  82% 8.07G/9.90G [00:24<00:03, 489MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  82% 8.13G/9.90G [00:24<00:03, 447MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  83% 8.18G/9.90G [00:24<00:04, 399MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  83% 8.22G/9.90G [00:25<00:04, 373MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  83% 8.26G/9.90G [00:25<00:04, 358MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  84% 8.30G/9.90G [00:25<00:04, 336MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  84% 8.35G/9.90G [00:25<00:04, 321MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  85% 8.39G/9.90G [00:25<00:05, 301MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  85% 8.42G/9.90G [00:25<00:05, 293MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  85% 8.45G/9.90G [00:25<00:04, 291MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  86% 8.49G/9.90G [00:25<00:04, 285MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  86% 8.52G/9.90G [00:26<00:04, 282MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  86% 8.56G/9.90G [00:26<00:04, 278MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  87% 8.59G/9.90G [00:26<00:04, 284MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  87% 8.62G/9.90G [00:26<00:04, 278MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  87% 8.65G/9.90G [00:26<00:04, 274MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  88% 8.68G/9.90G [00:26<00:04, 271MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  88% 8.71G/9.90G [00:26<00:04, 266MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  88% 8.75G/9.90G [00:26<00:04, 273MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  89% 8.79G/9.90G [00:27<00:03, 298MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  89% 8.83G/9.90G [00:27<00:03, 314MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  90% 8.87G/9.90G [00:27<00:03, 323MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  90% 8.91G/9.90G [00:27<00:02, 338MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  90% 8.95G/9.90G [00:27<00:02, 344MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  91% 9.00G/9.90G [00:27<00:02, 350MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  91% 9.04G/9.90G [00:27<00:02, 344MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  92% 9.08G/9.90G [00:27<00:02, 335MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  92% 9.12G/9.90G [00:28<00:02, 329MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  93% 9.16G/9.90G [00:28<00:02, 323MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  93% 9.21G/9.90G [00:28<00:02, 332MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  93% 9.25G/9.90G [00:28<00:02, 314MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  94% 9.29G/9.90G [00:28<00:01, 314MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  94% 9.33G/9.90G [00:28<00:02, 221MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  95% 9.38G/9.90G [00:28<00:01, 272MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  95% 9.44G/9.90G [00:29<00:01, 312MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  96% 9.49G/9.90G [00:29<00:01, 351MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  96% 9.54G/9.90G [00:29<00:00, 375MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  97% 9.59G/9.90G [00:29<00:00, 409MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  97% 9.65G/9.90G [00:29<00:00, 424MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  98% 9.70G/9.90G [00:29<00:00, 444MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  98% 9.75G/9.90G [00:29<00:00, 459MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  99% 9.80G/9.90G [00:29<00:00, 445MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors: 100% 9.86G/9.90G [00:29<00:00, 455MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors: 100% 9.90G/9.90G [00:30<00:00, 329MB/s]\n",
            "Downloading shards:  67% 2/3 [00:58<00:29, 29.47s/it]\n",
            "Downloading (…)of-00003.safetensors:   0% 0.00/6.18G [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   0% 21.0M/6.18G [00:00<00:36, 171MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   1% 52.4M/6.18G [00:00<00:26, 230MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   2% 94.4M/6.18G [00:00<00:19, 305MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   2% 126M/6.18G [00:00<00:20, 297MB/s] \u001b[A\n",
            "Downloading (…)of-00003.safetensors:   3% 157M/6.18G [00:00<00:20, 298MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   3% 189M/6.18G [00:00<00:20, 297MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   4% 220M/6.18G [00:00<00:20, 295MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   4% 252M/6.18G [00:00<00:19, 297MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   5% 283M/6.18G [00:00<00:19, 298MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   5% 315M/6.18G [00:01<00:19, 296MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   6% 346M/6.18G [00:01<00:20, 279MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   6% 377M/6.18G [00:01<00:21, 268MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   7% 409M/6.18G [00:01<00:21, 266MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   7% 440M/6.18G [00:01<00:22, 258MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   8% 472M/6.18G [00:01<00:22, 250MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   8% 503M/6.18G [00:01<00:23, 242MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   9% 535M/6.18G [00:02<00:24, 234MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:   9% 566M/6.18G [00:02<00:23, 240MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  10% 598M/6.18G [00:02<00:24, 232MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  10% 629M/6.18G [00:02<00:24, 225MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  11% 661M/6.18G [00:02<00:24, 222MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  11% 692M/6.18G [00:02<00:24, 224MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  12% 724M/6.18G [00:02<00:24, 224MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  12% 755M/6.18G [00:02<00:24, 225MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  13% 786M/6.18G [00:03<00:23, 228MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  13% 818M/6.18G [00:03<00:22, 237MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  14% 849M/6.18G [00:03<00:21, 243MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  15% 902M/6.18G [00:03<00:17, 297MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  15% 954M/6.18G [00:03<00:15, 341MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  16% 996M/6.18G [00:03<00:14, 361MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  17% 1.05G/6.18G [00:03<00:13, 392MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  18% 1.10G/6.18G [00:03<00:12, 416MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  19% 1.15G/6.18G [00:04<00:11, 437MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  20% 1.21G/6.18G [00:04<00:10, 455MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  20% 1.26G/6.18G [00:04<00:10, 450MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  21% 1.31G/6.18G [00:04<00:10, 449MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  22% 1.36G/6.18G [00:04<00:11, 430MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  23% 1.42G/6.18G [00:04<00:12, 389MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  24% 1.46G/6.18G [00:04<00:13, 361MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  24% 1.50G/6.18G [00:04<00:14, 330MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  25% 1.54G/6.18G [00:05<00:14, 321MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  26% 1.58G/6.18G [00:05<00:14, 312MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  26% 1.63G/6.18G [00:05<00:15, 298MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  27% 1.66G/6.18G [00:05<00:15, 299MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  27% 1.69G/6.18G [00:05<00:15, 295MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  28% 1.72G/6.18G [00:05<00:15, 293MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  28% 1.75G/6.18G [00:05<00:15, 282MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  29% 1.78G/6.18G [00:05<00:15, 279MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  29% 1.81G/6.18G [00:06<00:15, 274MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  30% 1.85G/6.18G [00:06<00:16, 271MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  30% 1.88G/6.18G [00:06<00:16, 267MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  31% 1.91G/6.18G [00:06<00:15, 277MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  32% 1.95G/6.18G [00:06<00:13, 309MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  32% 1.99G/6.18G [00:06<00:12, 330MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  33% 2.04G/6.18G [00:06<00:10, 379MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  34% 2.10G/6.18G [00:06<00:09, 415MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  35% 2.15G/6.18G [00:06<00:09, 442MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  36% 2.20G/6.18G [00:07<00:08, 462MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  36% 2.25G/6.18G [00:07<00:08, 461MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  37% 2.31G/6.18G [00:07<00:08, 455MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  38% 2.36G/6.18G [00:07<00:08, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  39% 2.41G/6.18G [00:07<00:08, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  40% 2.46G/6.18G [00:07<00:08, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  41% 2.52G/6.18G [00:07<00:07, 464MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 2.57G/6.18G [00:07<00:07, 458MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  42% 2.62G/6.18G [00:08<00:08, 438MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  43% 2.67G/6.18G [00:08<00:07, 444MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  44% 2.73G/6.18G [00:08<00:07, 444MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  45% 2.78G/6.18G [00:08<00:07, 436MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  46% 2.83G/6.18G [00:08<00:07, 447MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  47% 2.88G/6.18G [00:08<00:07, 448MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  48% 2.94G/6.18G [00:08<00:07, 449MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  48% 2.99G/6.18G [00:08<00:07, 449MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  49% 3.04G/6.18G [00:09<00:10, 298MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  50% 3.09G/6.18G [00:09<00:09, 339MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  51% 3.15G/6.18G [00:09<00:08, 371MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  52% 3.20G/6.18G [00:09<00:07, 395MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  53% 3.25G/6.18G [00:09<00:06, 420MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  53% 3.30G/6.18G [00:09<00:06, 438MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  54% 3.36G/6.18G [00:09<00:06, 460MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  55% 3.41G/6.18G [00:09<00:05, 469MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  56% 3.46G/6.18G [00:09<00:05, 483MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  57% 3.51G/6.18G [00:10<00:06, 404MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  58% 3.57G/6.18G [00:10<00:07, 354MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  58% 3.61G/6.18G [00:10<00:07, 334MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  59% 3.65G/6.18G [00:10<00:07, 325MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  60% 3.69G/6.18G [00:10<00:07, 314MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  60% 3.73G/6.18G [00:10<00:08, 299MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  61% 3.76G/6.18G [00:11<00:08, 296MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  61% 3.80G/6.18G [00:11<00:08, 290MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  62% 3.83G/6.18G [00:11<00:08, 282MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  62% 3.86G/6.18G [00:11<00:08, 277MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  63% 3.89G/6.18G [00:11<00:08, 271MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  63% 3.92G/6.18G [00:11<00:08, 267MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  64% 3.95G/6.18G [00:11<00:08, 258MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  64% 3.98G/6.18G [00:11<00:08, 248MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  65% 4.02G/6.18G [00:12<00:09, 238MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  66% 4.05G/6.18G [00:12<00:09, 232MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  66% 4.08G/6.18G [00:12<00:09, 222MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  67% 4.11G/6.18G [00:12<00:09, 212MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  67% 4.14G/6.18G [00:12<00:09, 205MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  67% 4.16G/6.18G [00:12<00:09, 206MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  68% 4.18G/6.18G [00:12<00:09, 204MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  68% 4.20G/6.18G [00:13<00:09, 201MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  68% 4.23G/6.18G [00:13<00:09, 201MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  69% 4.25G/6.18G [00:13<00:09, 198MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  69% 4.28G/6.18G [00:13<00:09, 210MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  70% 4.31G/6.18G [00:13<00:07, 234MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  70% 4.35G/6.18G [00:13<00:06, 279MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  71% 4.40G/6.18G [00:13<00:05, 332MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  72% 4.46G/6.18G [00:13<00:04, 373MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  73% 4.51G/6.18G [00:13<00:04, 392MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  74% 4.56G/6.18G [00:14<00:03, 415MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 4.61G/6.18G [00:14<00:03, 415MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  75% 4.66G/6.18G [00:14<00:03, 404MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  76% 4.70G/6.18G [00:14<00:03, 401MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  77% 4.74G/6.18G [00:14<00:03, 386MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  77% 4.78G/6.18G [00:14<00:03, 367MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  78% 4.82G/6.18G [00:14<00:03, 363MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  79% 4.87G/6.18G [00:14<00:03, 352MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  79% 4.91G/6.18G [00:15<00:03, 335MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  80% 4.95G/6.18G [00:15<00:03, 328MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  81% 4.99G/6.18G [00:15<00:03, 316MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  81% 5.03G/6.18G [00:15<00:03, 305MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  82% 5.06G/6.18G [00:15<00:03, 303MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  82% 5.10G/6.18G [00:15<00:03, 290MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  83% 5.13G/6.18G [00:15<00:03, 288MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  83% 5.16G/6.18G [00:15<00:03, 281MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  84% 5.19G/6.18G [00:16<00:03, 279MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  85% 5.22G/6.18G [00:16<00:03, 272MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  85% 5.25G/6.18G [00:16<00:03, 262MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  86% 5.28G/6.18G [00:16<00:03, 262MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  86% 5.33G/6.18G [00:16<00:03, 283MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  87% 5.37G/6.18G [00:16<00:02, 314MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  88% 5.41G/6.18G [00:16<00:02, 342MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  88% 5.46G/6.18G [00:16<00:01, 373MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  89% 5.52G/6.18G [00:16<00:01, 404MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  90% 5.57G/6.18G [00:17<00:01, 411MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  91% 5.62G/6.18G [00:17<00:01, 433MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  92% 5.67G/6.18G [00:17<00:01, 448MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  93% 5.73G/6.18G [00:17<00:01, 430MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  94% 5.78G/6.18G [00:17<00:00, 429MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  94% 5.83G/6.18G [00:17<00:00, 415MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  95% 5.87G/6.18G [00:17<00:00, 414MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  96% 5.91G/6.18G [00:17<00:00, 410MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  96% 5.96G/6.18G [00:17<00:00, 402MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  97% 6.00G/6.18G [00:18<00:00, 397MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  98% 6.05G/6.18G [00:18<00:00, 409MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors:  99% 6.10G/6.18G [00:18<00:00, 419MB/s]\u001b[A\n",
            "Downloading (…)of-00003.safetensors: 100% 6.18G/6.18G [00:18<00:00, 333MB/s]\n",
            "Downloading shards: 100% 3/3 [01:17<00:00, 25.78s/it]\n",
            "Loading checkpoint shards: 100% 3/3 [00:08<00:00,  2.73s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Downloading (…)neration_config.json: 100% 188/188 [00:00<00:00, 655kB/s]\n",
            "Found 7 modules to quantize: ['q_proj', 'down_proj', 'v_proj', 'o_proj', 'up_proj', 'k_proj', 'gate_proj']\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Downloading (…)okenizer_config.json: 100% 776/776 [00:00<00:00, 2.52MB/s]\n",
            "Downloading tokenizer.model: 100% 500k/500k [00:00<00:00, 413MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.84M/1.84M [00:00<00:00, 1.96MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 414/414 [00:00<00:00, 1.58MB/s]\n",
            "Using pad_token, but it is not set yet.\n",
            "TORCH DTYPE IS torch.bfloat16\n",
            "Map: 100% 65252/65252 [00:07<00:00, 8885.83 examples/s] \n",
            "Map: 100% 4991/4991 [00:00<00:00, 10562.75 examples/s]\n",
            "  0% 0/509 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{'loss': 1.3707, 'learning_rate': 0.000125, 'epoch': 0.02}\n",
            "{'loss': 1.2265, 'learning_rate': 0.00019837728194726167, 'epoch': 0.04}\n",
            "{'loss': 1.121, 'learning_rate': 0.00019432048681541583, 'epoch': 0.06}\n",
            "{'loss': 1.1421, 'learning_rate': 0.00019026369168357, 'epoch': 0.08}\n",
            "{'loss': 1.2264, 'learning_rate': 0.00018620689655172415, 'epoch': 0.1}\n",
            " 10% 51/509 [12:47<2:16:53, 17.93s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:13,  2.33it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:09,  1.63it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:03,  1.27it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:23,  1.17it/s]\u001b[A\n",
            "  2% 6/312 [00:05<06:35,  1.29s/it]\u001b[A\n",
            "  2% 7/312 [00:06<06:06,  1.20s/it]\u001b[A\n",
            "  3% 8/312 [00:07<05:42,  1.13s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:03,  1.00s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:06,  1.02s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:56,  1.02it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:51,  1.03it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:37,  1.08it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:13,  1.18it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:30,  1.10it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:10,  1.18it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:56,  1.25it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:51,  1.27it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:56,  1.24it/s]\u001b[A\n",
            "  6% 20/312 [00:17<03:48,  1.28it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:40,  1.32it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:07,  1.17it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:54,  1.23it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:50,  1.25it/s]\u001b[A\n",
            "  8% 25/312 [00:21<03:55,  1.22it/s]\u001b[A\n",
            "  8% 26/312 [00:22<03:58,  1.20it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:52,  1.23it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:01,  1.06s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:48,  1.02s/it]\u001b[A\n",
            " 10% 30/312 [00:26<04:22,  1.07it/s]\u001b[A\n",
            " 10% 31/312 [00:27<04:25,  1.06it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:45,  1.02s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:03,  1.09s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:35,  1.01it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:33,  1.01it/s]\u001b[A\n",
            " 12% 36/312 [00:32<04:25,  1.04it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:55,  1.17it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:54,  1.17it/s]\u001b[A\n",
            " 12% 39/312 [00:35<03:57,  1.15it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:06,  1.10it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:51,  1.17it/s]\u001b[A\n",
            " 13% 42/312 [00:37<03:34,  1.26it/s]\u001b[A\n",
            " 14% 43/312 [00:38<03:49,  1.17it/s]\u001b[A\n",
            " 14% 44/312 [00:39<04:23,  1.02it/s]\u001b[A\n",
            " 14% 45/312 [00:40<04:10,  1.07it/s]\u001b[A\n",
            " 15% 46/312 [00:41<03:56,  1.12it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:46,  1.17it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:40,  1.20it/s]\u001b[A\n",
            " 16% 49/312 [00:43<03:37,  1.21it/s]\u001b[A\n",
            " 16% 50/312 [00:44<03:32,  1.23it/s]\u001b[A\n",
            " 16% 51/312 [00:45<03:56,  1.11it/s]\u001b[A\n",
            " 17% 52/312 [00:46<03:37,  1.20it/s]\u001b[A\n",
            " 17% 53/312 [00:47<03:59,  1.08it/s]\u001b[A\n",
            " 17% 54/312 [00:48<03:54,  1.10it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:34,  1.20it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:36,  1.18it/s]\u001b[A\n",
            " 18% 57/312 [00:50<03:26,  1.24it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:17,  1.29it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:18,  1.27it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:24,  1.23it/s]\u001b[A\n",
            " 20% 61/312 [00:53<03:19,  1.26it/s]\u001b[A\n",
            " 20% 62/312 [00:55<04:20,  1.04s/it]\u001b[A\n",
            " 20% 63/312 [00:56<04:13,  1.02s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:54,  1.06it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:52,  1.06it/s]\u001b[A\n",
            " 21% 66/312 [00:58<03:30,  1.17it/s]\u001b[A\n",
            " 21% 67/312 [00:59<03:17,  1.24it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:09,  1.29it/s]\u001b[A\n",
            " 22% 69/312 [01:00<03:03,  1.33it/s]\u001b[A\n",
            " 22% 70/312 [01:01<02:56,  1.37it/s]\u001b[A\n",
            " 23% 71/312 [01:02<03:01,  1.33it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:11,  1.26it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:08,  1.27it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:23,  1.17it/s]\u001b[A\n",
            " 24% 75/312 [01:05<03:11,  1.24it/s]\u001b[A\n",
            " 24% 76/312 [01:06<03:03,  1.28it/s]\u001b[A\n",
            " 25% 77/312 [01:07<03:02,  1.29it/s]\u001b[A\n",
            " 25% 78/312 [01:08<03:18,  1.18it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:15,  1.19it/s]\u001b[A\n",
            " 26% 80/312 [01:09<03:06,  1.25it/s]\u001b[A\n",
            " 26% 81/312 [01:10<03:01,  1.27it/s]\u001b[A\n",
            " 26% 82/312 [01:11<03:07,  1.23it/s]\u001b[A\n",
            " 27% 83/312 [01:12<03:07,  1.22it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:11,  1.19it/s]\u001b[A\n",
            " 27% 85/312 [01:13<03:02,  1.24it/s]\u001b[A\n",
            " 28% 86/312 [01:15<03:39,  1.03it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:32,  1.06it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:28,  1.07it/s]\u001b[A\n",
            " 29% 89/312 [01:17<03:20,  1.11it/s]\u001b[A\n",
            " 29% 90/312 [01:18<03:11,  1.16it/s]\u001b[A\n",
            " 29% 91/312 [01:19<02:57,  1.24it/s]\u001b[A\n",
            " 29% 92/312 [01:20<02:59,  1.22it/s]\u001b[A\n",
            " 30% 93/312 [01:21<02:58,  1.23it/s]\u001b[A\n",
            " 30% 94/312 [01:21<03:03,  1.19it/s]\u001b[A\n",
            " 30% 95/312 [01:22<02:47,  1.30it/s]\u001b[A\n",
            " 31% 96/312 [01:23<02:51,  1.26it/s]\u001b[A\n",
            " 31% 97/312 [01:23<02:39,  1.35it/s]\u001b[A\n",
            " 31% 98/312 [01:24<02:50,  1.25it/s]\u001b[A\n",
            " 32% 99/312 [01:26<03:21,  1.06it/s]\u001b[A\n",
            " 32% 100/312 [01:26<03:03,  1.16it/s]\u001b[A\n",
            " 32% 101/312 [01:27<02:45,  1.27it/s]\u001b[A\n",
            " 33% 102/312 [01:28<02:35,  1.35it/s]\u001b[A\n",
            " 33% 103/312 [01:28<02:33,  1.37it/s]\u001b[A\n",
            " 33% 104/312 [01:30<03:19,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:01,  1.14it/s]\u001b[A\n",
            " 34% 106/312 [01:31<03:03,  1.12it/s]\u001b[A\n",
            " 34% 107/312 [01:32<02:54,  1.18it/s]\u001b[A\n",
            " 35% 108/312 [01:34<03:50,  1.13s/it]\u001b[A\n",
            " 35% 109/312 [01:35<03:30,  1.04s/it]\u001b[A\n",
            " 35% 110/312 [01:36<03:16,  1.03it/s]\u001b[A\n",
            " 36% 111/312 [01:36<02:58,  1.12it/s]\u001b[A\n",
            " 36% 112/312 [01:37<02:42,  1.23it/s]\u001b[A\n",
            " 36% 113/312 [01:38<02:44,  1.21it/s]\u001b[A\n",
            " 37% 114/312 [01:39<02:54,  1.13it/s]\u001b[A\n",
            " 37% 115/312 [01:40<02:49,  1.16it/s]\u001b[A\n",
            " 37% 116/312 [01:41<02:57,  1.11it/s]\u001b[A\n",
            " 38% 117/312 [01:41<02:46,  1.17it/s]\u001b[A\n",
            " 38% 118/312 [01:42<02:39,  1.22it/s]\u001b[A\n",
            " 38% 119/312 [01:43<02:34,  1.25it/s]\u001b[A\n",
            " 38% 120/312 [01:44<02:26,  1.31it/s]\u001b[A\n",
            " 39% 121/312 [01:45<02:49,  1.13it/s]\u001b[A\n",
            " 39% 122/312 [01:46<02:49,  1.12it/s]\u001b[A\n",
            " 39% 123/312 [01:46<02:34,  1.22it/s]\u001b[A\n",
            " 40% 124/312 [01:47<02:32,  1.24it/s]\u001b[A\n",
            " 40% 125/312 [01:49<03:58,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:50<03:32,  1.14s/it]\u001b[A\n",
            " 41% 127/312 [01:51<03:15,  1.06s/it]\u001b[A\n",
            " 41% 128/312 [01:52<02:54,  1.05it/s]\u001b[A\n",
            " 41% 129/312 [01:53<02:45,  1.10it/s]\u001b[A\n",
            " 42% 130/312 [01:53<02:40,  1.14it/s]\u001b[A\n",
            " 42% 131/312 [01:54<02:27,  1.22it/s]\u001b[A\n",
            " 42% 132/312 [01:55<02:25,  1.24it/s]\u001b[A\n",
            " 43% 133/312 [01:56<02:33,  1.16it/s]\u001b[A\n",
            " 43% 134/312 [01:57<02:30,  1.18it/s]\u001b[A\n",
            " 43% 135/312 [01:57<02:19,  1.27it/s]\u001b[A\n",
            " 44% 136/312 [01:58<02:12,  1.33it/s]\u001b[A\n",
            " 44% 137/312 [01:59<02:19,  1.26it/s]\u001b[A\n",
            " 44% 138/312 [02:00<02:23,  1.21it/s]\u001b[A\n",
            " 45% 139/312 [02:01<02:20,  1.23it/s]\u001b[A\n",
            " 45% 140/312 [02:01<02:19,  1.23it/s]\u001b[A\n",
            " 45% 141/312 [02:03<02:38,  1.08it/s]\u001b[A\n",
            " 46% 142/312 [02:04<03:12,  1.13s/it]\u001b[A\n",
            " 46% 143/312 [02:05<02:53,  1.03s/it]\u001b[A\n",
            " 46% 144/312 [02:06<02:38,  1.06it/s]\u001b[A\n",
            " 46% 145/312 [02:07<02:29,  1.12it/s]\u001b[A\n",
            " 47% 146/312 [02:07<02:24,  1.15it/s]\u001b[A\n",
            " 47% 147/312 [02:08<02:23,  1.15it/s]\u001b[A\n",
            " 47% 148/312 [02:09<02:23,  1.14it/s]\u001b[A\n",
            " 48% 149/312 [02:10<02:14,  1.21it/s]\u001b[A\n",
            " 48% 150/312 [02:10<02:07,  1.27it/s]\u001b[A\n",
            " 48% 151/312 [02:12<02:19,  1.16it/s]\u001b[A\n",
            " 49% 152/312 [02:13<03:06,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:14<02:42,  1.02s/it]\u001b[A\n",
            " 49% 154/312 [02:16<03:06,  1.18s/it]\u001b[A\n",
            " 50% 155/312 [02:17<03:12,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:18<03:15,  1.25s/it]\u001b[A\n",
            " 50% 157/312 [02:20<03:14,  1.25s/it]\u001b[A\n",
            " 51% 158/312 [02:20<02:56,  1.14s/it]\u001b[A\n",
            " 51% 159/312 [02:22<02:53,  1.14s/it]\u001b[A\n",
            " 51% 160/312 [02:22<02:37,  1.03s/it]\u001b[A\n",
            " 52% 161/312 [02:23<02:23,  1.05it/s]\u001b[A\n",
            " 52% 162/312 [02:24<02:13,  1.12it/s]\u001b[A\n",
            " 52% 163/312 [02:25<02:09,  1.15it/s]\u001b[A\n",
            " 53% 164/312 [02:26<02:11,  1.12it/s]\u001b[A\n",
            " 53% 165/312 [02:27<02:29,  1.02s/it]\u001b[A\n",
            " 53% 166/312 [02:29<02:59,  1.23s/it]\u001b[A\n",
            " 54% 167/312 [02:29<02:36,  1.08s/it]\u001b[A\n",
            " 54% 168/312 [02:30<02:25,  1.01s/it]\u001b[A\n",
            " 54% 169/312 [02:31<02:14,  1.07it/s]\u001b[A\n",
            " 54% 170/312 [02:32<02:04,  1.14it/s]\u001b[A\n",
            " 55% 171/312 [02:32<01:55,  1.22it/s]\u001b[A\n",
            " 55% 172/312 [02:33<01:54,  1.22it/s]\u001b[A\n",
            " 55% 173/312 [02:35<02:14,  1.03it/s]\u001b[A\n",
            " 56% 174/312 [02:35<02:11,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:36<01:59,  1.14it/s]\u001b[A\n",
            " 56% 176/312 [02:37<01:52,  1.21it/s]\u001b[A\n",
            " 57% 177/312 [02:38<01:49,  1.23it/s]\u001b[A\n",
            " 57% 178/312 [02:38<01:48,  1.23it/s]\u001b[A\n",
            " 57% 179/312 [02:39<01:45,  1.26it/s]\u001b[A\n",
            " 58% 180/312 [02:40<01:40,  1.31it/s]\u001b[A\n",
            " 58% 181/312 [02:41<01:36,  1.35it/s]\u001b[A\n",
            " 58% 182/312 [02:41<01:38,  1.31it/s]\u001b[A\n",
            " 59% 183/312 [02:43<01:52,  1.15it/s]\u001b[A\n",
            " 59% 184/312 [02:44<02:01,  1.05it/s]\u001b[A\n",
            " 59% 185/312 [02:44<01:55,  1.10it/s]\u001b[A\n",
            " 60% 186/312 [02:45<01:58,  1.07it/s]\u001b[A\n",
            " 60% 187/312 [02:46<01:50,  1.13it/s]\u001b[A\n",
            " 60% 188/312 [02:47<01:42,  1.21it/s]\u001b[A\n",
            " 61% 189/312 [02:48<01:36,  1.28it/s]\u001b[A\n",
            " 61% 190/312 [02:48<01:34,  1.28it/s]\u001b[A\n",
            " 61% 191/312 [02:49<01:35,  1.27it/s]\u001b[A\n",
            " 62% 192/312 [02:50<01:35,  1.26it/s]\u001b[A\n",
            " 62% 193/312 [02:51<01:31,  1.31it/s]\u001b[A\n",
            " 62% 194/312 [02:52<01:31,  1.28it/s]\u001b[A\n",
            " 62% 195/312 [02:52<01:29,  1.31it/s]\u001b[A\n",
            " 63% 196/312 [02:53<01:25,  1.36it/s]\u001b[A\n",
            " 63% 197/312 [02:54<01:23,  1.38it/s]\u001b[A\n",
            " 63% 198/312 [02:54<01:21,  1.40it/s]\u001b[A\n",
            " 64% 199/312 [02:55<01:20,  1.41it/s]\u001b[A\n",
            " 64% 200/312 [02:56<01:24,  1.32it/s]\u001b[A\n",
            " 64% 201/312 [02:57<01:27,  1.27it/s]\u001b[A\n",
            " 65% 202/312 [02:58<01:26,  1.27it/s]\u001b[A\n",
            " 65% 203/312 [02:58<01:29,  1.22it/s]\u001b[A\n",
            " 65% 204/312 [02:59<01:25,  1.27it/s]\u001b[A\n",
            " 66% 205/312 [03:00<01:20,  1.34it/s]\u001b[A\n",
            " 66% 206/312 [03:00<01:17,  1.37it/s]\u001b[A\n",
            " 66% 207/312 [03:01<01:20,  1.30it/s]\u001b[A\n",
            " 67% 208/312 [03:02<01:30,  1.15it/s]\u001b[A\n",
            " 67% 209/312 [03:03<01:22,  1.25it/s]\u001b[A\n",
            " 67% 210/312 [03:04<01:20,  1.26it/s]\u001b[A\n",
            " 68% 211/312 [03:05<01:35,  1.06it/s]\u001b[A\n",
            " 68% 212/312 [03:06<01:30,  1.11it/s]\u001b[A\n",
            " 68% 213/312 [03:07<01:30,  1.10it/s]\u001b[A\n",
            " 69% 214/312 [03:08<01:25,  1.15it/s]\u001b[A\n",
            " 69% 215/312 [03:09<01:24,  1.15it/s]\u001b[A\n",
            " 69% 216/312 [03:10<01:28,  1.08it/s]\u001b[A\n",
            " 70% 217/312 [03:10<01:23,  1.14it/s]\u001b[A\n",
            " 70% 218/312 [03:11<01:22,  1.14it/s]\u001b[A\n",
            " 70% 219/312 [03:13<01:36,  1.04s/it]\u001b[A\n",
            " 71% 220/312 [03:14<01:43,  1.12s/it]\u001b[A\n",
            " 71% 221/312 [03:15<01:37,  1.07s/it]\u001b[A\n",
            " 71% 222/312 [03:16<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:17<01:23,  1.07it/s]\u001b[A\n",
            " 72% 224/312 [03:17<01:13,  1.20it/s]\u001b[A\n",
            " 72% 225/312 [03:18<01:18,  1.11it/s]\u001b[A\n",
            " 72% 226/312 [03:19<01:13,  1.17it/s]\u001b[A\n",
            " 73% 227/312 [03:20<01:16,  1.11it/s]\u001b[A\n",
            " 73% 228/312 [03:21<01:13,  1.15it/s]\u001b[A\n",
            " 73% 229/312 [03:21<01:07,  1.24it/s]\u001b[A\n",
            " 74% 230/312 [03:22<01:04,  1.28it/s]\u001b[A\n",
            " 74% 231/312 [03:23<01:05,  1.24it/s]\u001b[A\n",
            " 74% 232/312 [03:24<00:59,  1.34it/s]\u001b[A\n",
            " 75% 233/312 [03:25<01:10,  1.12it/s]\u001b[A\n",
            " 75% 234/312 [03:25<01:04,  1.21it/s]\u001b[A\n",
            " 75% 235/312 [03:26<01:04,  1.18it/s]\u001b[A\n",
            " 76% 236/312 [03:27<01:00,  1.25it/s]\u001b[A\n",
            " 76% 237/312 [03:28<01:00,  1.25it/s]\u001b[A\n",
            " 76% 238/312 [03:29<00:56,  1.31it/s]\u001b[A\n",
            " 77% 239/312 [03:29<00:57,  1.26it/s]\u001b[A\n",
            " 77% 240/312 [03:30<00:56,  1.27it/s]\u001b[A\n",
            " 77% 241/312 [03:31<00:57,  1.24it/s]\u001b[A\n",
            " 78% 242/312 [03:32<01:04,  1.09it/s]\u001b[A\n",
            " 78% 243/312 [03:34<01:22,  1.19s/it]\u001b[A\n",
            " 78% 244/312 [03:35<01:11,  1.05s/it]\u001b[A\n",
            " 79% 245/312 [03:36<01:08,  1.02s/it]\u001b[A\n",
            " 79% 246/312 [03:37<01:07,  1.02s/it]\u001b[A\n",
            " 79% 247/312 [03:37<01:00,  1.08it/s]\u001b[A\n",
            " 79% 248/312 [03:38<00:55,  1.14it/s]\u001b[A\n",
            " 80% 249/312 [03:39<00:53,  1.18it/s]\u001b[A\n",
            " 80% 250/312 [03:40<00:56,  1.09it/s]\u001b[A\n",
            " 80% 251/312 [03:41<00:53,  1.15it/s]\u001b[A\n",
            " 81% 252/312 [03:42<00:51,  1.17it/s]\u001b[A\n",
            " 81% 253/312 [03:42<00:50,  1.16it/s]\u001b[A\n",
            " 81% 254/312 [03:43<00:49,  1.16it/s]\u001b[A\n",
            " 82% 255/312 [03:44<00:49,  1.16it/s]\u001b[A\n",
            " 82% 256/312 [03:45<00:45,  1.22it/s]\u001b[A\n",
            " 82% 257/312 [03:46<00:49,  1.10it/s]\u001b[A\n",
            " 83% 258/312 [03:47<00:46,  1.17it/s]\u001b[A\n",
            " 83% 259/312 [03:48<00:47,  1.11it/s]\u001b[A\n",
            " 83% 260/312 [03:48<00:42,  1.21it/s]\u001b[A\n",
            " 84% 261/312 [03:49<00:40,  1.25it/s]\u001b[A\n",
            " 84% 262/312 [03:50<00:41,  1.22it/s]\u001b[A\n",
            " 84% 263/312 [03:51<00:39,  1.23it/s]\u001b[A\n",
            " 85% 264/312 [03:53<00:59,  1.24s/it]\u001b[A\n",
            " 85% 265/312 [03:55<01:05,  1.40s/it]\u001b[A\n",
            " 85% 266/312 [03:56<00:56,  1.23s/it]\u001b[A\n",
            " 86% 267/312 [03:57<00:52,  1.17s/it]\u001b[A\n",
            " 86% 268/312 [03:57<00:46,  1.05s/it]\u001b[A\n",
            " 86% 269/312 [03:58<00:43,  1.01s/it]\u001b[A\n",
            " 87% 270/312 [03:59<00:39,  1.07it/s]\u001b[A\n",
            " 87% 271/312 [04:00<00:35,  1.16it/s]\u001b[A\n",
            " 87% 272/312 [04:01<00:33,  1.19it/s]\u001b[A\n",
            " 88% 273/312 [04:02<00:36,  1.08it/s]\u001b[A\n",
            " 88% 274/312 [04:03<00:33,  1.12it/s]\u001b[A\n",
            " 88% 275/312 [04:03<00:30,  1.20it/s]\u001b[A\n",
            " 88% 276/312 [04:04<00:28,  1.28it/s]\u001b[A\n",
            " 89% 277/312 [04:05<00:31,  1.13it/s]\u001b[A\n",
            " 89% 278/312 [04:06<00:30,  1.12it/s]\u001b[A\n",
            " 89% 279/312 [04:07<00:29,  1.12it/s]\u001b[A\n",
            " 90% 280/312 [04:08<00:28,  1.13it/s]\u001b[A\n",
            " 90% 281/312 [04:08<00:25,  1.21it/s]\u001b[A\n",
            " 90% 282/312 [04:09<00:23,  1.26it/s]\u001b[A\n",
            " 91% 283/312 [04:10<00:25,  1.13it/s]\u001b[A\n",
            " 91% 284/312 [04:11<00:23,  1.17it/s]\u001b[A\n",
            " 91% 285/312 [04:12<00:22,  1.20it/s]\u001b[A\n",
            " 92% 286/312 [04:13<00:21,  1.19it/s]\u001b[A\n",
            " 92% 287/312 [04:14<00:21,  1.17it/s]\u001b[A\n",
            " 92% 288/312 [04:14<00:19,  1.23it/s]\u001b[A\n",
            " 93% 289/312 [04:15<00:18,  1.27it/s]\u001b[A\n",
            " 93% 290/312 [04:16<00:17,  1.26it/s]\u001b[A\n",
            " 93% 291/312 [04:17<00:16,  1.26it/s]\u001b[A\n",
            " 94% 292/312 [04:18<00:17,  1.12it/s]\u001b[A\n",
            " 94% 293/312 [04:19<00:17,  1.07it/s]\u001b[A\n",
            " 94% 294/312 [04:19<00:15,  1.17it/s]\u001b[A\n",
            " 95% 295/312 [04:20<00:13,  1.22it/s]\u001b[A\n",
            " 95% 296/312 [04:21<00:13,  1.23it/s]\u001b[A\n",
            " 95% 297/312 [04:22<00:12,  1.17it/s]\u001b[A\n",
            " 96% 298/312 [04:23<00:11,  1.19it/s]\u001b[A\n",
            " 96% 299/312 [04:24<00:13,  1.02s/it]\u001b[A\n",
            " 96% 300/312 [04:25<00:12,  1.06s/it]\u001b[A\n",
            " 96% 301/312 [04:26<00:11,  1.04s/it]\u001b[A\n",
            " 97% 302/312 [04:27<00:09,  1.06it/s]\u001b[A\n",
            " 97% 303/312 [04:28<00:08,  1.05it/s]\u001b[A\n",
            " 97% 304/312 [04:29<00:07,  1.12it/s]\u001b[A\n",
            " 98% 305/312 [04:30<00:06,  1.17it/s]\u001b[A\n",
            " 98% 306/312 [04:30<00:05,  1.13it/s]\u001b[A\n",
            " 98% 307/312 [04:31<00:04,  1.20it/s]\u001b[A\n",
            " 99% 308/312 [04:32<00:03,  1.15it/s]\u001b[A\n",
            " 99% 309/312 [04:33<00:02,  1.24it/s]\u001b[A\n",
            " 99% 310/312 [04:33<00:01,  1.29it/s]\u001b[A\n",
            "100% 311/312 [04:34<00:00,  1.33it/s]\u001b[A\n",
            "100% 312/312 [04:35<00:00,  1.37it/s]\u001b[A\n",
            "{'eval_loss': 1.1930088996887207, 'eval_runtime': 276.6326, 'eval_samples_per_second': 18.042, 'eval_steps_per_second': 1.128, 'epoch': 0.1}\n",
            "\n",
            " 10% 51/509 [17:23<2:16:53, 17.93s/it]\n",
            "{'loss': 1.2672, 'learning_rate': 0.00018215010141987828, 'epoch': 0.12}\n",
            "{'loss': 1.1692, 'learning_rate': 0.00017809330628803247, 'epoch': 0.14}\n",
            "{'loss': 1.1042, 'learning_rate': 0.00017403651115618663, 'epoch': 0.16}\n",
            "{'loss': 1.1341, 'learning_rate': 0.00016997971602434076, 'epoch': 0.18}\n",
            "{'loss': 1.2074, 'learning_rate': 0.00016592292089249492, 'epoch': 0.2}\n",
            " 20% 102/509 [30:10<2:12:17, 19.50s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:16,  2.27it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:12,  1.61it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:06,  1.25it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:26,  1.15it/s]\u001b[A\n",
            "  2% 6/312 [00:06<06:38,  1.30s/it]\u001b[A\n",
            "  2% 7/312 [00:07<06:09,  1.21s/it]\u001b[A\n",
            "  3% 8/312 [00:08<05:44,  1.13s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:06,  1.01s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:09,  1.02s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:58,  1.01it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:53,  1.02it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:38,  1.07it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:14,  1.17it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:31,  1.09it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:12,  1.17it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:58,  1.24it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:53,  1.26it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:58,  1.23it/s]\u001b[A\n",
            "  6% 20/312 [00:17<03:50,  1.27it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:41,  1.31it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:09,  1.16it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:56,  1.22it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:52,  1.24it/s]\u001b[A\n",
            "  8% 25/312 [00:22<03:57,  1.21it/s]\u001b[A\n",
            "  8% 26/312 [00:23<04:00,  1.19it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:54,  1.22it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:03,  1.07s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:50,  1.03s/it]\u001b[A\n",
            " 10% 30/312 [00:27<04:24,  1.07it/s]\u001b[A\n",
            " 10% 31/312 [00:28<04:27,  1.05it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:47,  1.03s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:04,  1.09s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:37,  1.00it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:35,  1.00it/s]\u001b[A\n",
            " 12% 36/312 [00:33<04:27,  1.03it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:57,  1.16it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:55,  1.16it/s]\u001b[A\n",
            " 12% 39/312 [00:35<03:58,  1.14it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:07,  1.10it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:51,  1.17it/s]\u001b[A\n",
            " 13% 42/312 [00:37<03:35,  1.25it/s]\u001b[A\n",
            " 14% 43/312 [00:38<03:49,  1.17it/s]\u001b[A\n",
            " 14% 44/312 [00:40<04:24,  1.01it/s]\u001b[A\n",
            " 14% 45/312 [00:41<04:10,  1.06it/s]\u001b[A\n",
            " 15% 46/312 [00:41<03:56,  1.12it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:47,  1.17it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:40,  1.20it/s]\u001b[A\n",
            " 16% 49/312 [00:44<03:37,  1.21it/s]\u001b[A\n",
            " 16% 50/312 [00:44<03:32,  1.23it/s]\u001b[A\n",
            " 16% 51/312 [00:46<03:55,  1.11it/s]\u001b[A\n",
            " 17% 52/312 [00:46<03:37,  1.20it/s]\u001b[A\n",
            " 17% 53/312 [00:47<03:57,  1.09it/s]\u001b[A\n",
            " 17% 54/312 [00:48<03:53,  1.11it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:33,  1.20it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:35,  1.19it/s]\u001b[A\n",
            " 18% 57/312 [00:51<03:25,  1.24it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:16,  1.29it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:19,  1.27it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:25,  1.23it/s]\u001b[A\n",
            " 20% 61/312 [00:54<03:19,  1.26it/s]\u001b[A\n",
            " 20% 62/312 [00:55<04:20,  1.04s/it]\u001b[A\n",
            " 20% 63/312 [00:56<04:13,  1.02s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:54,  1.06it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:52,  1.06it/s]\u001b[A\n",
            " 21% 66/312 [00:59<03:32,  1.16it/s]\u001b[A\n",
            " 21% 67/312 [00:59<03:19,  1.23it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:10,  1.28it/s]\u001b[A\n",
            " 22% 69/312 [01:01<03:04,  1.32it/s]\u001b[A\n",
            " 22% 70/312 [01:01<02:57,  1.36it/s]\u001b[A\n",
            " 23% 71/312 [01:02<03:02,  1.32it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:12,  1.25it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:09,  1.26it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:25,  1.16it/s]\u001b[A\n",
            " 24% 75/312 [01:06<03:13,  1.23it/s]\u001b[A\n",
            " 24% 76/312 [01:06<03:04,  1.28it/s]\u001b[A\n",
            " 25% 77/312 [01:07<03:03,  1.28it/s]\u001b[A\n",
            " 25% 78/312 [01:08<03:19,  1.17it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:16,  1.19it/s]\u001b[A\n",
            " 26% 80/312 [01:10<03:07,  1.24it/s]\u001b[A\n",
            " 26% 81/312 [01:10<03:03,  1.26it/s]\u001b[A\n",
            " 26% 82/312 [01:11<03:09,  1.22it/s]\u001b[A\n",
            " 27% 83/312 [01:12<03:08,  1.21it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:13,  1.18it/s]\u001b[A\n",
            " 27% 85/312 [01:14<03:04,  1.23it/s]\u001b[A\n",
            " 28% 86/312 [01:15<03:41,  1.02it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:34,  1.05it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:30,  1.06it/s]\u001b[A\n",
            " 29% 89/312 [01:18<03:21,  1.10it/s]\u001b[A\n",
            " 29% 90/312 [01:19<03:13,  1.15it/s]\u001b[A\n",
            " 29% 91/312 [01:19<02:59,  1.23it/s]\u001b[A\n",
            " 29% 92/312 [01:20<03:01,  1.21it/s]\u001b[A\n",
            " 30% 93/312 [01:21<03:00,  1.22it/s]\u001b[A\n",
            " 30% 94/312 [01:22<03:04,  1.18it/s]\u001b[A\n",
            " 30% 95/312 [01:22<02:48,  1.29it/s]\u001b[A\n",
            " 31% 96/312 [01:23<02:53,  1.25it/s]\u001b[A\n",
            " 31% 97/312 [01:24<02:40,  1.34it/s]\u001b[A\n",
            " 31% 98/312 [01:25<02:52,  1.24it/s]\u001b[A\n",
            " 32% 99/312 [01:26<03:22,  1.05it/s]\u001b[A\n",
            " 32% 100/312 [01:27<03:04,  1.15it/s]\u001b[A\n",
            " 32% 101/312 [01:27<02:47,  1.26it/s]\u001b[A\n",
            " 33% 102/312 [01:28<02:36,  1.34it/s]\u001b[A\n",
            " 33% 103/312 [01:29<02:34,  1.35it/s]\u001b[A\n",
            " 33% 104/312 [01:30<03:20,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:02,  1.13it/s]\u001b[A\n",
            " 34% 106/312 [01:32<03:05,  1.11it/s]\u001b[A\n",
            " 34% 107/312 [01:33<02:55,  1.17it/s]\u001b[A\n",
            " 35% 108/312 [01:34<03:51,  1.14s/it]\u001b[A\n",
            " 35% 109/312 [01:35<03:31,  1.04s/it]\u001b[A\n",
            " 35% 110/312 [01:36<03:17,  1.02it/s]\u001b[A\n",
            " 36% 111/312 [01:37<03:00,  1.12it/s]\u001b[A\n",
            " 36% 112/312 [01:37<02:44,  1.22it/s]\u001b[A\n",
            " 36% 113/312 [01:38<02:45,  1.20it/s]\u001b[A\n",
            " 37% 114/312 [01:39<02:55,  1.13it/s]\u001b[A\n",
            " 37% 115/312 [01:40<02:51,  1.15it/s]\u001b[A\n",
            " 37% 116/312 [01:41<02:58,  1.10it/s]\u001b[A\n",
            " 38% 117/312 [01:42<02:47,  1.16it/s]\u001b[A\n",
            " 38% 118/312 [01:43<02:40,  1.21it/s]\u001b[A\n",
            " 38% 119/312 [01:43<02:36,  1.23it/s]\u001b[A\n",
            " 38% 120/312 [01:44<02:27,  1.30it/s]\u001b[A\n",
            " 39% 121/312 [01:45<02:51,  1.11it/s]\u001b[A\n",
            " 39% 122/312 [01:46<02:51,  1.11it/s]\u001b[A\n",
            " 39% 123/312 [01:47<02:36,  1.21it/s]\u001b[A\n",
            " 40% 124/312 [01:48<02:33,  1.22it/s]\u001b[A\n",
            " 40% 125/312 [01:50<03:59,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:51<03:33,  1.15s/it]\u001b[A\n",
            " 41% 127/312 [01:52<03:16,  1.06s/it]\u001b[A\n",
            " 41% 128/312 [01:52<02:55,  1.05it/s]\u001b[A\n",
            " 41% 129/312 [01:53<02:46,  1.10it/s]\u001b[A\n",
            " 42% 130/312 [01:54<02:40,  1.13it/s]\u001b[A\n",
            " 42% 131/312 [01:55<02:28,  1.22it/s]\u001b[A\n",
            " 42% 132/312 [01:56<02:26,  1.23it/s]\u001b[A\n",
            " 43% 133/312 [01:57<02:34,  1.16it/s]\u001b[A\n",
            " 43% 134/312 [01:57<02:31,  1.17it/s]\u001b[A\n",
            " 43% 135/312 [01:58<02:21,  1.25it/s]\u001b[A\n",
            " 44% 136/312 [01:59<02:14,  1.31it/s]\u001b[A\n",
            " 44% 137/312 [02:00<02:20,  1.24it/s]\u001b[A\n",
            " 44% 138/312 [02:01<02:25,  1.20it/s]\u001b[A\n",
            " 45% 139/312 [02:01<02:22,  1.21it/s]\u001b[A\n",
            " 45% 140/312 [02:02<02:21,  1.22it/s]\u001b[A\n",
            " 45% 141/312 [02:03<02:40,  1.06it/s]\u001b[A\n",
            " 46% 142/312 [02:05<03:14,  1.14s/it]\u001b[A\n",
            " 46% 143/312 [02:06<02:55,  1.04s/it]\u001b[A\n",
            " 46% 144/312 [02:07<02:40,  1.05it/s]\u001b[A\n",
            " 46% 145/312 [02:07<02:31,  1.10it/s]\u001b[A\n",
            " 47% 146/312 [02:08<02:25,  1.14it/s]\u001b[A\n",
            " 47% 147/312 [02:09<02:24,  1.14it/s]\u001b[A\n",
            " 47% 148/312 [02:10<02:25,  1.13it/s]\u001b[A\n",
            " 48% 149/312 [02:11<02:15,  1.20it/s]\u001b[A\n",
            " 48% 150/312 [02:11<02:08,  1.26it/s]\u001b[A\n",
            " 48% 151/312 [02:12<02:20,  1.14it/s]\u001b[A\n",
            " 49% 152/312 [02:14<03:07,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:15<02:43,  1.03s/it]\u001b[A\n",
            " 49% 154/312 [02:16<03:07,  1.19s/it]\u001b[A\n",
            " 50% 155/312 [02:18<03:13,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:19<03:16,  1.26s/it]\u001b[A\n",
            " 50% 157/312 [02:20<03:15,  1.26s/it]\u001b[A\n",
            " 51% 158/312 [02:21<02:57,  1.15s/it]\u001b[A\n",
            " 51% 159/312 [02:22<02:55,  1.15s/it]\u001b[A\n",
            " 51% 160/312 [02:23<02:38,  1.04s/it]\u001b[A\n",
            " 52% 161/312 [02:24<02:25,  1.04it/s]\u001b[A\n",
            " 52% 162/312 [02:25<02:15,  1.11it/s]\u001b[A\n",
            " 52% 163/312 [02:26<02:10,  1.14it/s]\u001b[A\n",
            " 53% 164/312 [02:27<02:13,  1.11it/s]\u001b[A\n",
            " 53% 165/312 [02:28<02:31,  1.03s/it]\u001b[A\n",
            " 53% 166/312 [02:30<03:01,  1.24s/it]\u001b[A\n",
            " 54% 167/312 [02:30<02:37,  1.09s/it]\u001b[A\n",
            " 54% 168/312 [02:31<02:27,  1.02s/it]\u001b[A\n",
            " 54% 169/312 [02:32<02:15,  1.06it/s]\u001b[A\n",
            " 54% 170/312 [02:33<02:04,  1.14it/s]\u001b[A\n",
            " 55% 171/312 [02:33<01:56,  1.21it/s]\u001b[A\n",
            " 55% 172/312 [02:34<01:55,  1.21it/s]\u001b[A\n",
            " 55% 173/312 [02:36<02:15,  1.03it/s]\u001b[A\n",
            " 56% 174/312 [02:36<02:11,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:37<02:00,  1.13it/s]\u001b[A\n",
            " 56% 176/312 [02:38<01:53,  1.20it/s]\u001b[A\n",
            " 57% 177/312 [02:39<01:50,  1.22it/s]\u001b[A\n",
            " 57% 178/312 [02:40<01:49,  1.22it/s]\u001b[A\n",
            " 57% 179/312 [02:40<01:46,  1.25it/s]\u001b[A\n",
            " 58% 180/312 [02:41<01:41,  1.30it/s]\u001b[A\n",
            " 58% 181/312 [02:42<01:37,  1.34it/s]\u001b[A\n",
            " 58% 182/312 [02:42<01:39,  1.30it/s]\u001b[A\n",
            " 59% 183/312 [02:44<01:53,  1.14it/s]\u001b[A\n",
            " 59% 184/312 [02:45<02:02,  1.04it/s]\u001b[A\n",
            " 59% 185/312 [02:46<01:56,  1.09it/s]\u001b[A\n",
            " 60% 186/312 [02:47<01:59,  1.06it/s]\u001b[A\n",
            " 60% 187/312 [02:47<01:51,  1.12it/s]\u001b[A\n",
            " 60% 188/312 [02:48<01:42,  1.21it/s]\u001b[A\n",
            " 61% 189/312 [02:49<01:36,  1.27it/s]\u001b[A\n",
            " 61% 190/312 [02:50<01:35,  1.27it/s]\u001b[A\n",
            " 61% 191/312 [02:50<01:36,  1.26it/s]\u001b[A\n",
            " 62% 192/312 [02:51<01:36,  1.24it/s]\u001b[A\n",
            " 62% 193/312 [02:52<01:31,  1.30it/s]\u001b[A\n",
            " 62% 194/312 [02:53<01:32,  1.27it/s]\u001b[A\n",
            " 62% 195/312 [02:53<01:30,  1.30it/s]\u001b[A\n",
            " 63% 196/312 [02:54<01:26,  1.35it/s]\u001b[A\n",
            " 63% 197/312 [02:55<01:24,  1.36it/s]\u001b[A\n",
            " 63% 198/312 [02:56<01:22,  1.38it/s]\u001b[A\n",
            " 64% 199/312 [02:56<01:20,  1.40it/s]\u001b[A\n",
            " 64% 200/312 [02:57<01:25,  1.31it/s]\u001b[A\n",
            " 64% 201/312 [02:58<01:28,  1.25it/s]\u001b[A\n",
            " 65% 202/312 [02:59<01:27,  1.25it/s]\u001b[A\n",
            " 65% 203/312 [03:00<01:30,  1.21it/s]\u001b[A\n",
            " 65% 204/312 [03:00<01:26,  1.25it/s]\u001b[A\n",
            " 66% 205/312 [03:01<01:21,  1.32it/s]\u001b[A\n",
            " 66% 206/312 [03:02<01:18,  1.35it/s]\u001b[A\n",
            " 66% 207/312 [03:03<01:21,  1.29it/s]\u001b[A\n",
            " 67% 208/312 [03:04<01:31,  1.14it/s]\u001b[A\n",
            " 67% 209/312 [03:04<01:23,  1.24it/s]\u001b[A\n",
            " 67% 210/312 [03:05<01:21,  1.25it/s]\u001b[A\n",
            " 68% 211/312 [03:06<01:36,  1.05it/s]\u001b[A\n",
            " 68% 212/312 [03:07<01:30,  1.10it/s]\u001b[A\n",
            " 68% 213/312 [03:08<01:30,  1.09it/s]\u001b[A\n",
            " 69% 214/312 [03:09<01:25,  1.14it/s]\u001b[A\n",
            " 69% 215/312 [03:10<01:24,  1.14it/s]\u001b[A\n",
            " 69% 216/312 [03:11<01:29,  1.07it/s]\u001b[A\n",
            " 70% 217/312 [03:12<01:24,  1.13it/s]\u001b[A\n",
            " 70% 218/312 [03:13<01:22,  1.13it/s]\u001b[A\n",
            " 70% 219/312 [03:14<01:37,  1.05s/it]\u001b[A\n",
            " 71% 220/312 [03:15<01:43,  1.13s/it]\u001b[A\n",
            " 71% 221/312 [03:16<01:38,  1.08s/it]\u001b[A\n",
            " 71% 222/312 [03:17<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:18<01:24,  1.06it/s]\u001b[A\n",
            " 72% 224/312 [03:18<01:13,  1.19it/s]\u001b[A\n",
            " 72% 225/312 [03:20<01:18,  1.11it/s]\u001b[A\n",
            " 72% 226/312 [03:20<01:14,  1.16it/s]\u001b[A\n",
            " 73% 227/312 [03:21<01:17,  1.10it/s]\u001b[A\n",
            " 73% 228/312 [03:22<01:13,  1.14it/s]\u001b[A\n",
            " 73% 229/312 [03:23<01:07,  1.22it/s]\u001b[A\n",
            " 74% 230/312 [03:24<01:04,  1.27it/s]\u001b[A\n",
            " 74% 231/312 [03:24<01:05,  1.23it/s]\u001b[A\n",
            " 74% 232/312 [03:25<01:00,  1.33it/s]\u001b[A\n",
            " 75% 233/312 [03:26<01:11,  1.11it/s]\u001b[A\n",
            " 75% 234/312 [03:27<01:05,  1.20it/s]\u001b[A\n",
            " 75% 235/312 [03:28<01:05,  1.17it/s]\u001b[A\n",
            " 76% 236/312 [03:29<01:01,  1.24it/s]\u001b[A\n",
            " 76% 237/312 [03:29<01:00,  1.24it/s]\u001b[A\n",
            " 76% 238/312 [03:30<00:56,  1.30it/s]\u001b[A\n",
            " 77% 239/312 [03:31<00:58,  1.25it/s]\u001b[A\n",
            " 77% 240/312 [03:32<00:57,  1.26it/s]\u001b[A\n",
            " 77% 241/312 [03:33<00:57,  1.23it/s]\u001b[A\n",
            " 78% 242/312 [03:34<01:04,  1.08it/s]\u001b[A\n",
            " 78% 243/312 [03:36<01:22,  1.20s/it]\u001b[A\n",
            " 78% 244/312 [03:36<01:12,  1.06s/it]\u001b[A\n",
            " 79% 245/312 [03:37<01:09,  1.03s/it]\u001b[A\n",
            " 79% 246/312 [03:38<01:07,  1.03s/it]\u001b[A\n",
            " 79% 247/312 [03:39<01:00,  1.07it/s]\u001b[A\n",
            " 79% 248/312 [03:40<00:56,  1.13it/s]\u001b[A\n",
            " 80% 249/312 [03:41<00:53,  1.17it/s]\u001b[A\n",
            " 80% 250/312 [03:42<00:57,  1.08it/s]\u001b[A\n",
            " 80% 251/312 [03:42<00:53,  1.14it/s]\u001b[A\n",
            " 81% 252/312 [03:43<00:51,  1.16it/s]\u001b[A\n",
            " 81% 253/312 [03:44<00:51,  1.15it/s]\u001b[A\n",
            " 81% 254/312 [03:45<00:50,  1.15it/s]\u001b[A\n",
            " 82% 255/312 [03:46<00:49,  1.15it/s]\u001b[A\n",
            " 82% 256/312 [03:47<00:46,  1.21it/s]\u001b[A\n",
            " 82% 257/312 [03:48<00:50,  1.09it/s]\u001b[A\n",
            " 83% 258/312 [03:48<00:46,  1.16it/s]\u001b[A\n",
            " 83% 259/312 [03:49<00:48,  1.10it/s]\u001b[A\n",
            " 83% 260/312 [03:50<00:43,  1.20it/s]\u001b[A\n",
            " 84% 261/312 [03:51<00:41,  1.23it/s]\u001b[A\n",
            " 84% 262/312 [03:52<00:41,  1.21it/s]\u001b[A\n",
            " 84% 263/312 [03:53<00:40,  1.22it/s]\u001b[A\n",
            " 85% 264/312 [03:55<00:59,  1.24s/it]\u001b[A\n",
            " 85% 265/312 [03:57<01:06,  1.41s/it]\u001b[A\n",
            " 85% 266/312 [03:57<00:56,  1.24s/it]\u001b[A\n",
            " 86% 267/312 [03:58<00:52,  1.17s/it]\u001b[A\n",
            " 86% 268/312 [03:59<00:46,  1.06s/it]\u001b[A\n",
            " 86% 269/312 [04:00<00:43,  1.01s/it]\u001b[A\n",
            " 87% 270/312 [04:01<00:39,  1.07it/s]\u001b[A\n",
            " 87% 271/312 [04:02<00:35,  1.15it/s]\u001b[A\n",
            " 87% 272/312 [04:02<00:33,  1.19it/s]\u001b[A\n",
            " 88% 273/312 [04:04<00:36,  1.08it/s]\u001b[A\n",
            " 88% 274/312 [04:04<00:34,  1.11it/s]\u001b[A\n",
            " 88% 275/312 [04:05<00:31,  1.19it/s]\u001b[A\n",
            " 88% 276/312 [04:06<00:28,  1.26it/s]\u001b[A\n",
            " 89% 277/312 [04:07<00:31,  1.12it/s]\u001b[A\n",
            " 89% 278/312 [04:08<00:30,  1.11it/s]\u001b[A\n",
            " 89% 279/312 [04:09<00:29,  1.11it/s]\u001b[A\n",
            " 90% 280/312 [04:10<00:28,  1.12it/s]\u001b[A\n",
            " 90% 281/312 [04:10<00:25,  1.21it/s]\u001b[A\n",
            " 90% 282/312 [04:11<00:23,  1.25it/s]\u001b[A\n",
            " 91% 283/312 [04:12<00:25,  1.13it/s]\u001b[A\n",
            " 91% 284/312 [04:13<00:24,  1.17it/s]\u001b[A\n",
            " 91% 285/312 [04:14<00:22,  1.19it/s]\u001b[A\n",
            " 92% 286/312 [04:14<00:22,  1.18it/s]\u001b[A\n",
            " 92% 287/312 [04:15<00:21,  1.16it/s]\u001b[A\n",
            " 92% 288/312 [04:16<00:19,  1.22it/s]\u001b[A\n",
            " 93% 289/312 [04:17<00:18,  1.26it/s]\u001b[A\n",
            " 93% 290/312 [04:18<00:17,  1.25it/s]\u001b[A\n",
            " 93% 291/312 [04:18<00:16,  1.25it/s]\u001b[A\n",
            " 94% 292/312 [04:20<00:18,  1.11it/s]\u001b[A\n",
            " 94% 293/312 [04:21<00:17,  1.06it/s]\u001b[A\n",
            " 94% 294/312 [04:21<00:15,  1.16it/s]\u001b[A\n",
            " 95% 295/312 [04:22<00:13,  1.22it/s]\u001b[A\n",
            " 95% 296/312 [04:23<00:13,  1.22it/s]\u001b[A\n",
            " 95% 297/312 [04:24<00:12,  1.17it/s]\u001b[A\n",
            " 96% 298/312 [04:25<00:11,  1.18it/s]\u001b[A\n",
            " 96% 299/312 [04:26<00:13,  1.02s/it]\u001b[A\n",
            " 96% 300/312 [04:27<00:12,  1.07s/it]\u001b[A\n",
            " 96% 301/312 [04:28<00:11,  1.04s/it]\u001b[A\n",
            " 97% 302/312 [04:29<00:09,  1.05it/s]\u001b[A\n",
            " 97% 303/312 [04:30<00:08,  1.05it/s]\u001b[A\n",
            " 97% 304/312 [04:31<00:07,  1.11it/s]\u001b[A\n",
            " 98% 305/312 [04:31<00:06,  1.16it/s]\u001b[A\n",
            " 98% 306/312 [04:32<00:05,  1.12it/s]\u001b[A\n",
            " 98% 307/312 [04:33<00:04,  1.20it/s]\u001b[A\n",
            " 99% 308/312 [04:34<00:03,  1.15it/s]\u001b[A\n",
            " 99% 309/312 [04:35<00:02,  1.23it/s]\u001b[A\n",
            " 99% 310/312 [04:35<00:01,  1.28it/s]\u001b[A\n",
            "100% 311/312 [04:36<00:00,  1.32it/s]\u001b[A\n",
            "100% 312/312 [04:37<00:00,  1.36it/s]\u001b[A\n",
            "{'eval_loss': 1.1817880868911743, 'eval_runtime': 278.6118, 'eval_samples_per_second': 17.914, 'eval_steps_per_second': 1.12, 'epoch': 0.2}\n",
            "\n",
            " 20% 102/509 [34:48<2:12:17, 19.50s/it]\n",
            "{'loss': 1.273, 'learning_rate': 0.0001618661257606491, 'epoch': 0.22}\n",
            "{'loss': 1.1536, 'learning_rate': 0.00015780933062880325, 'epoch': 0.24}\n",
            "{'loss': 1.1035, 'learning_rate': 0.0001537525354969574, 'epoch': 0.25}\n",
            "{'loss': 1.1085, 'learning_rate': 0.00014969574036511157, 'epoch': 0.27}\n",
            "{'loss': 1.1872, 'learning_rate': 0.00014563894523326573, 'epoch': 0.29}\n",
            " 30% 153/509 [47:42<1:56:46, 19.68s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:13,  2.33it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:09,  1.63it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:03,  1.27it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:23,  1.17it/s]\u001b[A\n",
            "  2% 6/312 [00:05<06:36,  1.29s/it]\u001b[A\n",
            "  2% 7/312 [00:06<06:06,  1.20s/it]\u001b[A\n",
            "  3% 8/312 [00:07<05:42,  1.13s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:03,  1.00s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:06,  1.02s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:55,  1.02it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:51,  1.03it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:36,  1.08it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:12,  1.18it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:29,  1.10it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:10,  1.18it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:56,  1.25it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:51,  1.27it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:56,  1.24it/s]\u001b[A\n",
            "  6% 20/312 [00:17<03:48,  1.28it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:40,  1.32it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:07,  1.17it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:54,  1.23it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:49,  1.25it/s]\u001b[A\n",
            "  8% 25/312 [00:21<03:55,  1.22it/s]\u001b[A\n",
            "  8% 26/312 [00:22<03:58,  1.20it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:52,  1.23it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:01,  1.06s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:48,  1.02s/it]\u001b[A\n",
            " 10% 30/312 [00:26<04:23,  1.07it/s]\u001b[A\n",
            " 10% 31/312 [00:27<04:25,  1.06it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:46,  1.02s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:03,  1.09s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:36,  1.01it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:34,  1.01it/s]\u001b[A\n",
            " 12% 36/312 [00:32<04:26,  1.03it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:56,  1.16it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:55,  1.16it/s]\u001b[A\n",
            " 12% 39/312 [00:35<03:58,  1.14it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:07,  1.10it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:51,  1.17it/s]\u001b[A\n",
            " 13% 42/312 [00:37<03:36,  1.25it/s]\u001b[A\n",
            " 14% 43/312 [00:38<03:50,  1.17it/s]\u001b[A\n",
            " 14% 44/312 [00:40<04:25,  1.01it/s]\u001b[A\n",
            " 14% 45/312 [00:40<04:12,  1.06it/s]\u001b[A\n",
            " 15% 46/312 [00:41<03:58,  1.11it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:48,  1.16it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:42,  1.19it/s]\u001b[A\n",
            " 16% 49/312 [00:44<03:39,  1.20it/s]\u001b[A\n",
            " 16% 50/312 [00:44<03:34,  1.22it/s]\u001b[A\n",
            " 16% 51/312 [00:45<03:57,  1.10it/s]\u001b[A\n",
            " 17% 52/312 [00:46<03:38,  1.19it/s]\u001b[A\n",
            " 17% 53/312 [00:47<03:59,  1.08it/s]\u001b[A\n",
            " 17% 54/312 [00:48<03:54,  1.10it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:35,  1.19it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:37,  1.18it/s]\u001b[A\n",
            " 18% 57/312 [00:50<03:27,  1.23it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:18,  1.28it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:20,  1.26it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:26,  1.22it/s]\u001b[A\n",
            " 20% 61/312 [00:54<03:21,  1.25it/s]\u001b[A\n",
            " 20% 62/312 [00:55<04:22,  1.05s/it]\u001b[A\n",
            " 20% 63/312 [00:56<04:15,  1.03s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:56,  1.05it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:54,  1.05it/s]\u001b[A\n",
            " 21% 66/312 [00:59<03:32,  1.16it/s]\u001b[A\n",
            " 21% 67/312 [00:59<03:19,  1.23it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:10,  1.28it/s]\u001b[A\n",
            " 22% 69/312 [01:01<03:04,  1.32it/s]\u001b[A\n",
            " 22% 70/312 [01:01<02:57,  1.36it/s]\u001b[A\n",
            " 23% 71/312 [01:02<03:02,  1.32it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:12,  1.25it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:09,  1.26it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:24,  1.16it/s]\u001b[A\n",
            " 24% 75/312 [01:06<03:13,  1.23it/s]\u001b[A\n",
            " 24% 76/312 [01:06<03:04,  1.28it/s]\u001b[A\n",
            " 25% 77/312 [01:07<03:04,  1.28it/s]\u001b[A\n",
            " 25% 78/312 [01:08<03:20,  1.17it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:17,  1.18it/s]\u001b[A\n",
            " 26% 80/312 [01:10<03:08,  1.23it/s]\u001b[A\n",
            " 26% 81/312 [01:10<03:03,  1.26it/s]\u001b[A\n",
            " 26% 82/312 [01:11<03:09,  1.21it/s]\u001b[A\n",
            " 27% 83/312 [01:12<03:08,  1.22it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:12,  1.18it/s]\u001b[A\n",
            " 27% 85/312 [01:14<03:04,  1.23it/s]\u001b[A\n",
            " 28% 86/312 [01:15<03:41,  1.02it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:33,  1.05it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:30,  1.06it/s]\u001b[A\n",
            " 29% 89/312 [01:18<03:21,  1.11it/s]\u001b[A\n",
            " 29% 90/312 [01:19<03:13,  1.15it/s]\u001b[A\n",
            " 29% 91/312 [01:19<02:59,  1.23it/s]\u001b[A\n",
            " 29% 92/312 [01:20<03:01,  1.21it/s]\u001b[A\n",
            " 30% 93/312 [01:21<03:00,  1.21it/s]\u001b[A\n",
            " 30% 94/312 [01:22<03:05,  1.17it/s]\u001b[A\n",
            " 30% 95/312 [01:22<02:49,  1.28it/s]\u001b[A\n",
            " 31% 96/312 [01:23<02:53,  1.24it/s]\u001b[A\n",
            " 31% 97/312 [01:24<02:40,  1.34it/s]\u001b[A\n",
            " 31% 98/312 [01:25<02:52,  1.24it/s]\u001b[A\n",
            " 32% 99/312 [01:26<03:23,  1.05it/s]\u001b[A\n",
            " 32% 100/312 [01:27<03:04,  1.15it/s]\u001b[A\n",
            " 32% 101/312 [01:27<02:47,  1.26it/s]\u001b[A\n",
            " 33% 102/312 [01:28<02:37,  1.33it/s]\u001b[A\n",
            " 33% 103/312 [01:29<02:34,  1.35it/s]\u001b[A\n",
            " 33% 104/312 [01:30<03:20,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:02,  1.13it/s]\u001b[A\n",
            " 34% 106/312 [01:32<03:05,  1.11it/s]\u001b[A\n",
            " 34% 107/312 [01:33<02:55,  1.16it/s]\u001b[A\n",
            " 35% 108/312 [01:34<03:52,  1.14s/it]\u001b[A\n",
            " 35% 109/312 [01:35<03:32,  1.04s/it]\u001b[A\n",
            " 35% 110/312 [01:36<03:18,  1.02it/s]\u001b[A\n",
            " 36% 111/312 [01:37<03:00,  1.11it/s]\u001b[A\n",
            " 36% 112/312 [01:37<02:44,  1.22it/s]\u001b[A\n",
            " 36% 113/312 [01:38<02:45,  1.20it/s]\u001b[A\n",
            " 37% 114/312 [01:39<02:56,  1.12it/s]\u001b[A\n",
            " 37% 115/312 [01:40<02:50,  1.15it/s]\u001b[A\n",
            " 37% 116/312 [01:41<02:58,  1.10it/s]\u001b[A\n",
            " 38% 117/312 [01:42<02:47,  1.17it/s]\u001b[A\n",
            " 38% 118/312 [01:43<02:40,  1.21it/s]\u001b[A\n",
            " 38% 119/312 [01:43<02:35,  1.24it/s]\u001b[A\n",
            " 38% 120/312 [01:44<02:27,  1.30it/s]\u001b[A\n",
            " 39% 121/312 [01:45<02:50,  1.12it/s]\u001b[A\n",
            " 39% 122/312 [01:46<02:50,  1.11it/s]\u001b[A\n",
            " 39% 123/312 [01:47<02:35,  1.21it/s]\u001b[A\n",
            " 40% 124/312 [01:48<02:33,  1.23it/s]\u001b[A\n",
            " 40% 125/312 [01:50<03:59,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:51<03:34,  1.15s/it]\u001b[A\n",
            " 41% 127/312 [01:52<03:17,  1.07s/it]\u001b[A\n",
            " 41% 128/312 [01:52<02:56,  1.04it/s]\u001b[A\n",
            " 41% 129/312 [01:53<02:47,  1.10it/s]\u001b[A\n",
            " 42% 130/312 [01:54<02:41,  1.13it/s]\u001b[A\n",
            " 42% 131/312 [01:55<02:29,  1.21it/s]\u001b[A\n",
            " 42% 132/312 [01:56<02:26,  1.23it/s]\u001b[A\n",
            " 43% 133/312 [01:57<02:35,  1.15it/s]\u001b[A\n",
            " 43% 134/312 [01:57<02:32,  1.17it/s]\u001b[A\n",
            " 43% 135/312 [01:58<02:21,  1.25it/s]\u001b[A\n",
            " 44% 136/312 [01:59<02:13,  1.31it/s]\u001b[A\n",
            " 44% 137/312 [02:00<02:20,  1.24it/s]\u001b[A\n",
            " 44% 138/312 [02:00<02:25,  1.20it/s]\u001b[A\n",
            " 45% 139/312 [02:01<02:22,  1.22it/s]\u001b[A\n",
            " 45% 140/312 [02:02<02:20,  1.22it/s]\u001b[A\n",
            " 45% 141/312 [02:03<02:40,  1.07it/s]\u001b[A\n",
            " 46% 142/312 [02:05<03:13,  1.14s/it]\u001b[A\n",
            " 46% 143/312 [02:06<02:55,  1.04s/it]\u001b[A\n",
            " 46% 144/312 [02:06<02:39,  1.05it/s]\u001b[A\n",
            " 46% 145/312 [02:07<02:31,  1.11it/s]\u001b[A\n",
            " 47% 146/312 [02:08<02:25,  1.14it/s]\u001b[A\n",
            " 47% 147/312 [02:09<02:24,  1.14it/s]\u001b[A\n",
            " 47% 148/312 [02:10<02:25,  1.13it/s]\u001b[A\n",
            " 48% 149/312 [02:11<02:15,  1.20it/s]\u001b[A\n",
            " 48% 150/312 [02:11<02:08,  1.26it/s]\u001b[A\n",
            " 48% 151/312 [02:12<02:20,  1.15it/s]\u001b[A\n",
            " 49% 152/312 [02:14<03:07,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:15<02:43,  1.03s/it]\u001b[A\n",
            " 49% 154/312 [02:16<03:07,  1.18s/it]\u001b[A\n",
            " 50% 155/312 [02:18<03:13,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:19<03:16,  1.26s/it]\u001b[A\n",
            " 50% 157/312 [02:20<03:15,  1.26s/it]\u001b[A\n",
            " 51% 158/312 [02:21<02:57,  1.15s/it]\u001b[A\n",
            " 51% 159/312 [02:22<02:55,  1.15s/it]\u001b[A\n",
            " 51% 160/312 [02:23<02:38,  1.04s/it]\u001b[A\n",
            " 52% 161/312 [02:24<02:25,  1.04it/s]\u001b[A\n",
            " 52% 162/312 [02:25<02:15,  1.11it/s]\u001b[A\n",
            " 52% 163/312 [02:26<02:10,  1.14it/s]\u001b[A\n",
            " 53% 164/312 [02:27<02:13,  1.11it/s]\u001b[A\n",
            " 53% 165/312 [02:28<02:31,  1.03s/it]\u001b[A\n",
            " 53% 166/312 [02:30<03:00,  1.24s/it]\u001b[A\n",
            " 54% 167/312 [02:30<02:37,  1.09s/it]\u001b[A\n",
            " 54% 168/312 [02:31<02:26,  1.02s/it]\u001b[A\n",
            " 54% 169/312 [02:32<02:14,  1.06it/s]\u001b[A\n",
            " 54% 170/312 [02:33<02:04,  1.14it/s]\u001b[A\n",
            " 55% 171/312 [02:33<01:56,  1.21it/s]\u001b[A\n",
            " 55% 172/312 [02:34<01:55,  1.21it/s]\u001b[A\n",
            " 55% 173/312 [02:36<02:15,  1.02it/s]\u001b[A\n",
            " 56% 174/312 [02:36<02:11,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:37<02:00,  1.13it/s]\u001b[A\n",
            " 56% 176/312 [02:38<01:53,  1.20it/s]\u001b[A\n",
            " 57% 177/312 [02:39<01:50,  1.22it/s]\u001b[A\n",
            " 57% 178/312 [02:39<01:49,  1.22it/s]\u001b[A\n",
            " 57% 179/312 [02:40<01:46,  1.25it/s]\u001b[A\n",
            " 58% 180/312 [02:41<01:41,  1.30it/s]\u001b[A\n",
            " 58% 181/312 [02:42<01:37,  1.34it/s]\u001b[A\n",
            " 58% 182/312 [02:42<01:39,  1.30it/s]\u001b[A\n",
            " 59% 183/312 [02:44<01:53,  1.14it/s]\u001b[A\n",
            " 59% 184/312 [02:45<02:02,  1.05it/s]\u001b[A\n",
            " 59% 185/312 [02:46<01:56,  1.09it/s]\u001b[A\n",
            " 60% 186/312 [02:47<01:59,  1.06it/s]\u001b[A\n",
            " 60% 187/312 [02:47<01:51,  1.12it/s]\u001b[A\n",
            " 60% 188/312 [02:48<01:42,  1.21it/s]\u001b[A\n",
            " 61% 189/312 [02:49<01:36,  1.27it/s]\u001b[A\n",
            " 61% 190/312 [02:49<01:35,  1.28it/s]\u001b[A\n",
            " 61% 191/312 [02:50<01:36,  1.26it/s]\u001b[A\n",
            " 62% 192/312 [02:51<01:36,  1.24it/s]\u001b[A\n",
            " 62% 193/312 [02:52<01:32,  1.29it/s]\u001b[A\n",
            " 62% 194/312 [02:53<01:32,  1.27it/s]\u001b[A\n",
            " 62% 195/312 [02:53<01:30,  1.30it/s]\u001b[A\n",
            " 63% 196/312 [02:54<01:26,  1.35it/s]\u001b[A\n",
            " 63% 197/312 [02:55<01:24,  1.37it/s]\u001b[A\n",
            " 63% 198/312 [02:55<01:22,  1.39it/s]\u001b[A\n",
            " 64% 199/312 [02:56<01:20,  1.40it/s]\u001b[A\n",
            " 64% 200/312 [02:57<01:25,  1.31it/s]\u001b[A\n",
            " 64% 201/312 [02:58<01:28,  1.26it/s]\u001b[A\n",
            " 65% 202/312 [02:59<01:27,  1.25it/s]\u001b[A\n",
            " 65% 203/312 [03:00<01:30,  1.21it/s]\u001b[A\n",
            " 65% 204/312 [03:00<01:26,  1.26it/s]\u001b[A\n",
            " 66% 205/312 [03:01<01:20,  1.32it/s]\u001b[A\n",
            " 66% 206/312 [03:02<01:18,  1.36it/s]\u001b[A\n",
            " 66% 207/312 [03:03<01:21,  1.29it/s]\u001b[A\n",
            " 67% 208/312 [03:04<01:31,  1.14it/s]\u001b[A\n",
            " 67% 209/312 [03:04<01:23,  1.24it/s]\u001b[A\n",
            " 67% 210/312 [03:05<01:21,  1.25it/s]\u001b[A\n",
            " 68% 211/312 [03:06<01:36,  1.05it/s]\u001b[A\n",
            " 68% 212/312 [03:07<01:31,  1.10it/s]\u001b[A\n",
            " 68% 213/312 [03:08<01:31,  1.09it/s]\u001b[A\n",
            " 69% 214/312 [03:09<01:25,  1.14it/s]\u001b[A\n",
            " 69% 215/312 [03:10<01:25,  1.14it/s]\u001b[A\n",
            " 69% 216/312 [03:11<01:29,  1.07it/s]\u001b[A\n",
            " 70% 217/312 [03:12<01:24,  1.13it/s]\u001b[A\n",
            " 70% 218/312 [03:12<01:22,  1.13it/s]\u001b[A\n",
            " 70% 219/312 [03:14<01:37,  1.05s/it]\u001b[A\n",
            " 71% 220/312 [03:15<01:43,  1.13s/it]\u001b[A\n",
            " 71% 221/312 [03:16<01:37,  1.08s/it]\u001b[A\n",
            " 71% 222/312 [03:17<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:18<01:23,  1.06it/s]\u001b[A\n",
            " 72% 224/312 [03:18<01:14,  1.19it/s]\u001b[A\n",
            " 72% 225/312 [03:19<01:18,  1.11it/s]\u001b[A\n",
            " 72% 226/312 [03:20<01:14,  1.16it/s]\u001b[A\n",
            " 73% 227/312 [03:21<01:17,  1.10it/s]\u001b[A\n",
            " 73% 228/312 [03:22<01:13,  1.14it/s]\u001b[A\n",
            " 73% 229/312 [03:23<01:07,  1.22it/s]\u001b[A\n",
            " 74% 230/312 [03:23<01:04,  1.27it/s]\u001b[A\n",
            " 74% 231/312 [03:24<01:05,  1.23it/s]\u001b[A\n",
            " 74% 232/312 [03:25<01:00,  1.33it/s]\u001b[A\n",
            " 75% 233/312 [03:26<01:11,  1.11it/s]\u001b[A\n",
            " 75% 234/312 [03:27<01:04,  1.20it/s]\u001b[A\n",
            " 75% 235/312 [03:28<01:05,  1.18it/s]\u001b[A\n",
            " 76% 236/312 [03:28<01:01,  1.24it/s]\u001b[A\n",
            " 76% 237/312 [03:29<01:00,  1.24it/s]\u001b[A\n",
            " 76% 238/312 [03:30<00:56,  1.30it/s]\u001b[A\n",
            " 77% 239/312 [03:31<00:58,  1.25it/s]\u001b[A\n",
            " 77% 240/312 [03:32<00:57,  1.26it/s]\u001b[A\n",
            " 77% 241/312 [03:32<00:57,  1.23it/s]\u001b[A\n",
            " 78% 242/312 [03:34<01:04,  1.08it/s]\u001b[A\n",
            " 78% 243/312 [03:35<01:22,  1.20s/it]\u001b[A\n",
            " 78% 244/312 [03:36<01:12,  1.06s/it]\u001b[A\n",
            " 79% 245/312 [03:37<01:09,  1.03s/it]\u001b[A\n",
            " 79% 246/312 [03:38<01:08,  1.03s/it]\u001b[A\n",
            " 79% 247/312 [03:39<01:00,  1.07it/s]\u001b[A\n",
            " 79% 248/312 [03:40<00:56,  1.13it/s]\u001b[A\n",
            " 80% 249/312 [03:40<00:54,  1.17it/s]\u001b[A\n",
            " 80% 250/312 [03:42<00:57,  1.08it/s]\u001b[A\n",
            " 80% 251/312 [03:42<00:53,  1.13it/s]\u001b[A\n",
            " 81% 252/312 [03:43<00:51,  1.16it/s]\u001b[A\n",
            " 81% 253/312 [03:44<00:51,  1.14it/s]\u001b[A\n",
            " 81% 254/312 [03:45<00:50,  1.14it/s]\u001b[A\n",
            " 82% 255/312 [03:46<00:50,  1.14it/s]\u001b[A\n",
            " 82% 256/312 [03:47<00:46,  1.20it/s]\u001b[A\n",
            " 82% 257/312 [03:48<00:50,  1.09it/s]\u001b[A\n",
            " 83% 258/312 [03:48<00:46,  1.16it/s]\u001b[A\n",
            " 83% 259/312 [03:49<00:48,  1.10it/s]\u001b[A\n",
            " 83% 260/312 [03:50<00:43,  1.19it/s]\u001b[A\n",
            " 84% 261/312 [03:51<00:41,  1.22it/s]\u001b[A\n",
            " 84% 262/312 [03:52<00:41,  1.20it/s]\u001b[A\n",
            " 84% 263/312 [03:53<00:40,  1.21it/s]\u001b[A\n",
            " 85% 264/312 [03:55<00:59,  1.25s/it]\u001b[A\n",
            " 85% 265/312 [03:57<01:06,  1.41s/it]\u001b[A\n",
            " 85% 266/312 [03:57<00:57,  1.24s/it]\u001b[A\n",
            " 86% 267/312 [03:58<00:53,  1.18s/it]\u001b[A\n",
            " 86% 268/312 [03:59<00:46,  1.06s/it]\u001b[A\n",
            " 86% 269/312 [04:00<00:43,  1.02s/it]\u001b[A\n",
            " 87% 270/312 [04:01<00:39,  1.06it/s]\u001b[A\n",
            " 87% 271/312 [04:02<00:35,  1.15it/s]\u001b[A\n",
            " 87% 272/312 [04:02<00:33,  1.18it/s]\u001b[A\n",
            " 88% 273/312 [04:04<00:36,  1.07it/s]\u001b[A\n",
            " 88% 274/312 [04:04<00:34,  1.11it/s]\u001b[A\n",
            " 88% 275/312 [04:05<00:31,  1.18it/s]\u001b[A\n",
            " 88% 276/312 [04:06<00:28,  1.26it/s]\u001b[A\n",
            " 89% 277/312 [04:07<00:31,  1.12it/s]\u001b[A\n",
            " 89% 278/312 [04:08<00:30,  1.11it/s]\u001b[A\n",
            " 89% 279/312 [04:09<00:29,  1.11it/s]\u001b[A\n",
            " 90% 280/312 [04:10<00:28,  1.11it/s]\u001b[A\n",
            " 90% 281/312 [04:10<00:25,  1.20it/s]\u001b[A\n",
            " 90% 282/312 [04:11<00:24,  1.24it/s]\u001b[A\n",
            " 91% 283/312 [04:12<00:25,  1.12it/s]\u001b[A\n",
            " 91% 284/312 [04:13<00:24,  1.16it/s]\u001b[A\n",
            " 91% 285/312 [04:14<00:22,  1.18it/s]\u001b[A\n",
            " 92% 286/312 [04:15<00:22,  1.17it/s]\u001b[A\n",
            " 92% 287/312 [04:15<00:21,  1.16it/s]\u001b[A\n",
            " 92% 288/312 [04:16<00:19,  1.21it/s]\u001b[A\n",
            " 93% 289/312 [04:17<00:18,  1.25it/s]\u001b[A\n",
            " 93% 290/312 [04:18<00:17,  1.24it/s]\u001b[A\n",
            " 93% 291/312 [04:19<00:16,  1.24it/s]\u001b[A\n",
            " 94% 292/312 [04:20<00:18,  1.11it/s]\u001b[A\n",
            " 94% 293/312 [04:21<00:17,  1.06it/s]\u001b[A\n",
            " 94% 294/312 [04:21<00:15,  1.15it/s]\u001b[A\n",
            " 95% 295/312 [04:22<00:14,  1.21it/s]\u001b[A\n",
            " 95% 296/312 [04:23<00:13,  1.21it/s]\u001b[A\n",
            " 95% 297/312 [04:24<00:12,  1.16it/s]\u001b[A\n",
            " 96% 298/312 [04:25<00:11,  1.18it/s]\u001b[A\n",
            " 96% 299/312 [04:26<00:13,  1.03s/it]\u001b[A\n",
            " 96% 300/312 [04:27<00:12,  1.07s/it]\u001b[A\n",
            " 96% 301/312 [04:28<00:11,  1.05s/it]\u001b[A\n",
            " 97% 302/312 [04:29<00:09,  1.05it/s]\u001b[A\n",
            " 97% 303/312 [04:30<00:08,  1.04it/s]\u001b[A\n",
            " 97% 304/312 [04:31<00:07,  1.11it/s]\u001b[A\n",
            " 98% 305/312 [04:32<00:06,  1.15it/s]\u001b[A\n",
            " 98% 306/312 [04:33<00:05,  1.11it/s]\u001b[A\n",
            " 98% 307/312 [04:33<00:04,  1.19it/s]\u001b[A\n",
            " 99% 308/312 [04:34<00:03,  1.14it/s]\u001b[A\n",
            " 99% 309/312 [04:35<00:02,  1.22it/s]\u001b[A\n",
            " 99% 310/312 [04:36<00:01,  1.27it/s]\u001b[A\n",
            "100% 311/312 [04:36<00:00,  1.31it/s]\u001b[A\n",
            "100% 312/312 [04:37<00:00,  1.34it/s]\u001b[A\n",
            "{'eval_loss': 1.175948143005371, 'eval_runtime': 278.8117, 'eval_samples_per_second': 17.901, 'eval_steps_per_second': 1.119, 'epoch': 0.3}\n",
            "\n",
            " 30% 153/509 [52:21<1:56:46, 19.68s/it]\n",
            "{'loss': 1.2576, 'learning_rate': 0.0001415821501014199, 'epoch': 0.31}\n",
            "{'loss': 1.1616, 'learning_rate': 0.00013752535496957405, 'epoch': 0.33}\n",
            "{'loss': 1.0954, 'learning_rate': 0.00013346855983772818, 'epoch': 0.35}\n",
            "{'loss': 1.0969, 'learning_rate': 0.00012941176470588237, 'epoch': 0.37}\n",
            "{'loss': 1.2033, 'learning_rate': 0.00012535496957403653, 'epoch': 0.39}\n",
            " 40% 204/509 [1:05:13<1:40:05, 19.69s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:15,  2.29it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:11,  1.61it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:06,  1.25it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:27,  1.15it/s]\u001b[A\n",
            "  2% 6/312 [00:06<06:39,  1.30s/it]\u001b[A\n",
            "  2% 7/312 [00:07<06:10,  1.21s/it]\u001b[A\n",
            "  3% 8/312 [00:08<05:45,  1.14s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:07,  1.02s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:10,  1.03s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:59,  1.01it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:54,  1.02it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:39,  1.07it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:15,  1.17it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:31,  1.09it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:13,  1.17it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:59,  1.23it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:53,  1.26it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:58,  1.23it/s]\u001b[A\n",
            "  6% 20/312 [00:18<03:50,  1.27it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:42,  1.31it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:10,  1.16it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:56,  1.22it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:53,  1.24it/s]\u001b[A\n",
            "  8% 25/312 [00:22<03:58,  1.20it/s]\u001b[A\n",
            "  8% 26/312 [00:23<04:01,  1.18it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:55,  1.21it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:03,  1.07s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:51,  1.03s/it]\u001b[A\n",
            " 10% 30/312 [00:27<04:25,  1.06it/s]\u001b[A\n",
            " 10% 31/312 [00:28<04:27,  1.05it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:47,  1.03s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:05,  1.10s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:37,  1.00it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:35,  1.00it/s]\u001b[A\n",
            " 12% 36/312 [00:33<04:27,  1.03it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:57,  1.16it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:56,  1.16it/s]\u001b[A\n",
            " 12% 39/312 [00:35<04:00,  1.14it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:08,  1.09it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:53,  1.16it/s]\u001b[A\n",
            " 13% 42/312 [00:38<03:37,  1.24it/s]\u001b[A\n",
            " 14% 43/312 [00:39<03:51,  1.16it/s]\u001b[A\n",
            " 14% 44/312 [00:40<04:26,  1.01it/s]\u001b[A\n",
            " 14% 45/312 [00:41<04:13,  1.05it/s]\u001b[A\n",
            " 15% 46/312 [00:42<03:59,  1.11it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:49,  1.15it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:42,  1.18it/s]\u001b[A\n",
            " 16% 49/312 [00:44<03:39,  1.20it/s]\u001b[A\n",
            " 16% 50/312 [00:45<03:34,  1.22it/s]\u001b[A\n",
            " 16% 51/312 [00:46<03:58,  1.09it/s]\u001b[A\n",
            " 17% 52/312 [00:47<03:39,  1.19it/s]\u001b[A\n",
            " 17% 53/312 [00:48<04:00,  1.08it/s]\u001b[A\n",
            " 17% 54/312 [00:49<03:56,  1.09it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:36,  1.19it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:38,  1.17it/s]\u001b[A\n",
            " 18% 57/312 [00:51<03:27,  1.23it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:18,  1.28it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:20,  1.26it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:27,  1.22it/s]\u001b[A\n",
            " 20% 61/312 [00:54<03:22,  1.24it/s]\u001b[A\n",
            " 20% 62/312 [00:56<04:23,  1.05s/it]\u001b[A\n",
            " 20% 63/312 [00:57<04:15,  1.03s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:56,  1.05it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:54,  1.05it/s]\u001b[A\n",
            " 21% 66/312 [00:59<03:33,  1.15it/s]\u001b[A\n",
            " 21% 67/312 [01:00<03:20,  1.22it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:11,  1.28it/s]\u001b[A\n",
            " 22% 69/312 [01:01<03:05,  1.31it/s]\u001b[A\n",
            " 22% 70/312 [01:02<02:58,  1.36it/s]\u001b[A\n",
            " 23% 71/312 [01:03<03:03,  1.31it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:13,  1.24it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:10,  1.26it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:25,  1.16it/s]\u001b[A\n",
            " 24% 75/312 [01:06<03:14,  1.22it/s]\u001b[A\n",
            " 24% 76/312 [01:07<03:05,  1.27it/s]\u001b[A\n",
            " 25% 77/312 [01:08<03:05,  1.27it/s]\u001b[A\n",
            " 25% 78/312 [01:09<03:21,  1.16it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:17,  1.18it/s]\u001b[A\n",
            " 26% 80/312 [01:10<03:08,  1.23it/s]\u001b[A\n",
            " 26% 81/312 [01:11<03:04,  1.26it/s]\u001b[A\n",
            " 26% 82/312 [01:12<03:09,  1.21it/s]\u001b[A\n",
            " 27% 83/312 [01:13<03:08,  1.21it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:13,  1.18it/s]\u001b[A\n",
            " 27% 85/312 [01:14<03:04,  1.23it/s]\u001b[A\n",
            " 28% 86/312 [01:16<03:41,  1.02it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:34,  1.05it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:30,  1.06it/s]\u001b[A\n",
            " 29% 89/312 [01:18<03:22,  1.10it/s]\u001b[A\n",
            " 29% 90/312 [01:19<03:14,  1.14it/s]\u001b[A\n",
            " 29% 91/312 [01:20<02:59,  1.23it/s]\u001b[A\n",
            " 29% 92/312 [01:21<03:02,  1.21it/s]\u001b[A\n",
            " 30% 93/312 [01:21<03:01,  1.21it/s]\u001b[A\n",
            " 30% 94/312 [01:22<03:06,  1.17it/s]\u001b[A\n",
            " 30% 95/312 [01:23<02:49,  1.28it/s]\u001b[A\n",
            " 31% 96/312 [01:24<02:54,  1.24it/s]\u001b[A\n",
            " 31% 97/312 [01:24<02:41,  1.33it/s]\u001b[A\n",
            " 31% 98/312 [01:25<02:52,  1.24it/s]\u001b[A\n",
            " 32% 99/312 [01:27<03:23,  1.05it/s]\u001b[A\n",
            " 32% 100/312 [01:27<03:05,  1.14it/s]\u001b[A\n",
            " 32% 101/312 [01:28<02:48,  1.26it/s]\u001b[A\n",
            " 33% 102/312 [01:29<02:37,  1.33it/s]\u001b[A\n",
            " 33% 103/312 [01:29<02:35,  1.35it/s]\u001b[A\n",
            " 33% 104/312 [01:31<03:20,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:03,  1.13it/s]\u001b[A\n",
            " 34% 106/312 [01:32<03:05,  1.11it/s]\u001b[A\n",
            " 34% 107/312 [01:33<02:56,  1.16it/s]\u001b[A\n",
            " 35% 108/312 [01:35<03:52,  1.14s/it]\u001b[A\n",
            " 35% 109/312 [01:36<03:32,  1.05s/it]\u001b[A\n",
            " 35% 110/312 [01:37<03:17,  1.02it/s]\u001b[A\n",
            " 36% 111/312 [01:37<03:00,  1.11it/s]\u001b[A\n",
            " 36% 112/312 [01:38<02:44,  1.21it/s]\u001b[A\n",
            " 36% 113/312 [01:39<02:46,  1.20it/s]\u001b[A\n",
            " 37% 114/312 [01:40<02:56,  1.12it/s]\u001b[A\n",
            " 37% 115/312 [01:41<02:51,  1.15it/s]\u001b[A\n",
            " 37% 116/312 [01:42<02:59,  1.09it/s]\u001b[A\n",
            " 38% 117/312 [01:42<02:48,  1.16it/s]\u001b[A\n",
            " 38% 118/312 [01:43<02:41,  1.20it/s]\u001b[A\n",
            " 38% 119/312 [01:44<02:36,  1.23it/s]\u001b[A\n",
            " 38% 120/312 [01:45<02:27,  1.30it/s]\u001b[A\n",
            " 39% 121/312 [01:46<02:51,  1.12it/s]\u001b[A\n",
            " 39% 122/312 [01:47<02:50,  1.11it/s]\u001b[A\n",
            " 39% 123/312 [01:47<02:36,  1.21it/s]\u001b[A\n",
            " 40% 124/312 [01:48<02:33,  1.22it/s]\u001b[A\n",
            " 40% 125/312 [01:51<04:00,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:51<03:34,  1.15s/it]\u001b[A\n",
            " 41% 127/312 [01:52<03:17,  1.07s/it]\u001b[A\n",
            " 41% 128/312 [01:53<02:56,  1.04it/s]\u001b[A\n",
            " 41% 129/312 [01:54<02:47,  1.09it/s]\u001b[A\n",
            " 42% 130/312 [01:55<02:41,  1.13it/s]\u001b[A\n",
            " 42% 131/312 [01:55<02:29,  1.21it/s]\u001b[A\n",
            " 42% 132/312 [01:56<02:27,  1.22it/s]\u001b[A\n",
            " 43% 133/312 [01:57<02:35,  1.15it/s]\u001b[A\n",
            " 43% 134/312 [01:58<02:32,  1.17it/s]\u001b[A\n",
            " 43% 135/312 [01:59<02:21,  1.25it/s]\u001b[A\n",
            " 44% 136/312 [01:59<02:13,  1.32it/s]\u001b[A\n",
            " 44% 137/312 [02:00<02:20,  1.24it/s]\u001b[A\n",
            " 44% 138/312 [02:01<02:24,  1.20it/s]\u001b[A\n",
            " 45% 139/312 [02:02<02:22,  1.21it/s]\u001b[A\n",
            " 45% 140/312 [02:03<02:21,  1.21it/s]\u001b[A\n",
            " 45% 141/312 [02:04<02:40,  1.06it/s]\u001b[A\n",
            " 46% 142/312 [02:05<03:14,  1.14s/it]\u001b[A\n",
            " 46% 143/312 [02:06<02:55,  1.04s/it]\u001b[A\n",
            " 46% 144/312 [02:07<02:40,  1.05it/s]\u001b[A\n",
            " 46% 145/312 [02:08<02:31,  1.10it/s]\u001b[A\n",
            " 47% 146/312 [02:09<02:26,  1.13it/s]\u001b[A\n",
            " 47% 147/312 [02:10<02:24,  1.14it/s]\u001b[A\n",
            " 47% 148/312 [02:10<02:25,  1.13it/s]\u001b[A\n",
            " 48% 149/312 [02:11<02:15,  1.20it/s]\u001b[A\n",
            " 48% 150/312 [02:12<02:08,  1.26it/s]\u001b[A\n",
            " 48% 151/312 [02:13<02:20,  1.14it/s]\u001b[A\n",
            " 49% 152/312 [02:15<03:07,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:15<02:43,  1.03s/it]\u001b[A\n",
            " 49% 154/312 [02:17<03:07,  1.19s/it]\u001b[A\n",
            " 50% 155/312 [02:18<03:13,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:20<03:16,  1.26s/it]\u001b[A\n",
            " 50% 157/312 [02:21<03:15,  1.26s/it]\u001b[A\n",
            " 51% 158/312 [02:22<02:57,  1.15s/it]\u001b[A\n",
            " 51% 159/312 [02:23<02:55,  1.15s/it]\u001b[A\n",
            " 51% 160/312 [02:24<02:38,  1.04s/it]\u001b[A\n",
            " 52% 161/312 [02:25<02:24,  1.04it/s]\u001b[A\n",
            " 52% 162/312 [02:25<02:15,  1.11it/s]\u001b[A\n",
            " 52% 163/312 [02:26<02:10,  1.14it/s]\u001b[A\n",
            " 53% 164/312 [02:27<02:13,  1.11it/s]\u001b[A\n",
            " 53% 165/312 [02:28<02:31,  1.03s/it]\u001b[A\n",
            " 53% 166/312 [02:30<03:00,  1.24s/it]\u001b[A\n",
            " 54% 167/312 [02:31<02:37,  1.09s/it]\u001b[A\n",
            " 54% 168/312 [02:32<02:27,  1.02s/it]\u001b[A\n",
            " 54% 169/312 [02:33<02:15,  1.06it/s]\u001b[A\n",
            " 54% 170/312 [02:33<02:04,  1.14it/s]\u001b[A\n",
            " 55% 171/312 [02:34<01:56,  1.21it/s]\u001b[A\n",
            " 55% 172/312 [02:35<01:55,  1.21it/s]\u001b[A\n",
            " 55% 173/312 [02:36<02:15,  1.02it/s]\u001b[A\n",
            " 56% 174/312 [02:37<02:11,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:38<02:00,  1.14it/s]\u001b[A\n",
            " 56% 176/312 [02:38<01:53,  1.20it/s]\u001b[A\n",
            " 57% 177/312 [02:39<01:50,  1.22it/s]\u001b[A\n",
            " 57% 178/312 [02:40<01:49,  1.22it/s]\u001b[A\n",
            " 57% 179/312 [02:41<01:46,  1.25it/s]\u001b[A\n",
            " 58% 180/312 [02:42<01:41,  1.30it/s]\u001b[A\n",
            " 58% 181/312 [02:42<01:37,  1.34it/s]\u001b[A\n",
            " 58% 182/312 [02:43<01:39,  1.30it/s]\u001b[A\n",
            " 59% 183/312 [02:44<01:53,  1.14it/s]\u001b[A\n",
            " 59% 184/312 [02:45<02:02,  1.05it/s]\u001b[A\n",
            " 59% 185/312 [02:46<01:56,  1.09it/s]\u001b[A\n",
            " 60% 186/312 [02:47<01:58,  1.06it/s]\u001b[A\n",
            " 60% 187/312 [02:48<01:51,  1.12it/s]\u001b[A\n",
            " 60% 188/312 [02:49<01:42,  1.21it/s]\u001b[A\n",
            " 61% 189/312 [02:49<01:36,  1.28it/s]\u001b[A\n",
            " 61% 190/312 [02:50<01:35,  1.28it/s]\u001b[A\n",
            " 61% 191/312 [02:51<01:36,  1.26it/s]\u001b[A\n",
            " 62% 192/312 [02:52<01:35,  1.25it/s]\u001b[A\n",
            " 62% 193/312 [02:52<01:31,  1.31it/s]\u001b[A\n",
            " 62% 194/312 [02:53<01:32,  1.28it/s]\u001b[A\n",
            " 62% 195/312 [02:54<01:29,  1.31it/s]\u001b[A\n",
            " 63% 196/312 [02:55<01:25,  1.36it/s]\u001b[A\n",
            " 63% 197/312 [02:55<01:23,  1.38it/s]\u001b[A\n",
            " 63% 198/312 [02:56<01:21,  1.40it/s]\u001b[A\n",
            " 64% 199/312 [02:57<01:20,  1.41it/s]\u001b[A\n",
            " 64% 200/312 [02:58<01:24,  1.32it/s]\u001b[A\n",
            " 64% 201/312 [02:58<01:27,  1.27it/s]\u001b[A\n",
            " 65% 202/312 [02:59<01:27,  1.26it/s]\u001b[A\n",
            " 65% 203/312 [03:00<01:29,  1.22it/s]\u001b[A\n",
            " 65% 204/312 [03:01<01:25,  1.26it/s]\u001b[A\n",
            " 66% 205/312 [03:01<01:20,  1.33it/s]\u001b[A\n",
            " 66% 206/312 [03:02<01:17,  1.37it/s]\u001b[A\n",
            " 66% 207/312 [03:03<01:20,  1.30it/s]\u001b[A\n",
            " 67% 208/312 [03:04<01:30,  1.15it/s]\u001b[A\n",
            " 67% 209/312 [03:05<01:22,  1.24it/s]\u001b[A\n",
            " 67% 210/312 [03:06<01:20,  1.26it/s]\u001b[A\n",
            " 68% 211/312 [03:07<01:35,  1.06it/s]\u001b[A\n",
            " 68% 212/312 [03:08<01:30,  1.11it/s]\u001b[A\n",
            " 68% 213/312 [03:09<01:30,  1.09it/s]\u001b[A\n",
            " 69% 214/312 [03:09<01:25,  1.15it/s]\u001b[A\n",
            " 69% 215/312 [03:10<01:24,  1.15it/s]\u001b[A\n",
            " 69% 216/312 [03:11<01:29,  1.08it/s]\u001b[A\n",
            " 70% 217/312 [03:12<01:23,  1.14it/s]\u001b[A\n",
            " 70% 218/312 [03:13<01:22,  1.14it/s]\u001b[A\n",
            " 70% 219/312 [03:14<01:36,  1.04s/it]\u001b[A\n",
            " 71% 220/312 [03:16<01:43,  1.12s/it]\u001b[A\n",
            " 71% 221/312 [03:17<01:37,  1.07s/it]\u001b[A\n",
            " 71% 222/312 [03:17<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:18<01:23,  1.07it/s]\u001b[A\n",
            " 72% 224/312 [03:19<01:13,  1.20it/s]\u001b[A\n",
            " 72% 225/312 [03:20<01:18,  1.11it/s]\u001b[A\n",
            " 72% 226/312 [03:21<01:14,  1.16it/s]\u001b[A\n",
            " 73% 227/312 [03:22<01:17,  1.10it/s]\u001b[A\n",
            " 73% 228/312 [03:22<01:13,  1.15it/s]\u001b[A\n",
            " 73% 229/312 [03:23<01:07,  1.23it/s]\u001b[A\n",
            " 74% 230/312 [03:24<01:04,  1.27it/s]\u001b[A\n",
            " 74% 231/312 [03:25<01:05,  1.24it/s]\u001b[A\n",
            " 74% 232/312 [03:25<00:59,  1.34it/s]\u001b[A\n",
            " 75% 233/312 [03:26<01:10,  1.12it/s]\u001b[A\n",
            " 75% 234/312 [03:27<01:04,  1.21it/s]\u001b[A\n",
            " 75% 235/312 [03:28<01:04,  1.19it/s]\u001b[A\n",
            " 76% 236/312 [03:29<01:00,  1.25it/s]\u001b[A\n",
            " 76% 237/312 [03:30<00:59,  1.25it/s]\u001b[A\n",
            " 76% 238/312 [03:30<00:56,  1.31it/s]\u001b[A\n",
            " 77% 239/312 [03:31<00:57,  1.27it/s]\u001b[A\n",
            " 77% 240/312 [03:32<00:56,  1.27it/s]\u001b[A\n",
            " 77% 241/312 [03:33<00:57,  1.24it/s]\u001b[A\n",
            " 78% 242/312 [03:34<01:04,  1.08it/s]\u001b[A\n",
            " 78% 243/312 [03:36<01:22,  1.20s/it]\u001b[A\n",
            " 78% 244/312 [03:36<01:11,  1.05s/it]\u001b[A\n",
            " 79% 245/312 [03:37<01:08,  1.02s/it]\u001b[A\n",
            " 79% 246/312 [03:38<01:07,  1.02s/it]\u001b[A\n",
            " 79% 247/312 [03:39<01:00,  1.08it/s]\u001b[A\n",
            " 79% 248/312 [03:40<00:55,  1.14it/s]\u001b[A\n",
            " 80% 249/312 [03:41<00:53,  1.18it/s]\u001b[A\n",
            " 80% 250/312 [03:42<00:56,  1.09it/s]\u001b[A\n",
            " 80% 251/312 [03:43<00:53,  1.15it/s]\u001b[A\n",
            " 81% 252/312 [03:43<00:51,  1.17it/s]\u001b[A\n",
            " 81% 253/312 [03:44<00:50,  1.16it/s]\u001b[A\n",
            " 81% 254/312 [03:45<00:49,  1.16it/s]\u001b[A\n",
            " 82% 255/312 [03:46<00:49,  1.16it/s]\u001b[A\n",
            " 82% 256/312 [03:47<00:45,  1.22it/s]\u001b[A\n",
            " 82% 257/312 [03:48<00:49,  1.10it/s]\u001b[A\n",
            " 83% 258/312 [03:48<00:46,  1.17it/s]\u001b[A\n",
            " 83% 259/312 [03:50<00:47,  1.11it/s]\u001b[A\n",
            " 83% 260/312 [03:50<00:43,  1.21it/s]\u001b[A\n",
            " 84% 261/312 [03:51<00:41,  1.24it/s]\u001b[A\n",
            " 84% 262/312 [03:52<00:41,  1.21it/s]\u001b[A\n",
            " 84% 263/312 [03:53<00:39,  1.23it/s]\u001b[A\n",
            " 85% 264/312 [03:55<00:59,  1.24s/it]\u001b[A\n",
            " 85% 265/312 [03:57<01:05,  1.40s/it]\u001b[A\n",
            " 85% 266/312 [03:57<00:56,  1.23s/it]\u001b[A\n",
            " 86% 267/312 [03:58<00:52,  1.17s/it]\u001b[A\n",
            " 86% 268/312 [03:59<00:46,  1.05s/it]\u001b[A\n",
            " 86% 269/312 [04:00<00:43,  1.01s/it]\u001b[A\n",
            " 87% 270/312 [04:01<00:39,  1.07it/s]\u001b[A\n",
            " 87% 271/312 [04:02<00:35,  1.16it/s]\u001b[A\n",
            " 87% 272/312 [04:02<00:33,  1.20it/s]\u001b[A\n",
            " 88% 273/312 [04:03<00:35,  1.08it/s]\u001b[A\n",
            " 88% 274/312 [04:04<00:33,  1.12it/s]\u001b[A\n",
            " 88% 275/312 [04:05<00:30,  1.20it/s]\u001b[A\n",
            " 88% 276/312 [04:06<00:28,  1.27it/s]\u001b[A\n",
            " 89% 277/312 [04:07<00:31,  1.13it/s]\u001b[A\n",
            " 89% 278/312 [04:08<00:30,  1.12it/s]\u001b[A\n",
            " 89% 279/312 [04:09<00:29,  1.12it/s]\u001b[A\n",
            " 90% 280/312 [04:09<00:28,  1.13it/s]\u001b[A\n",
            " 90% 281/312 [04:10<00:25,  1.22it/s]\u001b[A\n",
            " 90% 282/312 [04:11<00:23,  1.26it/s]\u001b[A\n",
            " 91% 283/312 [04:12<00:25,  1.14it/s]\u001b[A\n",
            " 91% 284/312 [04:13<00:23,  1.18it/s]\u001b[A\n",
            " 91% 285/312 [04:14<00:22,  1.20it/s]\u001b[A\n",
            " 92% 286/312 [04:14<00:21,  1.19it/s]\u001b[A\n",
            " 92% 287/312 [04:15<00:21,  1.17it/s]\u001b[A\n",
            " 92% 288/312 [04:16<00:19,  1.23it/s]\u001b[A\n",
            " 93% 289/312 [04:17<00:18,  1.27it/s]\u001b[A\n",
            " 93% 290/312 [04:17<00:17,  1.26it/s]\u001b[A\n",
            " 93% 291/312 [04:18<00:16,  1.26it/s]\u001b[A\n",
            " 94% 292/312 [04:19<00:17,  1.12it/s]\u001b[A\n",
            " 94% 293/312 [04:20<00:17,  1.07it/s]\u001b[A\n",
            " 94% 294/312 [04:21<00:15,  1.17it/s]\u001b[A\n",
            " 95% 295/312 [04:22<00:13,  1.22it/s]\u001b[A\n",
            " 95% 296/312 [04:23<00:12,  1.23it/s]\u001b[A\n",
            " 95% 297/312 [04:24<00:12,  1.17it/s]\u001b[A\n",
            " 96% 298/312 [04:24<00:11,  1.19it/s]\u001b[A\n",
            " 96% 299/312 [04:26<00:13,  1.02s/it]\u001b[A\n",
            " 96% 300/312 [04:27<00:12,  1.06s/it]\u001b[A\n",
            " 96% 301/312 [04:28<00:11,  1.04s/it]\u001b[A\n",
            " 97% 302/312 [04:29<00:09,  1.06it/s]\u001b[A\n",
            " 97% 303/312 [04:30<00:08,  1.06it/s]\u001b[A\n",
            " 97% 304/312 [04:30<00:07,  1.12it/s]\u001b[A\n",
            " 98% 305/312 [04:31<00:05,  1.17it/s]\u001b[A\n",
            " 98% 306/312 [04:32<00:05,  1.13it/s]\u001b[A\n",
            " 98% 307/312 [04:33<00:04,  1.20it/s]\u001b[A\n",
            " 99% 308/312 [04:34<00:03,  1.15it/s]\u001b[A\n",
            " 99% 309/312 [04:34<00:02,  1.23it/s]\u001b[A\n",
            " 99% 310/312 [04:35<00:01,  1.29it/s]\u001b[A\n",
            "100% 311/312 [04:36<00:00,  1.33it/s]\u001b[A\n",
            "100% 312/312 [04:37<00:00,  1.36it/s]\u001b[A\n",
            "{'eval_loss': 1.1717621088027954, 'eval_runtime': 278.3603, 'eval_samples_per_second': 17.93, 'eval_steps_per_second': 1.121, 'epoch': 0.4}\n",
            "\n",
            " 40% 204/509 [1:09:52<1:40:05, 19.69s/it]\n",
            "{'loss': 1.2574, 'learning_rate': 0.00012129817444219068, 'epoch': 0.41}\n",
            "{'loss': 1.1617, 'learning_rate': 0.00011724137931034482, 'epoch': 0.43}\n",
            "{'loss': 1.1101, 'learning_rate': 0.000113184584178499, 'epoch': 0.45}\n",
            "{'loss': 1.0918, 'learning_rate': 0.00010912778904665314, 'epoch': 0.47}\n",
            "{'loss': 1.1837, 'learning_rate': 0.0001050709939148073, 'epoch': 0.49}\n",
            " 50% 255/509 [1:22:40<1:21:34, 19.27s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:13,  2.32it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:09,  1.63it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:04,  1.26it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:24,  1.16it/s]\u001b[A\n",
            "  2% 6/312 [00:05<06:37,  1.30s/it]\u001b[A\n",
            "  2% 7/312 [00:07<06:08,  1.21s/it]\u001b[A\n",
            "  3% 8/312 [00:07<05:43,  1.13s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:04,  1.01s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:08,  1.02s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:57,  1.01it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:52,  1.03it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:37,  1.08it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:13,  1.18it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:31,  1.10it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:11,  1.18it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:57,  1.24it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:52,  1.27it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:57,  1.24it/s]\u001b[A\n",
            "  6% 20/312 [00:17<03:49,  1.27it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:41,  1.32it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:09,  1.16it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:55,  1.23it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:51,  1.24it/s]\u001b[A\n",
            "  8% 25/312 [00:22<03:56,  1.21it/s]\u001b[A\n",
            "  8% 26/312 [00:22<03:58,  1.20it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:52,  1.22it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:01,  1.06s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:49,  1.02s/it]\u001b[A\n",
            " 10% 30/312 [00:26<04:23,  1.07it/s]\u001b[A\n",
            " 10% 31/312 [00:27<04:25,  1.06it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:45,  1.02s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:03,  1.09s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:36,  1.01it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:34,  1.01it/s]\u001b[A\n",
            " 12% 36/312 [00:33<04:25,  1.04it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:56,  1.16it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:55,  1.17it/s]\u001b[A\n",
            " 12% 39/312 [00:35<03:58,  1.15it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:06,  1.10it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:51,  1.17it/s]\u001b[A\n",
            " 13% 42/312 [00:37<03:34,  1.26it/s]\u001b[A\n",
            " 14% 43/312 [00:38<03:49,  1.17it/s]\u001b[A\n",
            " 14% 44/312 [00:40<04:24,  1.01it/s]\u001b[A\n",
            " 14% 45/312 [00:40<04:10,  1.06it/s]\u001b[A\n",
            " 15% 46/312 [00:41<03:57,  1.12it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:47,  1.17it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:40,  1.20it/s]\u001b[A\n",
            " 16% 49/312 [00:44<03:37,  1.21it/s]\u001b[A\n",
            " 16% 50/312 [00:44<03:32,  1.23it/s]\u001b[A\n",
            " 16% 51/312 [00:45<03:56,  1.10it/s]\u001b[A\n",
            " 17% 52/312 [00:46<03:37,  1.19it/s]\u001b[A\n",
            " 17% 53/312 [00:47<03:58,  1.09it/s]\u001b[A\n",
            " 17% 54/312 [00:48<03:54,  1.10it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:35,  1.19it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:36,  1.18it/s]\u001b[A\n",
            " 18% 57/312 [00:50<03:26,  1.24it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:17,  1.29it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:19,  1.27it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:25,  1.23it/s]\u001b[A\n",
            " 20% 61/312 [00:54<03:19,  1.26it/s]\u001b[A\n",
            " 20% 62/312 [00:55<04:20,  1.04s/it]\u001b[A\n",
            " 20% 63/312 [00:56<04:13,  1.02s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:54,  1.06it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:52,  1.06it/s]\u001b[A\n",
            " 21% 66/312 [00:59<03:31,  1.16it/s]\u001b[A\n",
            " 21% 67/312 [00:59<03:18,  1.23it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:09,  1.29it/s]\u001b[A\n",
            " 22% 69/312 [01:01<03:03,  1.32it/s]\u001b[A\n",
            " 22% 70/312 [01:01<02:56,  1.37it/s]\u001b[A\n",
            " 23% 71/312 [01:02<03:02,  1.32it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:11,  1.25it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:08,  1.27it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:23,  1.17it/s]\u001b[A\n",
            " 24% 75/312 [01:05<03:12,  1.23it/s]\u001b[A\n",
            " 24% 76/312 [01:06<03:03,  1.28it/s]\u001b[A\n",
            " 25% 77/312 [01:07<03:02,  1.29it/s]\u001b[A\n",
            " 25% 78/312 [01:08<03:18,  1.18it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:15,  1.19it/s]\u001b[A\n",
            " 26% 80/312 [01:10<03:06,  1.25it/s]\u001b[A\n",
            " 26% 81/312 [01:10<03:01,  1.27it/s]\u001b[A\n",
            " 26% 82/312 [01:11<03:07,  1.23it/s]\u001b[A\n",
            " 27% 83/312 [01:12<03:07,  1.22it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:11,  1.19it/s]\u001b[A\n",
            " 27% 85/312 [01:14<03:02,  1.24it/s]\u001b[A\n",
            " 28% 86/312 [01:15<03:39,  1.03it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:32,  1.06it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:29,  1.07it/s]\u001b[A\n",
            " 29% 89/312 [01:18<03:20,  1.11it/s]\u001b[A\n",
            " 29% 90/312 [01:18<03:12,  1.15it/s]\u001b[A\n",
            " 29% 91/312 [01:19<02:58,  1.24it/s]\u001b[A\n",
            " 29% 92/312 [01:20<03:00,  1.22it/s]\u001b[A\n",
            " 30% 93/312 [01:21<02:59,  1.22it/s]\u001b[A\n",
            " 30% 94/312 [01:22<03:04,  1.18it/s]\u001b[A\n",
            " 30% 95/312 [01:22<02:48,  1.29it/s]\u001b[A\n",
            " 31% 96/312 [01:23<02:52,  1.25it/s]\u001b[A\n",
            " 31% 97/312 [01:24<02:39,  1.35it/s]\u001b[A\n",
            " 31% 98/312 [01:25<02:50,  1.25it/s]\u001b[A\n",
            " 32% 99/312 [01:26<03:21,  1.06it/s]\u001b[A\n",
            " 32% 100/312 [01:27<03:03,  1.16it/s]\u001b[A\n",
            " 32% 101/312 [01:27<02:45,  1.27it/s]\u001b[A\n",
            " 33% 102/312 [01:28<02:35,  1.35it/s]\u001b[A\n",
            " 33% 103/312 [01:29<02:33,  1.36it/s]\u001b[A\n",
            " 33% 104/312 [01:30<03:19,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:01,  1.14it/s]\u001b[A\n",
            " 34% 106/312 [01:32<03:03,  1.12it/s]\u001b[A\n",
            " 34% 107/312 [01:32<02:54,  1.17it/s]\u001b[A\n",
            " 35% 108/312 [01:34<03:51,  1.13s/it]\u001b[A\n",
            " 35% 109/312 [01:35<03:31,  1.04s/it]\u001b[A\n",
            " 35% 110/312 [01:36<03:16,  1.03it/s]\u001b[A\n",
            " 36% 111/312 [01:37<02:59,  1.12it/s]\u001b[A\n",
            " 36% 112/312 [01:37<02:43,  1.22it/s]\u001b[A\n",
            " 36% 113/312 [01:38<02:44,  1.21it/s]\u001b[A\n",
            " 37% 114/312 [01:39<02:54,  1.13it/s]\u001b[A\n",
            " 37% 115/312 [01:40<02:50,  1.16it/s]\u001b[A\n",
            " 37% 116/312 [01:41<02:57,  1.10it/s]\u001b[A\n",
            " 38% 117/312 [01:42<02:46,  1.17it/s]\u001b[A\n",
            " 38% 118/312 [01:42<02:39,  1.22it/s]\u001b[A\n",
            " 38% 119/312 [01:43<02:34,  1.25it/s]\u001b[A\n",
            " 38% 120/312 [01:44<02:26,  1.31it/s]\u001b[A\n",
            " 39% 121/312 [01:45<02:49,  1.12it/s]\u001b[A\n",
            " 39% 122/312 [01:46<02:50,  1.12it/s]\u001b[A\n",
            " 39% 123/312 [01:46<02:35,  1.22it/s]\u001b[A\n",
            " 40% 124/312 [01:47<02:32,  1.23it/s]\u001b[A\n",
            " 40% 125/312 [01:50<03:59,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:50<03:33,  1.15s/it]\u001b[A\n",
            " 41% 127/312 [01:51<03:16,  1.06s/it]\u001b[A\n",
            " 41% 128/312 [01:52<02:55,  1.05it/s]\u001b[A\n",
            " 41% 129/312 [01:53<02:45,  1.10it/s]\u001b[A\n",
            " 42% 130/312 [01:54<02:39,  1.14it/s]\u001b[A\n",
            " 42% 131/312 [01:54<02:27,  1.22it/s]\u001b[A\n",
            " 42% 132/312 [01:55<02:25,  1.24it/s]\u001b[A\n",
            " 43% 133/312 [01:56<02:33,  1.16it/s]\u001b[A\n",
            " 43% 134/312 [01:57<02:30,  1.18it/s]\u001b[A\n",
            " 43% 135/312 [01:58<02:19,  1.27it/s]\u001b[A\n",
            " 44% 136/312 [01:58<02:12,  1.33it/s]\u001b[A\n",
            " 44% 137/312 [01:59<02:19,  1.26it/s]\u001b[A\n",
            " 44% 138/312 [02:00<02:23,  1.21it/s]\u001b[A\n",
            " 45% 139/312 [02:01<02:21,  1.22it/s]\u001b[A\n",
            " 45% 140/312 [02:02<02:20,  1.23it/s]\u001b[A\n",
            " 45% 141/312 [02:03<02:39,  1.07it/s]\u001b[A\n",
            " 46% 142/312 [02:04<03:13,  1.14s/it]\u001b[A\n",
            " 46% 143/312 [02:05<02:54,  1.03s/it]\u001b[A\n",
            " 46% 144/312 [02:06<02:39,  1.06it/s]\u001b[A\n",
            " 46% 145/312 [02:07<02:29,  1.11it/s]\u001b[A\n",
            " 47% 146/312 [02:08<02:24,  1.15it/s]\u001b[A\n",
            " 47% 147/312 [02:08<02:23,  1.15it/s]\u001b[A\n",
            " 47% 148/312 [02:09<02:24,  1.14it/s]\u001b[A\n",
            " 48% 149/312 [02:10<02:14,  1.21it/s]\u001b[A\n",
            " 48% 150/312 [02:11<02:07,  1.27it/s]\u001b[A\n",
            " 48% 151/312 [02:12<02:19,  1.15it/s]\u001b[A\n",
            " 49% 152/312 [02:14<03:07,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:14<02:43,  1.03s/it]\u001b[A\n",
            " 49% 154/312 [02:16<03:06,  1.18s/it]\u001b[A\n",
            " 50% 155/312 [02:17<03:12,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:19<03:15,  1.26s/it]\u001b[A\n",
            " 50% 157/312 [02:20<03:15,  1.26s/it]\u001b[A\n",
            " 51% 158/312 [02:21<02:56,  1.15s/it]\u001b[A\n",
            " 51% 159/312 [02:22<02:54,  1.14s/it]\u001b[A\n",
            " 51% 160/312 [02:23<02:37,  1.04s/it]\u001b[A\n",
            " 52% 161/312 [02:23<02:23,  1.05it/s]\u001b[A\n",
            " 52% 162/312 [02:24<02:14,  1.12it/s]\u001b[A\n",
            " 52% 163/312 [02:25<02:09,  1.15it/s]\u001b[A\n",
            " 53% 164/312 [02:26<02:12,  1.12it/s]\u001b[A\n",
            " 53% 165/312 [02:27<02:29,  1.02s/it]\u001b[A\n",
            " 53% 166/312 [02:29<02:59,  1.23s/it]\u001b[A\n",
            " 54% 167/312 [02:30<02:36,  1.08s/it]\u001b[A\n",
            " 54% 168/312 [02:31<02:25,  1.01s/it]\u001b[A\n",
            " 54% 169/312 [02:31<02:14,  1.07it/s]\u001b[A\n",
            " 54% 170/312 [02:32<02:04,  1.14it/s]\u001b[A\n",
            " 55% 171/312 [02:33<01:55,  1.22it/s]\u001b[A\n",
            " 55% 172/312 [02:34<01:54,  1.22it/s]\u001b[A\n",
            " 55% 173/312 [02:35<02:14,  1.03it/s]\u001b[A\n",
            " 56% 174/312 [02:36<02:10,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:36<01:59,  1.14it/s]\u001b[A\n",
            " 56% 176/312 [02:37<01:52,  1.21it/s]\u001b[A\n",
            " 57% 177/312 [02:38<01:49,  1.23it/s]\u001b[A\n",
            " 57% 178/312 [02:39<01:48,  1.23it/s]\u001b[A\n",
            " 57% 179/312 [02:40<01:45,  1.26it/s]\u001b[A\n",
            " 58% 180/312 [02:40<01:40,  1.31it/s]\u001b[A\n",
            " 58% 181/312 [02:41<01:36,  1.35it/s]\u001b[A\n",
            " 58% 182/312 [02:42<01:38,  1.32it/s]\u001b[A\n",
            " 59% 183/312 [02:43<01:52,  1.15it/s]\u001b[A\n",
            " 59% 184/312 [02:44<02:01,  1.05it/s]\u001b[A\n",
            " 59% 185/312 [02:45<01:55,  1.10it/s]\u001b[A\n",
            " 60% 186/312 [02:46<01:58,  1.07it/s]\u001b[A\n",
            " 60% 187/312 [02:47<01:50,  1.13it/s]\u001b[A\n",
            " 60% 188/312 [02:47<01:42,  1.22it/s]\u001b[A\n",
            " 61% 189/312 [02:48<01:36,  1.28it/s]\u001b[A\n",
            " 61% 190/312 [02:49<01:34,  1.29it/s]\u001b[A\n",
            " 61% 191/312 [02:49<01:35,  1.27it/s]\u001b[A\n",
            " 62% 192/312 [02:50<01:35,  1.26it/s]\u001b[A\n",
            " 62% 193/312 [02:51<01:30,  1.31it/s]\u001b[A\n",
            " 62% 194/312 [02:52<01:32,  1.28it/s]\u001b[A\n",
            " 62% 195/312 [02:53<01:29,  1.31it/s]\u001b[A\n",
            " 63% 196/312 [02:53<01:25,  1.36it/s]\u001b[A\n",
            " 63% 197/312 [02:54<01:23,  1.38it/s]\u001b[A\n",
            " 63% 198/312 [02:55<01:21,  1.41it/s]\u001b[A\n",
            " 64% 199/312 [02:55<01:19,  1.42it/s]\u001b[A\n",
            " 64% 200/312 [02:56<01:24,  1.33it/s]\u001b[A\n",
            " 64% 201/312 [02:57<01:27,  1.27it/s]\u001b[A\n",
            " 65% 202/312 [02:58<01:26,  1.27it/s]\u001b[A\n",
            " 65% 203/312 [02:59<01:29,  1.22it/s]\u001b[A\n",
            " 65% 204/312 [02:59<01:25,  1.27it/s]\u001b[A\n",
            " 66% 205/312 [03:00<01:20,  1.34it/s]\u001b[A\n",
            " 66% 206/312 [03:01<01:17,  1.37it/s]\u001b[A\n",
            " 66% 207/312 [03:02<01:21,  1.29it/s]\u001b[A\n",
            " 67% 208/312 [03:03<01:30,  1.15it/s]\u001b[A\n",
            " 67% 209/312 [03:03<01:23,  1.24it/s]\u001b[A\n",
            " 67% 210/312 [03:04<01:20,  1.26it/s]\u001b[A\n",
            " 68% 211/312 [03:05<01:35,  1.06it/s]\u001b[A\n",
            " 68% 212/312 [03:06<01:30,  1.11it/s]\u001b[A\n",
            " 68% 213/312 [03:07<01:30,  1.09it/s]\u001b[A\n",
            " 69% 214/312 [03:08<01:25,  1.15it/s]\u001b[A\n",
            " 69% 215/312 [03:09<01:24,  1.15it/s]\u001b[A\n",
            " 69% 216/312 [03:10<01:28,  1.08it/s]\u001b[A\n",
            " 70% 217/312 [03:11<01:23,  1.14it/s]\u001b[A\n",
            " 70% 218/312 [03:12<01:22,  1.14it/s]\u001b[A\n",
            " 70% 219/312 [03:13<01:36,  1.04s/it]\u001b[A\n",
            " 71% 220/312 [03:14<01:43,  1.12s/it]\u001b[A\n",
            " 71% 221/312 [03:15<01:37,  1.07s/it]\u001b[A\n",
            " 71% 222/312 [03:16<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:17<01:23,  1.07it/s]\u001b[A\n",
            " 72% 224/312 [03:17<01:13,  1.20it/s]\u001b[A\n",
            " 72% 225/312 [03:18<01:18,  1.11it/s]\u001b[A\n",
            " 72% 226/312 [03:19<01:14,  1.16it/s]\u001b[A\n",
            " 73% 227/312 [03:20<01:16,  1.10it/s]\u001b[A\n",
            " 73% 228/312 [03:21<01:13,  1.15it/s]\u001b[A\n",
            " 73% 229/312 [03:22<01:07,  1.23it/s]\u001b[A\n",
            " 74% 230/312 [03:22<01:04,  1.28it/s]\u001b[A\n",
            " 74% 231/312 [03:23<01:05,  1.24it/s]\u001b[A\n",
            " 74% 232/312 [03:24<00:59,  1.34it/s]\u001b[A\n",
            " 75% 233/312 [03:25<01:10,  1.12it/s]\u001b[A\n",
            " 75% 234/312 [03:26<01:04,  1.21it/s]\u001b[A\n",
            " 75% 235/312 [03:27<01:04,  1.19it/s]\u001b[A\n",
            " 76% 236/312 [03:27<01:00,  1.25it/s]\u001b[A\n",
            " 76% 237/312 [03:28<00:59,  1.25it/s]\u001b[A\n",
            " 76% 238/312 [03:29<00:56,  1.32it/s]\u001b[A\n",
            " 77% 239/312 [03:30<00:57,  1.27it/s]\u001b[A\n",
            " 77% 240/312 [03:30<00:56,  1.27it/s]\u001b[A\n",
            " 77% 241/312 [03:31<00:57,  1.24it/s]\u001b[A\n",
            " 78% 242/312 [03:32<01:04,  1.09it/s]\u001b[A\n",
            " 78% 243/312 [03:34<01:22,  1.20s/it]\u001b[A\n",
            " 78% 244/312 [03:35<01:11,  1.05s/it]\u001b[A\n",
            " 79% 245/312 [03:36<01:08,  1.02s/it]\u001b[A\n",
            " 79% 246/312 [03:37<01:07,  1.02s/it]\u001b[A\n",
            " 79% 247/312 [03:38<01:00,  1.08it/s]\u001b[A\n",
            " 79% 248/312 [03:38<00:55,  1.14it/s]\u001b[A\n",
            " 80% 249/312 [03:39<00:53,  1.18it/s]\u001b[A\n",
            " 80% 250/312 [03:40<00:56,  1.09it/s]\u001b[A\n",
            " 80% 251/312 [03:41<00:53,  1.15it/s]\u001b[A\n",
            " 81% 252/312 [03:42<00:51,  1.17it/s]\u001b[A\n",
            " 81% 253/312 [03:43<00:50,  1.16it/s]\u001b[A\n",
            " 81% 254/312 [03:44<00:49,  1.16it/s]\u001b[A\n",
            " 82% 255/312 [03:45<00:49,  1.15it/s]\u001b[A\n",
            " 82% 256/312 [03:45<00:46,  1.21it/s]\u001b[A\n",
            " 82% 257/312 [03:46<00:50,  1.10it/s]\u001b[A\n",
            " 83% 258/312 [03:47<00:46,  1.17it/s]\u001b[A\n",
            " 83% 259/312 [03:48<00:47,  1.11it/s]\u001b[A\n",
            " 83% 260/312 [03:49<00:43,  1.21it/s]\u001b[A\n",
            " 84% 261/312 [03:50<00:41,  1.24it/s]\u001b[A\n",
            " 84% 262/312 [03:50<00:41,  1.21it/s]\u001b[A\n",
            " 84% 263/312 [03:51<00:39,  1.23it/s]\u001b[A\n",
            " 85% 264/312 [03:53<00:59,  1.24s/it]\u001b[A\n",
            " 85% 265/312 [03:55<01:05,  1.40s/it]\u001b[A\n",
            " 85% 266/312 [03:56<00:56,  1.24s/it]\u001b[A\n",
            " 86% 267/312 [03:57<00:52,  1.17s/it]\u001b[A\n",
            " 86% 268/312 [03:58<00:46,  1.05s/it]\u001b[A\n",
            " 86% 269/312 [03:59<00:43,  1.01s/it]\u001b[A\n",
            " 87% 270/312 [03:59<00:39,  1.07it/s]\u001b[A\n",
            " 87% 271/312 [04:00<00:35,  1.16it/s]\u001b[A\n",
            " 87% 272/312 [04:01<00:33,  1.20it/s]\u001b[A\n",
            " 88% 273/312 [04:02<00:35,  1.09it/s]\u001b[A\n",
            " 88% 274/312 [04:03<00:33,  1.12it/s]\u001b[A\n",
            " 88% 275/312 [04:04<00:30,  1.20it/s]\u001b[A\n",
            " 88% 276/312 [04:04<00:28,  1.28it/s]\u001b[A\n",
            " 89% 277/312 [04:05<00:30,  1.13it/s]\u001b[A\n",
            " 89% 278/312 [04:06<00:30,  1.13it/s]\u001b[A\n",
            " 89% 279/312 [04:07<00:29,  1.12it/s]\u001b[A\n",
            " 90% 280/312 [04:08<00:28,  1.13it/s]\u001b[A\n",
            " 90% 281/312 [04:09<00:25,  1.22it/s]\u001b[A\n",
            " 90% 282/312 [04:09<00:23,  1.27it/s]\u001b[A\n",
            " 91% 283/312 [04:11<00:25,  1.14it/s]\u001b[A\n",
            " 91% 284/312 [04:11<00:23,  1.18it/s]\u001b[A\n",
            " 91% 285/312 [04:12<00:22,  1.20it/s]\u001b[A\n",
            " 92% 286/312 [04:13<00:21,  1.19it/s]\u001b[A\n",
            " 92% 287/312 [04:14<00:21,  1.17it/s]\u001b[A\n",
            " 92% 288/312 [04:15<00:19,  1.23it/s]\u001b[A\n",
            " 93% 289/312 [04:15<00:18,  1.27it/s]\u001b[A\n",
            " 93% 290/312 [04:16<00:17,  1.26it/s]\u001b[A\n",
            " 93% 291/312 [04:17<00:16,  1.26it/s]\u001b[A\n",
            " 94% 292/312 [04:18<00:17,  1.12it/s]\u001b[A\n",
            " 94% 293/312 [04:19<00:17,  1.07it/s]\u001b[A\n",
            " 94% 294/312 [04:20<00:15,  1.17it/s]\u001b[A\n",
            " 95% 295/312 [04:20<00:13,  1.23it/s]\u001b[A\n",
            " 95% 296/312 [04:21<00:12,  1.23it/s]\u001b[A\n",
            " 95% 297/312 [04:22<00:12,  1.18it/s]\u001b[A\n",
            " 96% 298/312 [04:23<00:11,  1.19it/s]\u001b[A\n",
            " 96% 299/312 [04:24<00:13,  1.02s/it]\u001b[A\n",
            " 96% 300/312 [04:26<00:12,  1.06s/it]\u001b[A\n",
            " 96% 301/312 [04:27<00:11,  1.03s/it]\u001b[A\n",
            " 97% 302/312 [04:27<00:09,  1.06it/s]\u001b[A\n",
            " 97% 303/312 [04:28<00:08,  1.06it/s]\u001b[A\n",
            " 97% 304/312 [04:29<00:07,  1.12it/s]\u001b[A\n",
            " 98% 305/312 [04:30<00:05,  1.17it/s]\u001b[A\n",
            " 98% 306/312 [04:31<00:05,  1.13it/s]\u001b[A\n",
            " 98% 307/312 [04:31<00:04,  1.21it/s]\u001b[A\n",
            " 99% 308/312 [04:32<00:03,  1.16it/s]\u001b[A\n",
            " 99% 309/312 [04:33<00:02,  1.24it/s]\u001b[A\n",
            " 99% 310/312 [04:34<00:01,  1.30it/s]\u001b[A\n",
            "100% 311/312 [04:34<00:00,  1.33it/s]\u001b[A\n",
            "100% 312/312 [04:35<00:00,  1.37it/s]\u001b[A\n",
            "{'eval_loss': 1.1691168546676636, 'eval_runtime': 276.8602, 'eval_samples_per_second': 18.027, 'eval_steps_per_second': 1.127, 'epoch': 0.5}\n",
            "\n",
            " 50% 255/509 [1:27:17<1:21:34, 19.27s/it]\n",
            "{'loss': 1.2598, 'learning_rate': 0.00010101419878296145, 'epoch': 0.51}\n",
            "{'loss': 1.1656, 'learning_rate': 9.695740365111563e-05, 'epoch': 0.53}\n",
            "{'loss': 1.0839, 'learning_rate': 9.290060851926979e-05, 'epoch': 0.55}\n",
            "{'loss': 1.1028, 'learning_rate': 8.884381338742393e-05, 'epoch': 0.57}\n",
            "{'loss': 1.2006, 'learning_rate': 8.478701825557809e-05, 'epoch': 0.59}\n",
            " 60% 306/509 [1:39:56<1:03:37, 18.81s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:13,  2.33it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:09,  1.63it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:03,  1.26it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:23,  1.16it/s]\u001b[A\n",
            "  2% 6/312 [00:05<06:36,  1.30s/it]\u001b[A\n",
            "  2% 7/312 [00:07<06:08,  1.21s/it]\u001b[A\n",
            "  3% 8/312 [00:07<05:43,  1.13s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:04,  1.01s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:08,  1.02s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:57,  1.01it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:52,  1.03it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:37,  1.08it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:13,  1.18it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:29,  1.10it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:10,  1.18it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:56,  1.25it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:51,  1.27it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:56,  1.24it/s]\u001b[A\n",
            "  6% 20/312 [00:17<03:48,  1.28it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:40,  1.32it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:07,  1.17it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:54,  1.23it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:50,  1.25it/s]\u001b[A\n",
            "  8% 25/312 [00:22<03:56,  1.22it/s]\u001b[A\n",
            "  8% 26/312 [00:22<03:59,  1.19it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:53,  1.22it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:02,  1.07s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:50,  1.02s/it]\u001b[A\n",
            " 10% 30/312 [00:26<04:24,  1.07it/s]\u001b[A\n",
            " 10% 31/312 [00:27<04:26,  1.05it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:45,  1.02s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:03,  1.09s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:35,  1.01it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:33,  1.01it/s]\u001b[A\n",
            " 12% 36/312 [00:33<04:25,  1.04it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:55,  1.17it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:54,  1.17it/s]\u001b[A\n",
            " 12% 39/312 [00:35<03:57,  1.15it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:05,  1.11it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:50,  1.17it/s]\u001b[A\n",
            " 13% 42/312 [00:37<03:34,  1.26it/s]\u001b[A\n",
            " 14% 43/312 [00:38<03:49,  1.17it/s]\u001b[A\n",
            " 14% 44/312 [00:40<04:24,  1.01it/s]\u001b[A\n",
            " 14% 45/312 [00:40<04:10,  1.06it/s]\u001b[A\n",
            " 15% 46/312 [00:41<03:57,  1.12it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:47,  1.16it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:41,  1.19it/s]\u001b[A\n",
            " 16% 49/312 [00:44<03:38,  1.21it/s]\u001b[A\n",
            " 16% 50/312 [00:44<03:32,  1.23it/s]\u001b[A\n",
            " 16% 51/312 [00:45<03:56,  1.10it/s]\u001b[A\n",
            " 17% 52/312 [00:46<03:37,  1.19it/s]\u001b[A\n",
            " 17% 53/312 [00:47<03:58,  1.09it/s]\u001b[A\n",
            " 17% 54/312 [00:48<03:53,  1.10it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:33,  1.20it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:35,  1.19it/s]\u001b[A\n",
            " 18% 57/312 [00:50<03:25,  1.24it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:16,  1.29it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:19,  1.27it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:25,  1.23it/s]\u001b[A\n",
            " 20% 61/312 [00:53<03:19,  1.26it/s]\u001b[A\n",
            " 20% 62/312 [00:55<04:20,  1.04s/it]\u001b[A\n",
            " 20% 63/312 [00:56<04:13,  1.02s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:54,  1.06it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:52,  1.06it/s]\u001b[A\n",
            " 21% 66/312 [00:58<03:31,  1.16it/s]\u001b[A\n",
            " 21% 67/312 [00:59<03:18,  1.23it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:09,  1.29it/s]\u001b[A\n",
            " 22% 69/312 [01:01<03:03,  1.32it/s]\u001b[A\n",
            " 22% 70/312 [01:01<02:56,  1.37it/s]\u001b[A\n",
            " 23% 71/312 [01:02<03:01,  1.32it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:10,  1.26it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:07,  1.27it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:23,  1.17it/s]\u001b[A\n",
            " 24% 75/312 [01:05<03:11,  1.24it/s]\u001b[A\n",
            " 24% 76/312 [01:06<03:03,  1.29it/s]\u001b[A\n",
            " 25% 77/312 [01:07<03:02,  1.29it/s]\u001b[A\n",
            " 25% 78/312 [01:08<03:18,  1.18it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:15,  1.19it/s]\u001b[A\n",
            " 26% 80/312 [01:09<03:06,  1.25it/s]\u001b[A\n",
            " 26% 81/312 [01:10<03:01,  1.27it/s]\u001b[A\n",
            " 26% 82/312 [01:11<03:07,  1.23it/s]\u001b[A\n",
            " 27% 83/312 [01:12<03:06,  1.23it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:10,  1.19it/s]\u001b[A\n",
            " 27% 85/312 [01:13<03:02,  1.24it/s]\u001b[A\n",
            " 28% 86/312 [01:15<03:40,  1.03it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:32,  1.06it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:29,  1.07it/s]\u001b[A\n",
            " 29% 89/312 [01:17<03:20,  1.11it/s]\u001b[A\n",
            " 29% 90/312 [01:18<03:12,  1.15it/s]\u001b[A\n",
            " 29% 91/312 [01:19<02:58,  1.24it/s]\u001b[A\n",
            " 29% 92/312 [01:20<03:00,  1.22it/s]\u001b[A\n",
            " 30% 93/312 [01:21<02:59,  1.22it/s]\u001b[A\n",
            " 30% 94/312 [01:21<03:04,  1.18it/s]\u001b[A\n",
            " 30% 95/312 [01:22<02:47,  1.29it/s]\u001b[A\n",
            " 31% 96/312 [01:23<02:52,  1.25it/s]\u001b[A\n",
            " 31% 97/312 [01:24<02:39,  1.35it/s]\u001b[A\n",
            " 31% 98/312 [01:25<02:51,  1.25it/s]\u001b[A\n",
            " 32% 99/312 [01:26<03:21,  1.06it/s]\u001b[A\n",
            " 32% 100/312 [01:26<03:03,  1.16it/s]\u001b[A\n",
            " 32% 101/312 [01:27<02:45,  1.27it/s]\u001b[A\n",
            " 33% 102/312 [01:28<02:35,  1.35it/s]\u001b[A\n",
            " 33% 103/312 [01:28<02:33,  1.36it/s]\u001b[A\n",
            " 33% 104/312 [01:30<03:19,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:01,  1.14it/s]\u001b[A\n",
            " 34% 106/312 [01:32<03:03,  1.12it/s]\u001b[A\n",
            " 34% 107/312 [01:32<02:54,  1.17it/s]\u001b[A\n",
            " 35% 108/312 [01:34<03:50,  1.13s/it]\u001b[A\n",
            " 35% 109/312 [01:35<03:30,  1.04s/it]\u001b[A\n",
            " 35% 110/312 [01:36<03:16,  1.03it/s]\u001b[A\n",
            " 36% 111/312 [01:36<02:59,  1.12it/s]\u001b[A\n",
            " 36% 112/312 [01:37<02:43,  1.22it/s]\u001b[A\n",
            " 36% 113/312 [01:38<02:44,  1.21it/s]\u001b[A\n",
            " 37% 114/312 [01:39<02:55,  1.13it/s]\u001b[A\n",
            " 37% 115/312 [01:40<02:50,  1.16it/s]\u001b[A\n",
            " 37% 116/312 [01:41<02:57,  1.10it/s]\u001b[A\n",
            " 38% 117/312 [01:41<02:46,  1.17it/s]\u001b[A\n",
            " 38% 118/312 [01:42<02:39,  1.22it/s]\u001b[A\n",
            " 38% 119/312 [01:43<02:34,  1.25it/s]\u001b[A\n",
            " 38% 120/312 [01:44<02:26,  1.31it/s]\u001b[A\n",
            " 39% 121/312 [01:45<02:49,  1.13it/s]\u001b[A\n",
            " 39% 122/312 [01:46<02:49,  1.12it/s]\u001b[A\n",
            " 39% 123/312 [01:46<02:34,  1.22it/s]\u001b[A\n",
            " 40% 124/312 [01:47<02:32,  1.24it/s]\u001b[A\n",
            " 40% 125/312 [01:50<03:58,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:50<03:32,  1.14s/it]\u001b[A\n",
            " 41% 127/312 [01:51<03:15,  1.06s/it]\u001b[A\n",
            " 41% 128/312 [01:52<02:54,  1.05it/s]\u001b[A\n",
            " 41% 129/312 [01:53<02:46,  1.10it/s]\u001b[A\n",
            " 42% 130/312 [01:54<02:40,  1.14it/s]\u001b[A\n",
            " 42% 131/312 [01:54<02:28,  1.22it/s]\u001b[A\n",
            " 42% 132/312 [01:55<02:25,  1.24it/s]\u001b[A\n",
            " 43% 133/312 [01:56<02:34,  1.16it/s]\u001b[A\n",
            " 43% 134/312 [01:57<02:31,  1.18it/s]\u001b[A\n",
            " 43% 135/312 [01:57<02:20,  1.26it/s]\u001b[A\n",
            " 44% 136/312 [01:58<02:12,  1.33it/s]\u001b[A\n",
            " 44% 137/312 [01:59<02:19,  1.26it/s]\u001b[A\n",
            " 44% 138/312 [02:00<02:23,  1.21it/s]\u001b[A\n",
            " 45% 139/312 [02:01<02:21,  1.23it/s]\u001b[A\n",
            " 45% 140/312 [02:02<02:19,  1.23it/s]\u001b[A\n",
            " 45% 141/312 [02:03<02:38,  1.08it/s]\u001b[A\n",
            " 46% 142/312 [02:04<03:12,  1.14s/it]\u001b[A\n",
            " 46% 143/312 [02:05<02:54,  1.03s/it]\u001b[A\n",
            " 46% 144/312 [02:06<02:39,  1.06it/s]\u001b[A\n",
            " 46% 145/312 [02:07<02:30,  1.11it/s]\u001b[A\n",
            " 47% 146/312 [02:07<02:24,  1.15it/s]\u001b[A\n",
            " 47% 147/312 [02:08<02:24,  1.14it/s]\u001b[A\n",
            " 47% 148/312 [02:09<02:24,  1.13it/s]\u001b[A\n",
            " 48% 149/312 [02:10<02:15,  1.21it/s]\u001b[A\n",
            " 48% 150/312 [02:11<02:07,  1.27it/s]\u001b[A\n",
            " 48% 151/312 [02:12<02:20,  1.15it/s]\u001b[A\n",
            " 49% 152/312 [02:14<03:07,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:14<02:43,  1.03s/it]\u001b[A\n",
            " 49% 154/312 [02:16<03:06,  1.18s/it]\u001b[A\n",
            " 50% 155/312 [02:17<03:12,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:18<03:15,  1.26s/it]\u001b[A\n",
            " 50% 157/312 [02:20<03:14,  1.26s/it]\u001b[A\n",
            " 51% 158/312 [02:21<02:56,  1.15s/it]\u001b[A\n",
            " 51% 159/312 [02:22<02:54,  1.14s/it]\u001b[A\n",
            " 51% 160/312 [02:23<02:37,  1.04s/it]\u001b[A\n",
            " 52% 161/312 [02:23<02:24,  1.05it/s]\u001b[A\n",
            " 52% 162/312 [02:24<02:14,  1.12it/s]\u001b[A\n",
            " 52% 163/312 [02:25<02:09,  1.15it/s]\u001b[A\n",
            " 53% 164/312 [02:26<02:12,  1.12it/s]\u001b[A\n",
            " 53% 165/312 [02:27<02:30,  1.02s/it]\u001b[A\n",
            " 53% 166/312 [02:29<03:00,  1.23s/it]\u001b[A\n",
            " 54% 167/312 [02:30<02:36,  1.08s/it]\u001b[A\n",
            " 54% 168/312 [02:30<02:26,  1.01s/it]\u001b[A\n",
            " 54% 169/312 [02:31<02:14,  1.07it/s]\u001b[A\n",
            " 54% 170/312 [02:32<02:03,  1.15it/s]\u001b[A\n",
            " 55% 171/312 [02:33<01:55,  1.22it/s]\u001b[A\n",
            " 55% 172/312 [02:33<01:54,  1.22it/s]\u001b[A\n",
            " 55% 173/312 [02:35<02:14,  1.03it/s]\u001b[A\n",
            " 56% 174/312 [02:36<02:11,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:36<01:59,  1.14it/s]\u001b[A\n",
            " 56% 176/312 [02:37<01:52,  1.21it/s]\u001b[A\n",
            " 57% 177/312 [02:38<01:49,  1.23it/s]\u001b[A\n",
            " 57% 178/312 [02:39<01:48,  1.23it/s]\u001b[A\n",
            " 57% 179/312 [02:39<01:45,  1.26it/s]\u001b[A\n",
            " 58% 180/312 [02:40<01:40,  1.31it/s]\u001b[A\n",
            " 58% 181/312 [02:41<01:37,  1.35it/s]\u001b[A\n",
            " 58% 182/312 [02:42<01:39,  1.31it/s]\u001b[A\n",
            " 59% 183/312 [02:43<01:52,  1.15it/s]\u001b[A\n",
            " 59% 184/312 [02:44<02:01,  1.05it/s]\u001b[A\n",
            " 59% 185/312 [02:45<01:55,  1.10it/s]\u001b[A\n",
            " 60% 186/312 [02:46<01:58,  1.06it/s]\u001b[A\n",
            " 60% 187/312 [02:46<01:50,  1.13it/s]\u001b[A\n",
            " 60% 188/312 [02:47<01:41,  1.22it/s]\u001b[A\n",
            " 61% 189/312 [02:48<01:36,  1.28it/s]\u001b[A\n",
            " 61% 190/312 [02:49<01:35,  1.28it/s]\u001b[A\n",
            " 61% 191/312 [02:49<01:35,  1.27it/s]\u001b[A\n",
            " 62% 192/312 [02:50<01:35,  1.25it/s]\u001b[A\n",
            " 62% 193/312 [02:51<01:31,  1.30it/s]\u001b[A\n",
            " 62% 194/312 [02:52<01:32,  1.28it/s]\u001b[A\n",
            " 62% 195/312 [02:52<01:29,  1.31it/s]\u001b[A\n",
            " 63% 196/312 [02:53<01:25,  1.36it/s]\u001b[A\n",
            " 63% 197/312 [02:54<01:23,  1.38it/s]\u001b[A\n",
            " 63% 198/312 [02:55<01:21,  1.40it/s]\u001b[A\n",
            " 64% 199/312 [02:55<01:20,  1.41it/s]\u001b[A\n",
            " 64% 200/312 [02:56<01:24,  1.32it/s]\u001b[A\n",
            " 64% 201/312 [02:57<01:27,  1.26it/s]\u001b[A\n",
            " 65% 202/312 [02:58<01:27,  1.26it/s]\u001b[A\n",
            " 65% 203/312 [02:59<01:29,  1.22it/s]\u001b[A\n",
            " 65% 204/312 [02:59<01:25,  1.26it/s]\u001b[A\n",
            " 66% 205/312 [03:00<01:20,  1.32it/s]\u001b[A\n",
            " 66% 206/312 [03:01<01:18,  1.36it/s]\u001b[A\n",
            " 66% 207/312 [03:02<01:21,  1.29it/s]\u001b[A\n",
            " 67% 208/312 [03:03<01:31,  1.14it/s]\u001b[A\n",
            " 67% 209/312 [03:03<01:23,  1.24it/s]\u001b[A\n",
            " 67% 210/312 [03:04<01:21,  1.25it/s]\u001b[A\n",
            " 68% 211/312 [03:05<01:36,  1.05it/s]\u001b[A\n",
            " 68% 212/312 [03:06<01:30,  1.10it/s]\u001b[A\n",
            " 68% 213/312 [03:07<01:30,  1.09it/s]\u001b[A\n",
            " 69% 214/312 [03:08<01:25,  1.15it/s]\u001b[A\n",
            " 69% 215/312 [03:09<01:24,  1.15it/s]\u001b[A\n",
            " 69% 216/312 [03:10<01:29,  1.07it/s]\u001b[A\n",
            " 70% 217/312 [03:11<01:23,  1.13it/s]\u001b[A\n",
            " 70% 218/312 [03:12<01:22,  1.14it/s]\u001b[A\n",
            " 70% 219/312 [03:13<01:37,  1.04s/it]\u001b[A\n",
            " 71% 220/312 [03:14<01:43,  1.13s/it]\u001b[A\n",
            " 71% 221/312 [03:15<01:37,  1.07s/it]\u001b[A\n",
            " 71% 222/312 [03:16<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:17<01:23,  1.06it/s]\u001b[A\n",
            " 72% 224/312 [03:17<01:13,  1.20it/s]\u001b[A\n",
            " 72% 225/312 [03:19<01:18,  1.11it/s]\u001b[A\n",
            " 72% 226/312 [03:19<01:14,  1.16it/s]\u001b[A\n",
            " 73% 227/312 [03:20<01:17,  1.10it/s]\u001b[A\n",
            " 73% 228/312 [03:21<01:13,  1.14it/s]\u001b[A\n",
            " 73% 229/312 [03:22<01:07,  1.23it/s]\u001b[A\n",
            " 74% 230/312 [03:22<01:04,  1.27it/s]\u001b[A\n",
            " 74% 231/312 [03:23<01:05,  1.24it/s]\u001b[A\n",
            " 74% 232/312 [03:24<00:59,  1.34it/s]\u001b[A\n",
            " 75% 233/312 [03:25<01:10,  1.12it/s]\u001b[A\n",
            " 75% 234/312 [03:26<01:04,  1.21it/s]\u001b[A\n",
            " 75% 235/312 [03:27<01:05,  1.18it/s]\u001b[A\n",
            " 76% 236/312 [03:27<01:00,  1.25it/s]\u001b[A\n",
            " 76% 237/312 [03:28<01:00,  1.25it/s]\u001b[A\n",
            " 76% 238/312 [03:29<00:56,  1.31it/s]\u001b[A\n",
            " 77% 239/312 [03:30<00:57,  1.26it/s]\u001b[A\n",
            " 77% 240/312 [03:31<00:56,  1.26it/s]\u001b[A\n",
            " 77% 241/312 [03:31<00:57,  1.23it/s]\u001b[A\n",
            " 78% 242/312 [03:33<01:04,  1.08it/s]\u001b[A\n",
            " 78% 243/312 [03:34<01:22,  1.20s/it]\u001b[A\n",
            " 78% 244/312 [03:35<01:11,  1.06s/it]\u001b[A\n",
            " 79% 245/312 [03:36<01:08,  1.03s/it]\u001b[A\n",
            " 79% 246/312 [03:37<01:07,  1.03s/it]\u001b[A\n",
            " 79% 247/312 [03:38<01:00,  1.08it/s]\u001b[A\n",
            " 79% 248/312 [03:39<00:56,  1.14it/s]\u001b[A\n",
            " 80% 249/312 [03:39<00:53,  1.17it/s]\u001b[A\n",
            " 80% 250/312 [03:41<00:56,  1.09it/s]\u001b[A\n",
            " 80% 251/312 [03:41<00:53,  1.14it/s]\u001b[A\n",
            " 81% 252/312 [03:42<00:51,  1.17it/s]\u001b[A\n",
            " 81% 253/312 [03:43<00:50,  1.16it/s]\u001b[A\n",
            " 81% 254/312 [03:44<00:50,  1.16it/s]\u001b[A\n",
            " 82% 255/312 [03:45<00:49,  1.15it/s]\u001b[A\n",
            " 82% 256/312 [03:45<00:46,  1.21it/s]\u001b[A\n",
            " 82% 257/312 [03:47<00:50,  1.10it/s]\u001b[A\n",
            " 83% 258/312 [03:47<00:46,  1.17it/s]\u001b[A\n",
            " 83% 259/312 [03:48<00:48,  1.10it/s]\u001b[A\n",
            " 83% 260/312 [03:49<00:43,  1.20it/s]\u001b[A\n",
            " 84% 261/312 [03:50<00:41,  1.23it/s]\u001b[A\n",
            " 84% 262/312 [03:51<00:41,  1.21it/s]\u001b[A\n",
            " 84% 263/312 [03:51<00:40,  1.22it/s]\u001b[A\n",
            " 85% 264/312 [03:54<00:59,  1.24s/it]\u001b[A\n",
            " 85% 265/312 [03:55<01:06,  1.40s/it]\u001b[A\n",
            " 85% 266/312 [03:56<00:56,  1.24s/it]\u001b[A\n",
            " 86% 267/312 [03:57<00:52,  1.17s/it]\u001b[A\n",
            " 86% 268/312 [03:58<00:46,  1.06s/it]\u001b[A\n",
            " 86% 269/312 [03:59<00:43,  1.01s/it]\u001b[A\n",
            " 87% 270/312 [04:00<00:39,  1.07it/s]\u001b[A\n",
            " 87% 271/312 [04:00<00:35,  1.16it/s]\u001b[A\n",
            " 87% 272/312 [04:01<00:33,  1.19it/s]\u001b[A\n",
            " 88% 273/312 [04:02<00:36,  1.07it/s]\u001b[A\n",
            " 88% 274/312 [04:03<00:34,  1.11it/s]\u001b[A\n",
            " 88% 275/312 [04:04<00:31,  1.19it/s]\u001b[A\n",
            " 88% 276/312 [04:05<00:28,  1.27it/s]\u001b[A\n",
            " 89% 277/312 [04:06<00:31,  1.12it/s]\u001b[A\n",
            " 89% 278/312 [04:07<00:30,  1.12it/s]\u001b[A\n",
            " 89% 279/312 [04:07<00:29,  1.12it/s]\u001b[A\n",
            " 90% 280/312 [04:08<00:28,  1.12it/s]\u001b[A\n",
            " 90% 281/312 [04:09<00:25,  1.21it/s]\u001b[A\n",
            " 90% 282/312 [04:10<00:23,  1.26it/s]\u001b[A\n",
            " 91% 283/312 [04:11<00:25,  1.13it/s]\u001b[A\n",
            " 91% 284/312 [04:12<00:23,  1.17it/s]\u001b[A\n",
            " 91% 285/312 [04:12<00:22,  1.20it/s]\u001b[A\n",
            " 92% 286/312 [04:13<00:21,  1.18it/s]\u001b[A\n",
            " 92% 287/312 [04:14<00:21,  1.17it/s]\u001b[A\n",
            " 92% 288/312 [04:15<00:19,  1.23it/s]\u001b[A\n",
            " 93% 289/312 [04:16<00:18,  1.27it/s]\u001b[A\n",
            " 93% 290/312 [04:16<00:17,  1.26it/s]\u001b[A\n",
            " 93% 291/312 [04:17<00:16,  1.26it/s]\u001b[A\n",
            " 94% 292/312 [04:18<00:17,  1.12it/s]\u001b[A\n",
            " 94% 293/312 [04:19<00:17,  1.07it/s]\u001b[A\n",
            " 94% 294/312 [04:20<00:15,  1.16it/s]\u001b[A\n",
            " 95% 295/312 [04:21<00:13,  1.22it/s]\u001b[A\n",
            " 95% 296/312 [04:22<00:13,  1.23it/s]\u001b[A\n",
            " 95% 297/312 [04:23<00:12,  1.17it/s]\u001b[A\n",
            " 96% 298/312 [04:23<00:11,  1.19it/s]\u001b[A\n",
            " 96% 299/312 [04:25<00:13,  1.02s/it]\u001b[A\n",
            " 96% 300/312 [04:26<00:12,  1.06s/it]\u001b[A\n",
            " 96% 301/312 [04:27<00:11,  1.04s/it]\u001b[A\n",
            " 97% 302/312 [04:28<00:09,  1.05it/s]\u001b[A\n",
            " 97% 303/312 [04:29<00:08,  1.05it/s]\u001b[A\n",
            " 97% 304/312 [04:29<00:07,  1.12it/s]\u001b[A\n",
            " 98% 305/312 [04:30<00:06,  1.16it/s]\u001b[A\n",
            " 98% 306/312 [04:31<00:05,  1.12it/s]\u001b[A\n",
            " 98% 307/312 [04:32<00:04,  1.20it/s]\u001b[A\n",
            " 99% 308/312 [04:33<00:03,  1.15it/s]\u001b[A\n",
            " 99% 309/312 [04:33<00:02,  1.23it/s]\u001b[A\n",
            " 99% 310/312 [04:34<00:01,  1.29it/s]\u001b[A\n",
            "100% 311/312 [04:35<00:00,  1.32it/s]\u001b[A\n",
            "100% 312/312 [04:36<00:00,  1.36it/s]\u001b[A\n",
            "{'eval_loss': 1.1667163372039795, 'eval_runtime': 277.2942, 'eval_samples_per_second': 17.999, 'eval_steps_per_second': 1.125, 'epoch': 0.6}\n",
            "\n",
            " 60% 306/509 [1:44:34<1:03:37, 18.81s/it]\n",
            "{'loss': 1.2458, 'learning_rate': 8.073022312373225e-05, 'epoch': 0.61}\n",
            "{'loss': 1.1349, 'learning_rate': 7.667342799188641e-05, 'epoch': 0.63}\n",
            "{'loss': 1.1002, 'learning_rate': 7.261663286004057e-05, 'epoch': 0.65}\n",
            "{'loss': 1.0865, 'learning_rate': 6.855983772819473e-05, 'epoch': 0.67}\n",
            "{'loss': 1.1817, 'learning_rate': 6.450304259634888e-05, 'epoch': 0.69}\n",
            " 70% 357/509 [1:57:19<46:57, 18.54s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:14,  2.31it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:10,  1.62it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:05,  1.26it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:25,  1.16it/s]\u001b[A\n",
            "  2% 6/312 [00:06<06:37,  1.30s/it]\u001b[A\n",
            "  2% 7/312 [00:07<06:08,  1.21s/it]\u001b[A\n",
            "  3% 8/312 [00:07<05:44,  1.13s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:05,  1.01s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:08,  1.02s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:57,  1.01it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:53,  1.02it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:38,  1.07it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:14,  1.17it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:30,  1.10it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:11,  1.18it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:57,  1.24it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:52,  1.27it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:57,  1.23it/s]\u001b[A\n",
            "  6% 20/312 [00:17<03:49,  1.27it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:41,  1.32it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:08,  1.17it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:55,  1.23it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:51,  1.24it/s]\u001b[A\n",
            "  8% 25/312 [00:22<03:57,  1.21it/s]\u001b[A\n",
            "  8% 26/312 [00:22<04:00,  1.19it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:53,  1.22it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:02,  1.07s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:50,  1.03s/it]\u001b[A\n",
            " 10% 30/312 [00:27<04:24,  1.07it/s]\u001b[A\n",
            " 10% 31/312 [00:28<04:26,  1.05it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:45,  1.02s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:03,  1.09s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:36,  1.01it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:34,  1.01it/s]\u001b[A\n",
            " 12% 36/312 [00:33<04:26,  1.04it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:56,  1.16it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:55,  1.17it/s]\u001b[A\n",
            " 12% 39/312 [00:35<03:58,  1.15it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:07,  1.10it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:51,  1.17it/s]\u001b[A\n",
            " 13% 42/312 [00:37<03:35,  1.25it/s]\u001b[A\n",
            " 14% 43/312 [00:38<03:49,  1.17it/s]\u001b[A\n",
            " 14% 44/312 [00:40<04:24,  1.01it/s]\u001b[A\n",
            " 14% 45/312 [00:40<04:11,  1.06it/s]\u001b[A\n",
            " 15% 46/312 [00:41<03:57,  1.12it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:47,  1.16it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:41,  1.19it/s]\u001b[A\n",
            " 16% 49/312 [00:44<03:38,  1.20it/s]\u001b[A\n",
            " 16% 50/312 [00:44<03:33,  1.23it/s]\u001b[A\n",
            " 16% 51/312 [00:46<03:56,  1.10it/s]\u001b[A\n",
            " 17% 52/312 [00:46<03:38,  1.19it/s]\u001b[A\n",
            " 17% 53/312 [00:47<03:58,  1.09it/s]\u001b[A\n",
            " 17% 54/312 [00:48<03:54,  1.10it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:34,  1.20it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:36,  1.18it/s]\u001b[A\n",
            " 18% 57/312 [00:50<03:26,  1.23it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:17,  1.28it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:19,  1.27it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:25,  1.23it/s]\u001b[A\n",
            " 20% 61/312 [00:54<03:20,  1.25it/s]\u001b[A\n",
            " 20% 62/312 [00:55<04:22,  1.05s/it]\u001b[A\n",
            " 20% 63/312 [00:56<04:14,  1.02s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:55,  1.05it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:53,  1.06it/s]\u001b[A\n",
            " 21% 66/312 [00:59<03:32,  1.16it/s]\u001b[A\n",
            " 21% 67/312 [00:59<03:19,  1.23it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:10,  1.28it/s]\u001b[A\n",
            " 22% 69/312 [01:01<03:04,  1.32it/s]\u001b[A\n",
            " 22% 70/312 [01:01<02:57,  1.37it/s]\u001b[A\n",
            " 23% 71/312 [01:02<03:02,  1.32it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:12,  1.25it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:09,  1.26it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:24,  1.16it/s]\u001b[A\n",
            " 24% 75/312 [01:06<03:12,  1.23it/s]\u001b[A\n",
            " 24% 76/312 [01:06<03:04,  1.28it/s]\u001b[A\n",
            " 25% 77/312 [01:07<03:03,  1.28it/s]\u001b[A\n",
            " 25% 78/312 [01:08<03:20,  1.17it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:16,  1.18it/s]\u001b[A\n",
            " 26% 80/312 [01:10<03:07,  1.23it/s]\u001b[A\n",
            " 26% 81/312 [01:10<03:03,  1.26it/s]\u001b[A\n",
            " 26% 82/312 [01:11<03:08,  1.22it/s]\u001b[A\n",
            " 27% 83/312 [01:12<03:08,  1.22it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:12,  1.19it/s]\u001b[A\n",
            " 27% 85/312 [01:14<03:03,  1.24it/s]\u001b[A\n",
            " 28% 86/312 [01:15<03:41,  1.02it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:33,  1.05it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:29,  1.07it/s]\u001b[A\n",
            " 29% 89/312 [01:18<03:21,  1.11it/s]\u001b[A\n",
            " 29% 90/312 [01:19<03:13,  1.15it/s]\u001b[A\n",
            " 29% 91/312 [01:19<02:58,  1.24it/s]\u001b[A\n",
            " 29% 92/312 [01:20<03:00,  1.22it/s]\u001b[A\n",
            " 30% 93/312 [01:21<03:00,  1.22it/s]\u001b[A\n",
            " 30% 94/312 [01:22<03:05,  1.18it/s]\u001b[A\n",
            " 30% 95/312 [01:22<02:48,  1.28it/s]\u001b[A\n",
            " 31% 96/312 [01:23<02:53,  1.24it/s]\u001b[A\n",
            " 31% 97/312 [01:24<02:40,  1.34it/s]\u001b[A\n",
            " 31% 98/312 [01:25<02:52,  1.24it/s]\u001b[A\n",
            " 32% 99/312 [01:26<03:23,  1.05it/s]\u001b[A\n",
            " 32% 100/312 [01:27<03:04,  1.15it/s]\u001b[A\n",
            " 32% 101/312 [01:27<02:47,  1.26it/s]\u001b[A\n",
            " 33% 102/312 [01:28<02:36,  1.34it/s]\u001b[A\n",
            " 33% 103/312 [01:29<02:34,  1.35it/s]\u001b[A\n",
            " 33% 104/312 [01:30<03:20,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:02,  1.13it/s]\u001b[A\n",
            " 34% 106/312 [01:32<03:04,  1.11it/s]\u001b[A\n",
            " 34% 107/312 [01:33<02:55,  1.17it/s]\u001b[A\n",
            " 35% 108/312 [01:34<03:51,  1.14s/it]\u001b[A\n",
            " 35% 109/312 [01:35<03:31,  1.04s/it]\u001b[A\n",
            " 35% 110/312 [01:36<03:17,  1.02it/s]\u001b[A\n",
            " 36% 111/312 [01:37<03:00,  1.12it/s]\u001b[A\n",
            " 36% 112/312 [01:37<02:44,  1.22it/s]\u001b[A\n",
            " 36% 113/312 [01:38<02:45,  1.20it/s]\u001b[A\n",
            " 37% 114/312 [01:39<02:56,  1.12it/s]\u001b[A\n",
            " 37% 115/312 [01:40<02:51,  1.15it/s]\u001b[A\n",
            " 37% 116/312 [01:41<02:58,  1.10it/s]\u001b[A\n",
            " 38% 117/312 [01:42<02:47,  1.16it/s]\u001b[A\n",
            " 38% 118/312 [01:43<02:40,  1.21it/s]\u001b[A\n",
            " 38% 119/312 [01:43<02:36,  1.24it/s]\u001b[A\n",
            " 38% 120/312 [01:44<02:27,  1.30it/s]\u001b[A\n",
            " 39% 121/312 [01:45<02:51,  1.11it/s]\u001b[A\n",
            " 39% 122/312 [01:46<02:51,  1.11it/s]\u001b[A\n",
            " 39% 123/312 [01:47<02:36,  1.21it/s]\u001b[A\n",
            " 40% 124/312 [01:48<02:33,  1.22it/s]\u001b[A\n",
            " 40% 125/312 [01:50<04:00,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:51<03:34,  1.16s/it]\u001b[A\n",
            " 41% 127/312 [01:52<03:18,  1.07s/it]\u001b[A\n",
            " 41% 128/312 [01:52<02:57,  1.04it/s]\u001b[A\n",
            " 41% 129/312 [01:53<02:47,  1.09it/s]\u001b[A\n",
            " 42% 130/312 [01:54<02:41,  1.13it/s]\u001b[A\n",
            " 42% 131/312 [01:55<02:29,  1.21it/s]\u001b[A\n",
            " 42% 132/312 [01:56<02:26,  1.23it/s]\u001b[A\n",
            " 43% 133/312 [01:57<02:34,  1.16it/s]\u001b[A\n",
            " 43% 134/312 [01:57<02:31,  1.18it/s]\u001b[A\n",
            " 43% 135/312 [01:58<02:20,  1.26it/s]\u001b[A\n",
            " 44% 136/312 [01:59<02:13,  1.32it/s]\u001b[A\n",
            " 44% 137/312 [02:00<02:20,  1.25it/s]\u001b[A\n",
            " 44% 138/312 [02:00<02:24,  1.20it/s]\u001b[A\n",
            " 45% 139/312 [02:01<02:22,  1.22it/s]\u001b[A\n",
            " 45% 140/312 [02:02<02:20,  1.22it/s]\u001b[A\n",
            " 45% 141/312 [02:03<02:39,  1.07it/s]\u001b[A\n",
            " 46% 142/312 [02:05<03:13,  1.14s/it]\u001b[A\n",
            " 46% 143/312 [02:06<02:54,  1.03s/it]\u001b[A\n",
            " 46% 144/312 [02:06<02:39,  1.05it/s]\u001b[A\n",
            " 46% 145/312 [02:07<02:30,  1.11it/s]\u001b[A\n",
            " 47% 146/312 [02:08<02:25,  1.14it/s]\u001b[A\n",
            " 47% 147/312 [02:09<02:24,  1.14it/s]\u001b[A\n",
            " 47% 148/312 [02:10<02:25,  1.13it/s]\u001b[A\n",
            " 48% 149/312 [02:11<02:15,  1.21it/s]\u001b[A\n",
            " 48% 150/312 [02:11<02:08,  1.27it/s]\u001b[A\n",
            " 48% 151/312 [02:12<02:20,  1.15it/s]\u001b[A\n",
            " 49% 152/312 [02:14<03:07,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:15<02:43,  1.03s/it]\u001b[A\n",
            " 49% 154/312 [02:16<03:07,  1.18s/it]\u001b[A\n",
            " 50% 155/312 [02:18<03:13,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:19<03:16,  1.26s/it]\u001b[A\n",
            " 50% 157/312 [02:20<03:15,  1.26s/it]\u001b[A\n",
            " 51% 158/312 [02:21<02:57,  1.15s/it]\u001b[A\n",
            " 51% 159/312 [02:22<02:55,  1.14s/it]\u001b[A\n",
            " 51% 160/312 [02:23<02:38,  1.04s/it]\u001b[A\n",
            " 52% 161/312 [02:24<02:24,  1.04it/s]\u001b[A\n",
            " 52% 162/312 [02:25<02:14,  1.11it/s]\u001b[A\n",
            " 52% 163/312 [02:26<02:10,  1.14it/s]\u001b[A\n",
            " 53% 164/312 [02:26<02:12,  1.12it/s]\u001b[A\n",
            " 53% 165/312 [02:28<02:30,  1.03s/it]\u001b[A\n",
            " 53% 166/312 [02:30<03:00,  1.24s/it]\u001b[A\n",
            " 54% 167/312 [02:30<02:37,  1.09s/it]\u001b[A\n",
            " 54% 168/312 [02:31<02:26,  1.02s/it]\u001b[A\n",
            " 54% 169/312 [02:32<02:14,  1.06it/s]\u001b[A\n",
            " 54% 170/312 [02:33<02:04,  1.14it/s]\u001b[A\n",
            " 55% 171/312 [02:33<01:56,  1.22it/s]\u001b[A\n",
            " 55% 172/312 [02:34<01:54,  1.22it/s]\u001b[A\n",
            " 55% 173/312 [02:35<02:15,  1.03it/s]\u001b[A\n",
            " 56% 174/312 [02:36<02:11,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:37<02:00,  1.14it/s]\u001b[A\n",
            " 56% 176/312 [02:38<01:53,  1.20it/s]\u001b[A\n",
            " 57% 177/312 [02:39<01:50,  1.23it/s]\u001b[A\n",
            " 57% 178/312 [02:39<01:49,  1.23it/s]\u001b[A\n",
            " 57% 179/312 [02:40<01:46,  1.25it/s]\u001b[A\n",
            " 58% 180/312 [02:41<01:41,  1.30it/s]\u001b[A\n",
            " 58% 181/312 [02:42<01:37,  1.35it/s]\u001b[A\n",
            " 58% 182/312 [02:42<01:39,  1.31it/s]\u001b[A\n",
            " 59% 183/312 [02:43<01:53,  1.14it/s]\u001b[A\n",
            " 59% 184/312 [02:45<02:02,  1.05it/s]\u001b[A\n",
            " 59% 185/312 [02:45<01:56,  1.09it/s]\u001b[A\n",
            " 60% 186/312 [02:46<01:58,  1.06it/s]\u001b[A\n",
            " 60% 187/312 [02:47<01:51,  1.12it/s]\u001b[A\n",
            " 60% 188/312 [02:48<01:42,  1.21it/s]\u001b[A\n",
            " 61% 189/312 [02:49<01:36,  1.27it/s]\u001b[A\n",
            " 61% 190/312 [02:49<01:35,  1.28it/s]\u001b[A\n",
            " 61% 191/312 [02:50<01:35,  1.26it/s]\u001b[A\n",
            " 62% 192/312 [02:51<01:35,  1.25it/s]\u001b[A\n",
            " 62% 193/312 [02:52<01:31,  1.30it/s]\u001b[A\n",
            " 62% 194/312 [02:53<01:32,  1.27it/s]\u001b[A\n",
            " 62% 195/312 [02:53<01:29,  1.30it/s]\u001b[A\n",
            " 63% 196/312 [02:54<01:25,  1.36it/s]\u001b[A\n",
            " 63% 197/312 [02:55<01:23,  1.38it/s]\u001b[A\n",
            " 63% 198/312 [02:55<01:21,  1.40it/s]\u001b[A\n",
            " 64% 199/312 [02:56<01:20,  1.41it/s]\u001b[A\n",
            " 64% 200/312 [02:57<01:24,  1.32it/s]\u001b[A\n",
            " 64% 201/312 [02:58<01:27,  1.26it/s]\u001b[A\n",
            " 65% 202/312 [02:59<01:27,  1.26it/s]\u001b[A\n",
            " 65% 203/312 [02:59<01:29,  1.22it/s]\u001b[A\n",
            " 65% 204/312 [03:00<01:25,  1.26it/s]\u001b[A\n",
            " 66% 205/312 [03:01<01:20,  1.33it/s]\u001b[A\n",
            " 66% 206/312 [03:01<01:17,  1.36it/s]\u001b[A\n",
            " 66% 207/312 [03:02<01:21,  1.29it/s]\u001b[A\n",
            " 67% 208/312 [03:03<01:30,  1.15it/s]\u001b[A\n",
            " 67% 209/312 [03:04<01:22,  1.24it/s]\u001b[A\n",
            " 67% 210/312 [03:05<01:21,  1.26it/s]\u001b[A\n",
            " 68% 211/312 [03:06<01:35,  1.05it/s]\u001b[A\n",
            " 68% 212/312 [03:07<01:30,  1.10it/s]\u001b[A\n",
            " 68% 213/312 [03:08<01:30,  1.09it/s]\u001b[A\n",
            " 69% 214/312 [03:09<01:25,  1.14it/s]\u001b[A\n",
            " 69% 215/312 [03:10<01:24,  1.14it/s]\u001b[A\n",
            " 69% 216/312 [03:11<01:29,  1.07it/s]\u001b[A\n",
            " 70% 217/312 [03:11<01:23,  1.13it/s]\u001b[A\n",
            " 70% 218/312 [03:12<01:22,  1.14it/s]\u001b[A\n",
            " 70% 219/312 [03:14<01:37,  1.05s/it]\u001b[A\n",
            " 71% 220/312 [03:15<01:43,  1.12s/it]\u001b[A\n",
            " 71% 221/312 [03:16<01:37,  1.07s/it]\u001b[A\n",
            " 71% 222/312 [03:17<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:18<01:23,  1.06it/s]\u001b[A\n",
            " 72% 224/312 [03:18<01:13,  1.19it/s]\u001b[A\n",
            " 72% 225/312 [03:19<01:18,  1.11it/s]\u001b[A\n",
            " 72% 226/312 [03:20<01:14,  1.16it/s]\u001b[A\n",
            " 73% 227/312 [03:21<01:17,  1.10it/s]\u001b[A\n",
            " 73% 228/312 [03:22<01:13,  1.14it/s]\u001b[A\n",
            " 73% 229/312 [03:23<01:07,  1.23it/s]\u001b[A\n",
            " 74% 230/312 [03:23<01:04,  1.27it/s]\u001b[A\n",
            " 74% 231/312 [03:24<01:05,  1.23it/s]\u001b[A\n",
            " 74% 232/312 [03:25<01:00,  1.33it/s]\u001b[A\n",
            " 75% 233/312 [03:26<01:10,  1.11it/s]\u001b[A\n",
            " 75% 234/312 [03:27<01:04,  1.20it/s]\u001b[A\n",
            " 75% 235/312 [03:28<01:05,  1.18it/s]\u001b[A\n",
            " 76% 236/312 [03:28<01:00,  1.25it/s]\u001b[A\n",
            " 76% 237/312 [03:29<01:00,  1.25it/s]\u001b[A\n",
            " 76% 238/312 [03:30<00:56,  1.31it/s]\u001b[A\n",
            " 77% 239/312 [03:31<00:57,  1.26it/s]\u001b[A\n",
            " 77% 240/312 [03:31<00:56,  1.27it/s]\u001b[A\n",
            " 77% 241/312 [03:32<00:57,  1.23it/s]\u001b[A\n",
            " 78% 242/312 [03:33<01:04,  1.08it/s]\u001b[A\n",
            " 78% 243/312 [03:35<01:22,  1.20s/it]\u001b[A\n",
            " 78% 244/312 [03:36<01:11,  1.06s/it]\u001b[A\n",
            " 79% 245/312 [03:37<01:08,  1.03s/it]\u001b[A\n",
            " 79% 246/312 [03:38<01:07,  1.03s/it]\u001b[A\n",
            " 79% 247/312 [03:39<01:00,  1.08it/s]\u001b[A\n",
            " 79% 248/312 [03:39<00:56,  1.14it/s]\u001b[A\n",
            " 80% 249/312 [03:40<00:53,  1.17it/s]\u001b[A\n",
            " 80% 250/312 [03:41<00:57,  1.09it/s]\u001b[A\n",
            " 80% 251/312 [03:42<00:53,  1.14it/s]\u001b[A\n",
            " 81% 252/312 [03:43<00:51,  1.16it/s]\u001b[A\n",
            " 81% 253/312 [03:44<00:51,  1.16it/s]\u001b[A\n",
            " 81% 254/312 [03:45<00:50,  1.15it/s]\u001b[A\n",
            " 82% 255/312 [03:45<00:49,  1.15it/s]\u001b[A\n",
            " 82% 256/312 [03:46<00:46,  1.21it/s]\u001b[A\n",
            " 82% 257/312 [03:47<00:50,  1.10it/s]\u001b[A\n",
            " 83% 258/312 [03:48<00:46,  1.17it/s]\u001b[A\n",
            " 83% 259/312 [03:49<00:47,  1.10it/s]\u001b[A\n",
            " 83% 260/312 [03:50<00:43,  1.20it/s]\u001b[A\n",
            " 84% 261/312 [03:50<00:41,  1.24it/s]\u001b[A\n",
            " 84% 262/312 [03:51<00:41,  1.21it/s]\u001b[A\n",
            " 84% 263/312 [03:52<00:40,  1.22it/s]\u001b[A\n",
            " 85% 264/312 [03:54<00:59,  1.24s/it]\u001b[A\n",
            " 85% 265/312 [03:56<01:05,  1.40s/it]\u001b[A\n",
            " 85% 266/312 [03:57<00:56,  1.24s/it]\u001b[A\n",
            " 86% 267/312 [03:58<00:52,  1.17s/it]\u001b[A\n",
            " 86% 268/312 [03:59<00:46,  1.06s/it]\u001b[A\n",
            " 86% 269/312 [04:00<00:43,  1.01s/it]\u001b[A\n",
            " 87% 270/312 [04:00<00:39,  1.07it/s]\u001b[A\n",
            " 87% 271/312 [04:01<00:35,  1.16it/s]\u001b[A\n",
            " 87% 272/312 [04:02<00:33,  1.19it/s]\u001b[A\n",
            " 88% 273/312 [04:03<00:36,  1.08it/s]\u001b[A\n",
            " 88% 274/312 [04:04<00:34,  1.11it/s]\u001b[A\n",
            " 88% 275/312 [04:05<00:31,  1.19it/s]\u001b[A\n",
            " 88% 276/312 [04:05<00:28,  1.27it/s]\u001b[A\n",
            " 89% 277/312 [04:06<00:31,  1.13it/s]\u001b[A\n",
            " 89% 278/312 [04:07<00:30,  1.12it/s]\u001b[A\n",
            " 89% 279/312 [04:08<00:29,  1.12it/s]\u001b[A\n",
            " 90% 280/312 [04:09<00:28,  1.13it/s]\u001b[A\n",
            " 90% 281/312 [04:10<00:25,  1.21it/s]\u001b[A\n",
            " 90% 282/312 [04:10<00:23,  1.26it/s]\u001b[A\n",
            " 91% 283/312 [04:12<00:25,  1.13it/s]\u001b[A\n",
            " 91% 284/312 [04:12<00:23,  1.17it/s]\u001b[A\n",
            " 91% 285/312 [04:13<00:22,  1.20it/s]\u001b[A\n",
            " 92% 286/312 [04:14<00:21,  1.18it/s]\u001b[A\n",
            " 92% 287/312 [04:15<00:21,  1.17it/s]\u001b[A\n",
            " 92% 288/312 [04:16<00:19,  1.23it/s]\u001b[A\n",
            " 93% 289/312 [04:16<00:18,  1.27it/s]\u001b[A\n",
            " 93% 290/312 [04:17<00:17,  1.26it/s]\u001b[A\n",
            " 93% 291/312 [04:18<00:16,  1.26it/s]\u001b[A\n",
            " 94% 292/312 [04:19<00:17,  1.11it/s]\u001b[A\n",
            " 94% 293/312 [04:20<00:17,  1.07it/s]\u001b[A\n",
            " 94% 294/312 [04:21<00:15,  1.16it/s]\u001b[A\n",
            " 95% 295/312 [04:22<00:13,  1.22it/s]\u001b[A\n",
            " 95% 296/312 [04:22<00:13,  1.22it/s]\u001b[A\n",
            " 95% 297/312 [04:23<00:12,  1.17it/s]\u001b[A\n",
            " 96% 298/312 [04:24<00:11,  1.18it/s]\u001b[A\n",
            " 96% 299/312 [04:26<00:13,  1.02s/it]\u001b[A\n",
            " 96% 300/312 [04:27<00:12,  1.06s/it]\u001b[A\n",
            " 96% 301/312 [04:28<00:11,  1.04s/it]\u001b[A\n",
            " 97% 302/312 [04:28<00:09,  1.05it/s]\u001b[A\n",
            " 97% 303/312 [04:29<00:08,  1.05it/s]\u001b[A\n",
            " 97% 304/312 [04:30<00:07,  1.12it/s]\u001b[A\n",
            " 98% 305/312 [04:31<00:06,  1.16it/s]\u001b[A\n",
            " 98% 306/312 [04:32<00:05,  1.12it/s]\u001b[A\n",
            " 98% 307/312 [04:33<00:04,  1.20it/s]\u001b[A\n",
            " 99% 308/312 [04:34<00:03,  1.15it/s]\u001b[A\n",
            " 99% 309/312 [04:34<00:02,  1.23it/s]\u001b[A\n",
            " 99% 310/312 [04:35<00:01,  1.28it/s]\u001b[A\n",
            "100% 311/312 [04:36<00:00,  1.32it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.164762020111084, 'eval_runtime': 278.0958, 'eval_samples_per_second': 17.947, 'eval_steps_per_second': 1.122, 'epoch': 0.7}\n",
            " 70% 357/509 [2:01:57<46:57, 18.54s/it]\n",
            "100% 312/312 [04:36<00:00,  1.35it/s]\u001b[A\n",
            "{'loss': 1.2482, 'learning_rate': 6.044624746450305e-05, 'epoch': 0.71}\n",
            "{'loss': 1.1347, 'learning_rate': 5.63894523326572e-05, 'epoch': 0.73}\n",
            "{'loss': 1.0969, 'learning_rate': 5.233265720081136e-05, 'epoch': 0.75}\n",
            "{'loss': 1.0779, 'learning_rate': 4.827586206896552e-05, 'epoch': 0.76}\n",
            "{'loss': 1.2066, 'learning_rate': 4.421906693711968e-05, 'epoch': 0.78}\n",
            " 80% 408/509 [2:14:41<30:45, 18.27s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:13,  2.31it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:10,  1.63it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:04,  1.26it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:24,  1.16it/s]\u001b[A\n",
            "  2% 6/312 [00:05<06:37,  1.30s/it]\u001b[A\n",
            "  2% 7/312 [00:07<06:08,  1.21s/it]\u001b[A\n",
            "  3% 8/312 [00:07<05:45,  1.14s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:06,  1.01s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:09,  1.02s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:59,  1.01it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:54,  1.02it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:39,  1.07it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:14,  1.17it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:31,  1.09it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:12,  1.17it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:57,  1.24it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:52,  1.27it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:57,  1.24it/s]\u001b[A\n",
            "  6% 20/312 [00:17<03:49,  1.27it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:41,  1.32it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:08,  1.17it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:55,  1.23it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:51,  1.25it/s]\u001b[A\n",
            "  8% 25/312 [00:22<03:56,  1.21it/s]\u001b[A\n",
            "  8% 26/312 [00:22<03:59,  1.19it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:53,  1.22it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:02,  1.07s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:50,  1.03s/it]\u001b[A\n",
            " 10% 30/312 [00:27<04:24,  1.07it/s]\u001b[A\n",
            " 10% 31/312 [00:28<04:27,  1.05it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:46,  1.02s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:04,  1.09s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:36,  1.01it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:34,  1.01it/s]\u001b[A\n",
            " 12% 36/312 [00:33<04:26,  1.04it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:56,  1.16it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:55,  1.17it/s]\u001b[A\n",
            " 12% 39/312 [00:35<03:58,  1.15it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:06,  1.10it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:51,  1.17it/s]\u001b[A\n",
            " 13% 42/312 [00:37<03:35,  1.25it/s]\u001b[A\n",
            " 14% 43/312 [00:38<03:49,  1.17it/s]\u001b[A\n",
            " 14% 44/312 [00:40<04:24,  1.01it/s]\u001b[A\n",
            " 14% 45/312 [00:41<04:11,  1.06it/s]\u001b[A\n",
            " 15% 46/312 [00:41<03:58,  1.12it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:49,  1.16it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:42,  1.19it/s]\u001b[A\n",
            " 16% 49/312 [00:44<03:38,  1.20it/s]\u001b[A\n",
            " 16% 50/312 [00:44<03:33,  1.22it/s]\u001b[A\n",
            " 16% 51/312 [00:46<03:57,  1.10it/s]\u001b[A\n",
            " 17% 52/312 [00:46<03:38,  1.19it/s]\u001b[A\n",
            " 17% 53/312 [00:47<03:58,  1.08it/s]\u001b[A\n",
            " 17% 54/312 [00:48<03:54,  1.10it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:34,  1.20it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:36,  1.18it/s]\u001b[A\n",
            " 18% 57/312 [00:51<03:26,  1.24it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:17,  1.29it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:19,  1.27it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:25,  1.23it/s]\u001b[A\n",
            " 20% 61/312 [00:54<03:20,  1.25it/s]\u001b[A\n",
            " 20% 62/312 [00:55<04:21,  1.05s/it]\u001b[A\n",
            " 20% 63/312 [00:56<04:14,  1.02s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:55,  1.05it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:53,  1.06it/s]\u001b[A\n",
            " 21% 66/312 [00:59<03:32,  1.16it/s]\u001b[A\n",
            " 21% 67/312 [00:59<03:19,  1.23it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:10,  1.28it/s]\u001b[A\n",
            " 22% 69/312 [01:01<03:04,  1.31it/s]\u001b[A\n",
            " 22% 70/312 [01:01<02:57,  1.36it/s]\u001b[A\n",
            " 23% 71/312 [01:02<03:03,  1.31it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:12,  1.25it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:08,  1.27it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:23,  1.17it/s]\u001b[A\n",
            " 24% 75/312 [01:06<03:12,  1.23it/s]\u001b[A\n",
            " 24% 76/312 [01:06<03:03,  1.28it/s]\u001b[A\n",
            " 25% 77/312 [01:07<03:03,  1.28it/s]\u001b[A\n",
            " 25% 78/312 [01:08<03:19,  1.17it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:16,  1.19it/s]\u001b[A\n",
            " 26% 80/312 [01:10<03:07,  1.24it/s]\u001b[A\n",
            " 26% 81/312 [01:10<03:02,  1.26it/s]\u001b[A\n",
            " 26% 82/312 [01:11<03:08,  1.22it/s]\u001b[A\n",
            " 27% 83/312 [01:12<03:08,  1.22it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:12,  1.19it/s]\u001b[A\n",
            " 27% 85/312 [01:14<03:03,  1.24it/s]\u001b[A\n",
            " 28% 86/312 [01:15<03:40,  1.02it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:33,  1.05it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:30,  1.07it/s]\u001b[A\n",
            " 29% 89/312 [01:18<03:21,  1.11it/s]\u001b[A\n",
            " 29% 90/312 [01:19<03:13,  1.15it/s]\u001b[A\n",
            " 29% 91/312 [01:19<02:58,  1.23it/s]\u001b[A\n",
            " 29% 92/312 [01:20<03:01,  1.21it/s]\u001b[A\n",
            " 30% 93/312 [01:21<03:00,  1.22it/s]\u001b[A\n",
            " 30% 94/312 [01:22<03:04,  1.18it/s]\u001b[A\n",
            " 30% 95/312 [01:22<02:48,  1.29it/s]\u001b[A\n",
            " 31% 96/312 [01:23<02:52,  1.25it/s]\u001b[A\n",
            " 31% 97/312 [01:24<02:40,  1.34it/s]\u001b[A\n",
            " 31% 98/312 [01:25<02:51,  1.25it/s]\u001b[A\n",
            " 32% 99/312 [01:26<03:22,  1.05it/s]\u001b[A\n",
            " 32% 100/312 [01:27<03:03,  1.15it/s]\u001b[A\n",
            " 32% 101/312 [01:27<02:46,  1.27it/s]\u001b[A\n",
            " 33% 102/312 [01:28<02:36,  1.35it/s]\u001b[A\n",
            " 33% 103/312 [01:29<02:33,  1.36it/s]\u001b[A\n",
            " 33% 104/312 [01:30<03:19,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:01,  1.14it/s]\u001b[A\n",
            " 34% 106/312 [01:32<03:04,  1.12it/s]\u001b[A\n",
            " 34% 107/312 [01:33<02:55,  1.17it/s]\u001b[A\n",
            " 35% 108/312 [01:34<03:51,  1.13s/it]\u001b[A\n",
            " 35% 109/312 [01:35<03:31,  1.04s/it]\u001b[A\n",
            " 35% 110/312 [01:36<03:17,  1.02it/s]\u001b[A\n",
            " 36% 111/312 [01:37<02:59,  1.12it/s]\u001b[A\n",
            " 36% 112/312 [01:37<02:43,  1.22it/s]\u001b[A\n",
            " 36% 113/312 [01:38<02:45,  1.20it/s]\u001b[A\n",
            " 37% 114/312 [01:39<02:55,  1.13it/s]\u001b[A\n",
            " 37% 115/312 [01:40<02:50,  1.15it/s]\u001b[A\n",
            " 37% 116/312 [01:41<02:58,  1.10it/s]\u001b[A\n",
            " 38% 117/312 [01:42<02:46,  1.17it/s]\u001b[A\n",
            " 38% 118/312 [01:43<02:39,  1.21it/s]\u001b[A\n",
            " 38% 119/312 [01:43<02:35,  1.24it/s]\u001b[A\n",
            " 38% 120/312 [01:44<02:27,  1.31it/s]\u001b[A\n",
            " 39% 121/312 [01:45<02:50,  1.12it/s]\u001b[A\n",
            " 39% 122/312 [01:46<02:49,  1.12it/s]\u001b[A\n",
            " 39% 123/312 [01:47<02:35,  1.22it/s]\u001b[A\n",
            " 40% 124/312 [01:48<02:32,  1.23it/s]\u001b[A\n",
            " 40% 125/312 [01:50<03:59,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:51<03:33,  1.15s/it]\u001b[A\n",
            " 41% 127/312 [01:52<03:16,  1.06s/it]\u001b[A\n",
            " 41% 128/312 [01:52<02:56,  1.05it/s]\u001b[A\n",
            " 41% 129/312 [01:53<02:46,  1.10it/s]\u001b[A\n",
            " 42% 130/312 [01:54<02:41,  1.13it/s]\u001b[A\n",
            " 42% 131/312 [01:55<02:29,  1.21it/s]\u001b[A\n",
            " 42% 132/312 [01:55<02:26,  1.23it/s]\u001b[A\n",
            " 43% 133/312 [01:56<02:34,  1.16it/s]\u001b[A\n",
            " 43% 134/312 [01:57<02:31,  1.18it/s]\u001b[A\n",
            " 43% 135/312 [01:58<02:20,  1.26it/s]\u001b[A\n",
            " 44% 136/312 [01:59<02:13,  1.32it/s]\u001b[A\n",
            " 44% 137/312 [01:59<02:19,  1.25it/s]\u001b[A\n",
            " 44% 138/312 [02:00<02:23,  1.21it/s]\u001b[A\n",
            " 45% 139/312 [02:01<02:21,  1.22it/s]\u001b[A\n",
            " 45% 140/312 [02:02<02:20,  1.23it/s]\u001b[A\n",
            " 45% 141/312 [02:03<02:39,  1.07it/s]\u001b[A\n",
            " 46% 142/312 [02:05<03:13,  1.14s/it]\u001b[A\n",
            " 46% 143/312 [02:06<02:54,  1.03s/it]\u001b[A\n",
            " 46% 144/312 [02:06<02:39,  1.05it/s]\u001b[A\n",
            " 46% 145/312 [02:07<02:30,  1.11it/s]\u001b[A\n",
            " 47% 146/312 [02:08<02:25,  1.14it/s]\u001b[A\n",
            " 47% 147/312 [02:09<02:24,  1.14it/s]\u001b[A\n",
            " 47% 148/312 [02:10<02:25,  1.13it/s]\u001b[A\n",
            " 48% 149/312 [02:10<02:15,  1.21it/s]\u001b[A\n",
            " 48% 150/312 [02:11<02:08,  1.26it/s]\u001b[A\n",
            " 48% 151/312 [02:12<02:20,  1.15it/s]\u001b[A\n",
            " 49% 152/312 [02:14<03:07,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:15<02:43,  1.03s/it]\u001b[A\n",
            " 49% 154/312 [02:16<03:06,  1.18s/it]\u001b[A\n",
            " 50% 155/312 [02:18<03:13,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:19<03:15,  1.26s/it]\u001b[A\n",
            " 50% 157/312 [02:20<03:14,  1.26s/it]\u001b[A\n",
            " 51% 158/312 [02:21<02:56,  1.15s/it]\u001b[A\n",
            " 51% 159/312 [02:22<02:54,  1.14s/it]\u001b[A\n",
            " 51% 160/312 [02:23<02:38,  1.04s/it]\u001b[A\n",
            " 52% 161/312 [02:24<02:24,  1.04it/s]\u001b[A\n",
            " 52% 162/312 [02:25<02:14,  1.11it/s]\u001b[A\n",
            " 52% 163/312 [02:25<02:10,  1.14it/s]\u001b[A\n",
            " 53% 164/312 [02:26<02:12,  1.11it/s]\u001b[A\n",
            " 53% 165/312 [02:28<02:31,  1.03s/it]\u001b[A\n",
            " 53% 166/312 [02:29<03:00,  1.24s/it]\u001b[A\n",
            " 54% 167/312 [02:30<02:37,  1.09s/it]\u001b[A\n",
            " 54% 168/312 [02:31<02:26,  1.02s/it]\u001b[A\n",
            " 54% 169/312 [02:32<02:14,  1.06it/s]\u001b[A\n",
            " 54% 170/312 [02:32<02:04,  1.14it/s]\u001b[A\n",
            " 55% 171/312 [02:33<01:55,  1.22it/s]\u001b[A\n",
            " 55% 172/312 [02:34<01:54,  1.22it/s]\u001b[A\n",
            " 55% 173/312 [02:35<02:14,  1.03it/s]\u001b[A\n",
            " 56% 174/312 [02:36<02:11,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:37<02:00,  1.14it/s]\u001b[A\n",
            " 56% 176/312 [02:38<01:53,  1.20it/s]\u001b[A\n",
            " 57% 177/312 [02:38<01:50,  1.23it/s]\u001b[A\n",
            " 57% 178/312 [02:39<01:48,  1.23it/s]\u001b[A\n",
            " 57% 179/312 [02:40<01:46,  1.25it/s]\u001b[A\n",
            " 58% 180/312 [02:41<01:41,  1.31it/s]\u001b[A\n",
            " 58% 181/312 [02:41<01:37,  1.35it/s]\u001b[A\n",
            " 58% 182/312 [02:42<01:39,  1.31it/s]\u001b[A\n",
            " 59% 183/312 [02:43<01:53,  1.14it/s]\u001b[A\n",
            " 59% 184/312 [02:44<02:02,  1.05it/s]\u001b[A\n",
            " 59% 185/312 [02:45<01:56,  1.09it/s]\u001b[A\n",
            " 60% 186/312 [02:46<01:59,  1.06it/s]\u001b[A\n",
            " 60% 187/312 [02:47<01:51,  1.12it/s]\u001b[A\n",
            " 60% 188/312 [02:48<01:42,  1.21it/s]\u001b[A\n",
            " 61% 189/312 [02:48<01:36,  1.28it/s]\u001b[A\n",
            " 61% 190/312 [02:49<01:35,  1.28it/s]\u001b[A\n",
            " 61% 191/312 [02:50<01:36,  1.26it/s]\u001b[A\n",
            " 62% 192/312 [02:51<01:36,  1.25it/s]\u001b[A\n",
            " 62% 193/312 [02:52<01:31,  1.30it/s]\u001b[A\n",
            " 62% 194/312 [02:52<01:32,  1.27it/s]\u001b[A\n",
            " 62% 195/312 [02:53<01:29,  1.31it/s]\u001b[A\n",
            " 63% 196/312 [02:54<01:25,  1.36it/s]\u001b[A\n",
            " 63% 197/312 [02:54<01:23,  1.38it/s]\u001b[A\n",
            " 63% 198/312 [02:55<01:21,  1.40it/s]\u001b[A\n",
            " 64% 199/312 [02:56<01:20,  1.41it/s]\u001b[A\n",
            " 64% 200/312 [02:57<01:24,  1.32it/s]\u001b[A\n",
            " 64% 201/312 [02:58<01:27,  1.27it/s]\u001b[A\n",
            " 65% 202/312 [02:58<01:27,  1.26it/s]\u001b[A\n",
            " 65% 203/312 [02:59<01:29,  1.22it/s]\u001b[A\n",
            " 65% 204/312 [03:00<01:25,  1.26it/s]\u001b[A\n",
            " 66% 205/312 [03:01<01:20,  1.33it/s]\u001b[A\n",
            " 66% 206/312 [03:01<01:17,  1.36it/s]\u001b[A\n",
            " 66% 207/312 [03:02<01:21,  1.29it/s]\u001b[A\n",
            " 67% 208/312 [03:03<01:30,  1.15it/s]\u001b[A\n",
            " 67% 209/312 [03:04<01:23,  1.24it/s]\u001b[A\n",
            " 67% 210/312 [03:05<01:21,  1.25it/s]\u001b[A\n",
            " 68% 211/312 [03:06<01:36,  1.05it/s]\u001b[A\n",
            " 68% 212/312 [03:07<01:30,  1.10it/s]\u001b[A\n",
            " 68% 213/312 [03:08<01:30,  1.09it/s]\u001b[A\n",
            " 69% 214/312 [03:09<01:25,  1.14it/s]\u001b[A\n",
            " 69% 215/312 [03:09<01:24,  1.14it/s]\u001b[A\n",
            " 69% 216/312 [03:10<01:29,  1.07it/s]\u001b[A\n",
            " 70% 217/312 [03:11<01:23,  1.13it/s]\u001b[A\n",
            " 70% 218/312 [03:12<01:22,  1.14it/s]\u001b[A\n",
            " 70% 219/312 [03:14<01:37,  1.04s/it]\u001b[A\n",
            " 71% 220/312 [03:15<01:43,  1.12s/it]\u001b[A\n",
            " 71% 221/312 [03:16<01:37,  1.07s/it]\u001b[A\n",
            " 71% 222/312 [03:17<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:17<01:23,  1.06it/s]\u001b[A\n",
            " 72% 224/312 [03:18<01:13,  1.19it/s]\u001b[A\n",
            " 72% 225/312 [03:19<01:18,  1.11it/s]\u001b[A\n",
            " 72% 226/312 [03:20<01:14,  1.16it/s]\u001b[A\n",
            " 73% 227/312 [03:21<01:17,  1.10it/s]\u001b[A\n",
            " 73% 228/312 [03:22<01:13,  1.15it/s]\u001b[A\n",
            " 73% 229/312 [03:22<01:07,  1.23it/s]\u001b[A\n",
            " 74% 230/312 [03:23<01:04,  1.27it/s]\u001b[A\n",
            " 74% 231/312 [03:24<01:05,  1.24it/s]\u001b[A\n",
            " 74% 232/312 [03:25<00:59,  1.34it/s]\u001b[A\n",
            " 75% 233/312 [03:26<01:10,  1.12it/s]\u001b[A\n",
            " 75% 234/312 [03:26<01:04,  1.21it/s]\u001b[A\n",
            " 75% 235/312 [03:27<01:05,  1.18it/s]\u001b[A\n",
            " 76% 236/312 [03:28<01:00,  1.25it/s]\u001b[A\n",
            " 76% 237/312 [03:29<01:00,  1.25it/s]\u001b[A\n",
            " 76% 238/312 [03:29<00:56,  1.31it/s]\u001b[A\n",
            " 77% 239/312 [03:30<00:57,  1.26it/s]\u001b[A\n",
            " 77% 240/312 [03:31<00:56,  1.27it/s]\u001b[A\n",
            " 77% 241/312 [03:32<00:57,  1.24it/s]\u001b[A\n",
            " 78% 242/312 [03:33<01:04,  1.08it/s]\u001b[A\n",
            " 78% 243/312 [03:35<01:22,  1.20s/it]\u001b[A\n",
            " 78% 244/312 [03:36<01:11,  1.06s/it]\u001b[A\n",
            " 79% 245/312 [03:37<01:08,  1.03s/it]\u001b[A\n",
            " 79% 246/312 [03:38<01:07,  1.03s/it]\u001b[A\n",
            " 79% 247/312 [03:38<01:00,  1.07it/s]\u001b[A\n",
            " 79% 248/312 [03:39<00:56,  1.14it/s]\u001b[A\n",
            " 80% 249/312 [03:40<00:53,  1.17it/s]\u001b[A\n",
            " 80% 250/312 [03:41<00:56,  1.09it/s]\u001b[A\n",
            " 80% 251/312 [03:42<00:53,  1.15it/s]\u001b[A\n",
            " 81% 252/312 [03:43<00:51,  1.17it/s]\u001b[A\n",
            " 81% 253/312 [03:44<00:50,  1.16it/s]\u001b[A\n",
            " 81% 254/312 [03:44<00:50,  1.16it/s]\u001b[A\n",
            " 82% 255/312 [03:45<00:49,  1.16it/s]\u001b[A\n",
            " 82% 256/312 [03:46<00:46,  1.22it/s]\u001b[A\n",
            " 82% 257/312 [03:47<00:50,  1.10it/s]\u001b[A\n",
            " 83% 258/312 [03:48<00:46,  1.17it/s]\u001b[A\n",
            " 83% 259/312 [03:49<00:47,  1.11it/s]\u001b[A\n",
            " 83% 260/312 [03:49<00:43,  1.21it/s]\u001b[A\n",
            " 84% 261/312 [03:50<00:41,  1.24it/s]\u001b[A\n",
            " 84% 262/312 [03:51<00:41,  1.21it/s]\u001b[A\n",
            " 84% 263/312 [03:52<00:40,  1.22it/s]\u001b[A\n",
            " 85% 264/312 [03:54<00:59,  1.24s/it]\u001b[A\n",
            " 85% 265/312 [03:56<01:06,  1.40s/it]\u001b[A\n",
            " 85% 266/312 [03:57<00:56,  1.24s/it]\u001b[A\n",
            " 86% 267/312 [03:58<00:52,  1.17s/it]\u001b[A\n",
            " 86% 268/312 [03:59<00:46,  1.05s/it]\u001b[A\n",
            " 86% 269/312 [03:59<00:43,  1.01s/it]\u001b[A\n",
            " 87% 270/312 [04:00<00:39,  1.07it/s]\u001b[A\n",
            " 87% 271/312 [04:01<00:35,  1.16it/s]\u001b[A\n",
            " 87% 272/312 [04:02<00:33,  1.20it/s]\u001b[A\n",
            " 88% 273/312 [04:03<00:36,  1.08it/s]\u001b[A\n",
            " 88% 274/312 [04:04<00:33,  1.12it/s]\u001b[A\n",
            " 88% 275/312 [04:04<00:30,  1.20it/s]\u001b[A\n",
            " 88% 276/312 [04:05<00:28,  1.28it/s]\u001b[A\n",
            " 89% 277/312 [04:06<00:31,  1.13it/s]\u001b[A\n",
            " 89% 278/312 [04:07<00:30,  1.12it/s]\u001b[A\n",
            " 89% 279/312 [04:08<00:29,  1.12it/s]\u001b[A\n",
            " 90% 280/312 [04:09<00:28,  1.13it/s]\u001b[A\n",
            " 90% 281/312 [04:09<00:25,  1.21it/s]\u001b[A\n",
            " 90% 282/312 [04:10<00:23,  1.26it/s]\u001b[A\n",
            " 91% 283/312 [04:11<00:25,  1.13it/s]\u001b[A\n",
            " 91% 284/312 [04:12<00:23,  1.17it/s]\u001b[A\n",
            " 91% 285/312 [04:13<00:22,  1.20it/s]\u001b[A\n",
            " 92% 286/312 [04:14<00:21,  1.18it/s]\u001b[A\n",
            " 92% 287/312 [04:15<00:21,  1.17it/s]\u001b[A\n",
            " 92% 288/312 [04:15<00:19,  1.22it/s]\u001b[A\n",
            " 93% 289/312 [04:16<00:18,  1.27it/s]\u001b[A\n",
            " 93% 290/312 [04:17<00:17,  1.26it/s]\u001b[A\n",
            " 93% 291/312 [04:18<00:16,  1.26it/s]\u001b[A\n",
            " 94% 292/312 [04:19<00:17,  1.12it/s]\u001b[A\n",
            " 94% 293/312 [04:20<00:17,  1.07it/s]\u001b[A\n",
            " 94% 294/312 [04:21<00:15,  1.16it/s]\u001b[A\n",
            " 95% 295/312 [04:21<00:13,  1.22it/s]\u001b[A\n",
            " 95% 296/312 [04:22<00:13,  1.23it/s]\u001b[A\n",
            " 95% 297/312 [04:23<00:12,  1.17it/s]\u001b[A\n",
            " 96% 298/312 [04:24<00:11,  1.19it/s]\u001b[A\n",
            " 96% 299/312 [04:25<00:13,  1.02s/it]\u001b[A\n",
            " 96% 300/312 [04:26<00:12,  1.06s/it]\u001b[A\n",
            " 96% 301/312 [04:27<00:11,  1.04s/it]\u001b[A\n",
            " 97% 302/312 [04:28<00:09,  1.06it/s]\u001b[A\n",
            " 97% 303/312 [04:29<00:08,  1.05it/s]\u001b[A\n",
            " 97% 304/312 [04:30<00:07,  1.12it/s]\u001b[A\n",
            " 98% 305/312 [04:31<00:06,  1.16it/s]\u001b[A\n",
            " 98% 306/312 [04:32<00:05,  1.12it/s]\u001b[A\n",
            " 98% 307/312 [04:32<00:04,  1.20it/s]\u001b[A\n",
            " 99% 308/312 [04:33<00:03,  1.15it/s]\u001b[A\n",
            " 99% 309/312 [04:34<00:02,  1.23it/s]\u001b[A\n",
            " 99% 310/312 [04:35<00:01,  1.29it/s]\u001b[A\n",
            "100% 311/312 [04:35<00:00,  1.33it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.1629830598831177, 'eval_runtime': 277.7556, 'eval_samples_per_second': 17.969, 'eval_steps_per_second': 1.123, 'epoch': 0.8}\n",
            " 80% 408/509 [2:19:18<30:45, 18.27s/it]\n",
            "100% 312/312 [04:36<00:00,  1.36it/s]\u001b[A\n",
            "{'loss': 1.2525, 'learning_rate': 4.016227180527384e-05, 'epoch': 0.8}\n",
            "{'loss': 1.1482, 'learning_rate': 3.6105476673428e-05, 'epoch': 0.82}\n",
            "{'loss': 1.0874, 'learning_rate': 3.204868154158215e-05, 'epoch': 0.84}\n",
            "{'loss': 1.0847, 'learning_rate': 2.7991886409736312e-05, 'epoch': 0.86}\n",
            "{'loss': 1.1766, 'learning_rate': 2.393509127789047e-05, 'epoch': 0.88}\n",
            " 90% 459/509 [2:32:05<14:50, 17.81s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/312 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 2/312 [00:00<02:13,  2.32it/s]\u001b[A\n",
            "  1% 3/312 [00:01<03:09,  1.63it/s]\u001b[A\n",
            "  1% 4/312 [00:02<04:03,  1.26it/s]\u001b[A\n",
            "  2% 5/312 [00:03<04:23,  1.16it/s]\u001b[A\n",
            "  2% 6/312 [00:05<06:36,  1.30s/it]\u001b[A\n",
            "  2% 7/312 [00:06<06:07,  1.21s/it]\u001b[A\n",
            "  3% 8/312 [00:07<05:42,  1.13s/it]\u001b[A\n",
            "  3% 9/312 [00:08<05:04,  1.00s/it]\u001b[A\n",
            "  3% 10/312 [00:09<05:07,  1.02s/it]\u001b[A\n",
            "  4% 11/312 [00:10<04:56,  1.02it/s]\u001b[A\n",
            "  4% 12/312 [00:11<04:51,  1.03it/s]\u001b[A\n",
            "  4% 13/312 [00:12<04:36,  1.08it/s]\u001b[A\n",
            "  4% 14/312 [00:13<04:12,  1.18it/s]\u001b[A\n",
            "  5% 15/312 [00:14<04:29,  1.10it/s]\u001b[A\n",
            "  5% 16/312 [00:14<04:10,  1.18it/s]\u001b[A\n",
            "  5% 17/312 [00:15<03:56,  1.25it/s]\u001b[A\n",
            "  6% 18/312 [00:16<03:51,  1.27it/s]\u001b[A\n",
            "  6% 19/312 [00:17<03:56,  1.24it/s]\u001b[A\n",
            "  6% 20/312 [00:17<03:48,  1.28it/s]\u001b[A\n",
            "  7% 21/312 [00:18<03:40,  1.32it/s]\u001b[A\n",
            "  7% 22/312 [00:19<04:07,  1.17it/s]\u001b[A\n",
            "  7% 23/312 [00:20<03:54,  1.23it/s]\u001b[A\n",
            "  8% 24/312 [00:21<03:50,  1.25it/s]\u001b[A\n",
            "  8% 25/312 [00:21<03:55,  1.22it/s]\u001b[A\n",
            "  8% 26/312 [00:22<03:58,  1.20it/s]\u001b[A\n",
            "  9% 27/312 [00:23<03:52,  1.22it/s]\u001b[A\n",
            "  9% 28/312 [00:25<05:01,  1.06s/it]\u001b[A\n",
            "  9% 29/312 [00:26<04:49,  1.02s/it]\u001b[A\n",
            " 10% 30/312 [00:26<04:23,  1.07it/s]\u001b[A\n",
            " 10% 31/312 [00:27<04:25,  1.06it/s]\u001b[A\n",
            " 10% 32/312 [00:29<04:44,  1.02s/it]\u001b[A\n",
            " 11% 33/312 [00:30<05:02,  1.08s/it]\u001b[A\n",
            " 11% 34/312 [00:31<04:34,  1.01it/s]\u001b[A\n",
            " 11% 35/312 [00:32<04:33,  1.01it/s]\u001b[A\n",
            " 12% 36/312 [00:32<04:25,  1.04it/s]\u001b[A\n",
            " 12% 37/312 [00:33<03:55,  1.17it/s]\u001b[A\n",
            " 12% 38/312 [00:34<03:54,  1.17it/s]\u001b[A\n",
            " 12% 39/312 [00:35<03:57,  1.15it/s]\u001b[A\n",
            " 13% 40/312 [00:36<04:05,  1.11it/s]\u001b[A\n",
            " 13% 41/312 [00:37<03:50,  1.18it/s]\u001b[A\n",
            " 13% 42/312 [00:37<03:34,  1.26it/s]\u001b[A\n",
            " 14% 43/312 [00:38<03:49,  1.17it/s]\u001b[A\n",
            " 14% 44/312 [00:39<04:24,  1.01it/s]\u001b[A\n",
            " 14% 45/312 [00:40<04:10,  1.06it/s]\u001b[A\n",
            " 15% 46/312 [00:41<03:57,  1.12it/s]\u001b[A\n",
            " 15% 47/312 [00:42<03:47,  1.17it/s]\u001b[A\n",
            " 15% 48/312 [00:43<03:40,  1.20it/s]\u001b[A\n",
            " 16% 49/312 [00:43<03:37,  1.21it/s]\u001b[A\n",
            " 16% 50/312 [00:44<03:32,  1.23it/s]\u001b[A\n",
            " 16% 51/312 [00:45<03:55,  1.11it/s]\u001b[A\n",
            " 17% 52/312 [00:46<03:37,  1.20it/s]\u001b[A\n",
            " 17% 53/312 [00:47<03:57,  1.09it/s]\u001b[A\n",
            " 17% 54/312 [00:48<03:53,  1.10it/s]\u001b[A\n",
            " 18% 55/312 [00:49<03:34,  1.20it/s]\u001b[A\n",
            " 18% 56/312 [00:50<03:36,  1.18it/s]\u001b[A\n",
            " 18% 57/312 [00:50<03:25,  1.24it/s]\u001b[A\n",
            " 19% 58/312 [00:51<03:16,  1.29it/s]\u001b[A\n",
            " 19% 59/312 [00:52<03:19,  1.27it/s]\u001b[A\n",
            " 19% 60/312 [00:53<03:25,  1.23it/s]\u001b[A\n",
            " 20% 61/312 [00:53<03:21,  1.25it/s]\u001b[A\n",
            " 20% 62/312 [00:55<04:21,  1.05s/it]\u001b[A\n",
            " 20% 63/312 [00:56<04:13,  1.02s/it]\u001b[A\n",
            " 21% 64/312 [00:57<03:54,  1.06it/s]\u001b[A\n",
            " 21% 65/312 [00:58<03:52,  1.06it/s]\u001b[A\n",
            " 21% 66/312 [00:58<03:30,  1.17it/s]\u001b[A\n",
            " 21% 67/312 [00:59<03:18,  1.24it/s]\u001b[A\n",
            " 22% 68/312 [01:00<03:09,  1.29it/s]\u001b[A\n",
            " 22% 69/312 [01:00<03:03,  1.33it/s]\u001b[A\n",
            " 22% 70/312 [01:01<02:56,  1.37it/s]\u001b[A\n",
            " 23% 71/312 [01:02<03:01,  1.32it/s]\u001b[A\n",
            " 23% 72/312 [01:03<03:11,  1.26it/s]\u001b[A\n",
            " 23% 73/312 [01:04<03:07,  1.27it/s]\u001b[A\n",
            " 24% 74/312 [01:05<03:23,  1.17it/s]\u001b[A\n",
            " 24% 75/312 [01:05<03:11,  1.24it/s]\u001b[A\n",
            " 24% 76/312 [01:06<03:03,  1.28it/s]\u001b[A\n",
            " 25% 77/312 [01:07<03:02,  1.28it/s]\u001b[A\n",
            " 25% 78/312 [01:08<03:19,  1.17it/s]\u001b[A\n",
            " 25% 79/312 [01:09<03:16,  1.19it/s]\u001b[A\n",
            " 26% 80/312 [01:09<03:06,  1.24it/s]\u001b[A\n",
            " 26% 81/312 [01:10<03:02,  1.27it/s]\u001b[A\n",
            " 26% 82/312 [01:11<03:08,  1.22it/s]\u001b[A\n",
            " 27% 83/312 [01:12<03:07,  1.22it/s]\u001b[A\n",
            " 27% 84/312 [01:13<03:11,  1.19it/s]\u001b[A\n",
            " 27% 85/312 [01:13<03:03,  1.24it/s]\u001b[A\n",
            " 28% 86/312 [01:15<03:39,  1.03it/s]\u001b[A\n",
            " 28% 87/312 [01:16<03:32,  1.06it/s]\u001b[A\n",
            " 28% 88/312 [01:17<03:28,  1.07it/s]\u001b[A\n",
            " 29% 89/312 [01:17<03:20,  1.11it/s]\u001b[A\n",
            " 29% 90/312 [01:18<03:12,  1.15it/s]\u001b[A\n",
            " 29% 91/312 [01:19<02:58,  1.24it/s]\u001b[A\n",
            " 29% 92/312 [01:20<03:00,  1.22it/s]\u001b[A\n",
            " 30% 93/312 [01:21<02:59,  1.22it/s]\u001b[A\n",
            " 30% 94/312 [01:21<03:04,  1.18it/s]\u001b[A\n",
            " 30% 95/312 [01:22<02:47,  1.29it/s]\u001b[A\n",
            " 31% 96/312 [01:23<02:52,  1.25it/s]\u001b[A\n",
            " 31% 97/312 [01:24<02:39,  1.35it/s]\u001b[A\n",
            " 31% 98/312 [01:24<02:51,  1.25it/s]\u001b[A\n",
            " 32% 99/312 [01:26<03:21,  1.05it/s]\u001b[A\n",
            " 32% 100/312 [01:26<03:04,  1.15it/s]\u001b[A\n",
            " 32% 101/312 [01:27<02:46,  1.27it/s]\u001b[A\n",
            " 33% 102/312 [01:28<02:36,  1.34it/s]\u001b[A\n",
            " 33% 103/312 [01:28<02:33,  1.36it/s]\u001b[A\n",
            " 33% 104/312 [01:30<03:19,  1.04it/s]\u001b[A\n",
            " 34% 105/312 [01:31<03:01,  1.14it/s]\u001b[A\n",
            " 34% 106/312 [01:32<03:04,  1.12it/s]\u001b[A\n",
            " 34% 107/312 [01:32<02:54,  1.17it/s]\u001b[A\n",
            " 35% 108/312 [01:34<03:50,  1.13s/it]\u001b[A\n",
            " 35% 109/312 [01:35<03:30,  1.04s/it]\u001b[A\n",
            " 35% 110/312 [01:36<03:16,  1.03it/s]\u001b[A\n",
            " 36% 111/312 [01:36<02:59,  1.12it/s]\u001b[A\n",
            " 36% 112/312 [01:37<02:43,  1.22it/s]\u001b[A\n",
            " 36% 113/312 [01:38<02:44,  1.21it/s]\u001b[A\n",
            " 37% 114/312 [01:39<02:55,  1.13it/s]\u001b[A\n",
            " 37% 115/312 [01:40<02:50,  1.16it/s]\u001b[A\n",
            " 37% 116/312 [01:41<02:58,  1.10it/s]\u001b[A\n",
            " 38% 117/312 [01:41<02:46,  1.17it/s]\u001b[A\n",
            " 38% 118/312 [01:42<02:40,  1.21it/s]\u001b[A\n",
            " 38% 119/312 [01:43<02:35,  1.24it/s]\u001b[A\n",
            " 38% 120/312 [01:44<02:26,  1.31it/s]\u001b[A\n",
            " 39% 121/312 [01:45<02:50,  1.12it/s]\u001b[A\n",
            " 39% 122/312 [01:46<02:50,  1.12it/s]\u001b[A\n",
            " 39% 123/312 [01:46<02:35,  1.22it/s]\u001b[A\n",
            " 40% 124/312 [01:47<02:32,  1.23it/s]\u001b[A\n",
            " 40% 125/312 [01:50<03:58,  1.28s/it]\u001b[A\n",
            " 40% 126/312 [01:50<03:32,  1.14s/it]\u001b[A\n",
            " 41% 127/312 [01:51<03:16,  1.06s/it]\u001b[A\n",
            " 41% 128/312 [01:52<02:55,  1.05it/s]\u001b[A\n",
            " 41% 129/312 [01:53<02:46,  1.10it/s]\u001b[A\n",
            " 42% 130/312 [01:54<02:40,  1.14it/s]\u001b[A\n",
            " 42% 131/312 [01:54<02:28,  1.22it/s]\u001b[A\n",
            " 42% 132/312 [01:55<02:25,  1.24it/s]\u001b[A\n",
            " 43% 133/312 [01:56<02:34,  1.16it/s]\u001b[A\n",
            " 43% 134/312 [01:57<02:31,  1.18it/s]\u001b[A\n",
            " 43% 135/312 [01:57<02:20,  1.26it/s]\u001b[A\n",
            " 44% 136/312 [01:58<02:12,  1.33it/s]\u001b[A\n",
            " 44% 137/312 [01:59<02:19,  1.25it/s]\u001b[A\n",
            " 44% 138/312 [02:00<02:23,  1.21it/s]\u001b[A\n",
            " 45% 139/312 [02:01<02:21,  1.22it/s]\u001b[A\n",
            " 45% 140/312 [02:02<02:20,  1.22it/s]\u001b[A\n",
            " 45% 141/312 [02:03<02:39,  1.07it/s]\u001b[A\n",
            " 46% 142/312 [02:04<03:13,  1.14s/it]\u001b[A\n",
            " 46% 143/312 [02:05<02:54,  1.03s/it]\u001b[A\n",
            " 46% 144/312 [02:06<02:39,  1.05it/s]\u001b[A\n",
            " 46% 145/312 [02:07<02:30,  1.11it/s]\u001b[A\n",
            " 47% 146/312 [02:08<02:24,  1.15it/s]\u001b[A\n",
            " 47% 147/312 [02:08<02:23,  1.15it/s]\u001b[A\n",
            " 47% 148/312 [02:09<02:24,  1.13it/s]\u001b[A\n",
            " 48% 149/312 [02:10<02:14,  1.21it/s]\u001b[A\n",
            " 48% 150/312 [02:11<02:07,  1.27it/s]\u001b[A\n",
            " 48% 151/312 [02:12<02:19,  1.15it/s]\u001b[A\n",
            " 49% 152/312 [02:14<03:06,  1.17s/it]\u001b[A\n",
            " 49% 153/312 [02:14<02:42,  1.03s/it]\u001b[A\n",
            " 49% 154/312 [02:16<03:06,  1.18s/it]\u001b[A\n",
            " 50% 155/312 [02:17<03:12,  1.23s/it]\u001b[A\n",
            " 50% 156/312 [02:18<03:15,  1.26s/it]\u001b[A\n",
            " 50% 157/312 [02:20<03:14,  1.26s/it]\u001b[A\n",
            " 51% 158/312 [02:21<02:56,  1.15s/it]\u001b[A\n",
            " 51% 159/312 [02:22<02:54,  1.14s/it]\u001b[A\n",
            " 51% 160/312 [02:23<02:37,  1.04s/it]\u001b[A\n",
            " 52% 161/312 [02:23<02:24,  1.05it/s]\u001b[A\n",
            " 52% 162/312 [02:24<02:14,  1.12it/s]\u001b[A\n",
            " 52% 163/312 [02:25<02:10,  1.14it/s]\u001b[A\n",
            " 53% 164/312 [02:26<02:12,  1.12it/s]\u001b[A\n",
            " 53% 165/312 [02:27<02:30,  1.02s/it]\u001b[A\n",
            " 53% 166/312 [02:29<03:00,  1.24s/it]\u001b[A\n",
            " 54% 167/312 [02:30<02:37,  1.08s/it]\u001b[A\n",
            " 54% 168/312 [02:30<02:26,  1.02s/it]\u001b[A\n",
            " 54% 169/312 [02:31<02:14,  1.06it/s]\u001b[A\n",
            " 54% 170/312 [02:32<02:04,  1.14it/s]\u001b[A\n",
            " 55% 171/312 [02:33<01:56,  1.22it/s]\u001b[A\n",
            " 55% 172/312 [02:34<01:55,  1.22it/s]\u001b[A\n",
            " 55% 173/312 [02:35<02:15,  1.03it/s]\u001b[A\n",
            " 56% 174/312 [02:36<02:11,  1.05it/s]\u001b[A\n",
            " 56% 175/312 [02:36<02:00,  1.13it/s]\u001b[A\n",
            " 56% 176/312 [02:37<01:53,  1.20it/s]\u001b[A\n",
            " 57% 177/312 [02:38<01:50,  1.22it/s]\u001b[A\n",
            " 57% 178/312 [02:39<01:49,  1.23it/s]\u001b[A\n",
            " 57% 179/312 [02:40<01:46,  1.25it/s]\u001b[A\n",
            " 58% 180/312 [02:40<01:41,  1.31it/s]\u001b[A\n",
            " 58% 181/312 [02:41<01:37,  1.35it/s]\u001b[A\n",
            " 58% 182/312 [02:42<01:39,  1.31it/s]\u001b[A\n",
            " 59% 183/312 [02:43<01:52,  1.15it/s]\u001b[A\n",
            " 59% 184/312 [02:44<02:01,  1.05it/s]\u001b[A\n",
            " 59% 185/312 [02:45<01:55,  1.10it/s]\u001b[A\n",
            " 60% 186/312 [02:46<01:58,  1.07it/s]\u001b[A\n",
            " 60% 187/312 [02:47<01:50,  1.13it/s]\u001b[A\n",
            " 60% 188/312 [02:47<01:41,  1.22it/s]\u001b[A\n",
            " 61% 189/312 [02:48<01:36,  1.28it/s]\u001b[A\n",
            " 61% 190/312 [02:49<01:35,  1.28it/s]\u001b[A\n",
            " 61% 191/312 [02:50<01:35,  1.26it/s]\u001b[A\n",
            " 62% 192/312 [02:50<01:35,  1.26it/s]\u001b[A\n",
            " 62% 193/312 [02:51<01:31,  1.31it/s]\u001b[A\n",
            " 62% 194/312 [02:52<01:32,  1.28it/s]\u001b[A\n",
            " 62% 195/312 [02:53<01:29,  1.31it/s]\u001b[A\n",
            " 63% 196/312 [02:53<01:25,  1.36it/s]\u001b[A\n",
            " 63% 197/312 [02:54<01:23,  1.38it/s]\u001b[A\n",
            " 63% 198/312 [02:55<01:21,  1.40it/s]\u001b[A\n",
            " 64% 199/312 [02:55<01:19,  1.41it/s]\u001b[A\n",
            " 64% 200/312 [02:56<01:24,  1.32it/s]\u001b[A\n",
            " 64% 201/312 [02:57<01:27,  1.27it/s]\u001b[A\n",
            " 65% 202/312 [02:58<01:26,  1.27it/s]\u001b[A\n",
            " 65% 203/312 [02:59<01:29,  1.22it/s]\u001b[A\n",
            " 65% 204/312 [02:59<01:25,  1.27it/s]\u001b[A\n",
            " 66% 205/312 [03:00<01:20,  1.34it/s]\u001b[A\n",
            " 66% 206/312 [03:01<01:17,  1.37it/s]\u001b[A\n",
            " 66% 207/312 [03:02<01:20,  1.30it/s]\u001b[A\n",
            " 67% 208/312 [03:03<01:30,  1.15it/s]\u001b[A\n",
            " 67% 209/312 [03:03<01:22,  1.25it/s]\u001b[A\n",
            " 67% 210/312 [03:04<01:20,  1.26it/s]\u001b[A\n",
            " 68% 211/312 [03:05<01:35,  1.06it/s]\u001b[A\n",
            " 68% 212/312 [03:06<01:30,  1.11it/s]\u001b[A\n",
            " 68% 213/312 [03:07<01:30,  1.09it/s]\u001b[A\n",
            " 69% 214/312 [03:08<01:25,  1.15it/s]\u001b[A\n",
            " 69% 215/312 [03:09<01:24,  1.15it/s]\u001b[A\n",
            " 69% 216/312 [03:10<01:29,  1.08it/s]\u001b[A\n",
            " 70% 217/312 [03:11<01:23,  1.14it/s]\u001b[A\n",
            " 70% 218/312 [03:12<01:22,  1.14it/s]\u001b[A\n",
            " 70% 219/312 [03:13<01:37,  1.04s/it]\u001b[A\n",
            " 71% 220/312 [03:14<01:43,  1.12s/it]\u001b[A\n",
            " 71% 221/312 [03:15<01:37,  1.07s/it]\u001b[A\n",
            " 71% 222/312 [03:16<01:27,  1.03it/s]\u001b[A\n",
            " 71% 223/312 [03:17<01:23,  1.07it/s]\u001b[A\n",
            " 72% 224/312 [03:17<01:13,  1.20it/s]\u001b[A\n",
            " 72% 225/312 [03:18<01:17,  1.12it/s]\u001b[A\n",
            " 72% 226/312 [03:19<01:13,  1.17it/s]\u001b[A\n",
            " 73% 227/312 [03:20<01:16,  1.11it/s]\u001b[A\n",
            " 73% 228/312 [03:21<01:13,  1.15it/s]\u001b[A\n",
            " 73% 229/312 [03:22<01:07,  1.23it/s]\u001b[A\n",
            " 74% 230/312 [03:22<01:04,  1.27it/s]\u001b[A\n",
            " 74% 231/312 [03:23<01:05,  1.24it/s]\u001b[A\n",
            " 74% 232/312 [03:24<00:59,  1.34it/s]\u001b[A\n",
            " 75% 233/312 [03:25<01:10,  1.12it/s]\u001b[A\n",
            " 75% 234/312 [03:26<01:04,  1.21it/s]\u001b[A\n",
            " 75% 235/312 [03:27<01:05,  1.18it/s]\u001b[A\n",
            " 76% 236/312 [03:27<01:00,  1.25it/s]\u001b[A\n",
            " 76% 237/312 [03:28<01:00,  1.25it/s]\u001b[A\n",
            " 76% 238/312 [03:29<00:56,  1.31it/s]\u001b[A\n",
            " 77% 239/312 [03:30<00:57,  1.26it/s]\u001b[A\n",
            " 77% 240/312 [03:30<00:56,  1.27it/s]\u001b[A\n",
            " 77% 241/312 [03:31<00:57,  1.24it/s]\u001b[A\n",
            " 78% 242/312 [03:33<01:04,  1.09it/s]\u001b[A\n",
            " 78% 243/312 [03:34<01:22,  1.20s/it]\u001b[A\n",
            " 78% 244/312 [03:35<01:11,  1.05s/it]\u001b[A\n",
            " 79% 245/312 [03:36<01:08,  1.02s/it]\u001b[A\n",
            " 79% 246/312 [03:37<01:07,  1.02s/it]\u001b[A\n",
            " 79% 247/312 [03:38<01:00,  1.08it/s]\u001b[A\n",
            " 79% 248/312 [03:39<00:56,  1.14it/s]\u001b[A\n",
            " 80% 249/312 [03:39<00:53,  1.18it/s]\u001b[A\n",
            " 80% 250/312 [03:40<00:56,  1.09it/s]\u001b[A\n",
            " 80% 251/312 [03:41<00:53,  1.15it/s]\u001b[A\n",
            " 81% 252/312 [03:42<00:51,  1.17it/s]\u001b[A\n",
            " 81% 253/312 [03:43<00:50,  1.16it/s]\u001b[A\n",
            " 81% 254/312 [03:44<00:50,  1.16it/s]\u001b[A\n",
            " 82% 255/312 [03:45<00:49,  1.16it/s]\u001b[A\n",
            " 82% 256/312 [03:45<00:46,  1.22it/s]\u001b[A\n",
            " 82% 257/312 [03:46<00:49,  1.10it/s]\u001b[A\n",
            " 83% 258/312 [03:47<00:46,  1.17it/s]\u001b[A\n",
            " 83% 259/312 [03:48<00:47,  1.11it/s]\u001b[A\n",
            " 83% 260/312 [03:49<00:43,  1.21it/s]\u001b[A\n",
            " 84% 261/312 [03:50<00:41,  1.24it/s]\u001b[A\n",
            " 84% 262/312 [03:50<00:41,  1.22it/s]\u001b[A\n",
            " 84% 263/312 [03:51<00:39,  1.23it/s]\u001b[A\n",
            " 85% 264/312 [03:53<00:59,  1.24s/it]\u001b[A\n",
            " 85% 265/312 [03:55<01:05,  1.40s/it]\u001b[A\n",
            " 85% 266/312 [03:56<00:56,  1.23s/it]\u001b[A\n",
            " 86% 267/312 [03:57<00:52,  1.17s/it]\u001b[A\n",
            " 86% 268/312 [03:58<00:46,  1.05s/it]\u001b[A\n",
            " 86% 269/312 [03:59<00:43,  1.01s/it]\u001b[A\n",
            " 87% 270/312 [04:00<00:39,  1.07it/s]\u001b[A\n",
            " 87% 271/312 [04:00<00:35,  1.16it/s]\u001b[A\n",
            " 87% 272/312 [04:01<00:33,  1.20it/s]\u001b[A\n",
            " 88% 273/312 [04:02<00:36,  1.08it/s]\u001b[A\n",
            " 88% 274/312 [04:03<00:33,  1.12it/s]\u001b[A\n",
            " 88% 275/312 [04:04<00:30,  1.20it/s]\u001b[A\n",
            " 88% 276/312 [04:04<00:28,  1.27it/s]\u001b[A\n",
            " 89% 277/312 [04:05<00:31,  1.13it/s]\u001b[A\n",
            " 89% 278/312 [04:06<00:30,  1.12it/s]\u001b[A\n",
            " 89% 279/312 [04:07<00:29,  1.12it/s]\u001b[A\n",
            " 90% 280/312 [04:08<00:28,  1.13it/s]\u001b[A\n",
            " 90% 281/312 [04:09<00:25,  1.22it/s]\u001b[A\n",
            " 90% 282/312 [04:10<00:23,  1.26it/s]\u001b[A\n",
            " 91% 283/312 [04:11<00:25,  1.14it/s]\u001b[A\n",
            " 91% 284/312 [04:11<00:23,  1.18it/s]\u001b[A\n",
            " 91% 285/312 [04:12<00:22,  1.20it/s]\u001b[A\n",
            " 92% 286/312 [04:13<00:21,  1.19it/s]\u001b[A\n",
            " 92% 287/312 [04:14<00:21,  1.17it/s]\u001b[A\n",
            " 92% 288/312 [04:15<00:19,  1.23it/s]\u001b[A\n",
            " 93% 289/312 [04:15<00:18,  1.27it/s]\u001b[A\n",
            " 93% 290/312 [04:16<00:17,  1.26it/s]\u001b[A\n",
            " 93% 291/312 [04:17<00:16,  1.26it/s]\u001b[A\n",
            " 94% 292/312 [04:18<00:17,  1.12it/s]\u001b[A\n",
            " 94% 293/312 [04:19<00:17,  1.07it/s]\u001b[A\n",
            " 94% 294/312 [04:20<00:15,  1.16it/s]\u001b[A\n",
            " 95% 295/312 [04:21<00:13,  1.22it/s]\u001b[A\n",
            " 95% 296/312 [04:21<00:13,  1.23it/s]\u001b[A\n",
            " 95% 297/312 [04:22<00:12,  1.17it/s]\u001b[A\n",
            " 96% 298/312 [04:23<00:11,  1.19it/s]\u001b[A\n",
            " 96% 299/312 [04:25<00:13,  1.02s/it]\u001b[A\n",
            " 96% 300/312 [04:26<00:12,  1.06s/it]\u001b[A\n",
            " 96% 301/312 [04:27<00:11,  1.04s/it]\u001b[A\n",
            " 97% 302/312 [04:27<00:09,  1.06it/s]\u001b[A\n",
            " 97% 303/312 [04:28<00:08,  1.06it/s]\u001b[A\n",
            " 97% 304/312 [04:29<00:07,  1.12it/s]\u001b[A\n",
            " 98% 305/312 [04:30<00:05,  1.17it/s]\u001b[A\n",
            " 98% 306/312 [04:31<00:05,  1.13it/s]\u001b[A\n",
            " 98% 307/312 [04:32<00:04,  1.21it/s]\u001b[A\n",
            " 99% 308/312 [04:32<00:03,  1.15it/s]\u001b[A\n",
            " 99% 309/312 [04:33<00:02,  1.24it/s]\u001b[A\n",
            " 99% 310/312 [04:34<00:01,  1.29it/s]\u001b[A\n",
            "100% 311/312 [04:35<00:00,  1.33it/s]\u001b[A\n",
            "100% 312/312 [04:35<00:00,  1.36it/s]\u001b[A\n",
            "{'eval_loss': 1.161374807357788, 'eval_runtime': 277.0166, 'eval_samples_per_second': 18.017, 'eval_steps_per_second': 1.126, 'epoch': 0.9}\n",
            "\n",
            " 90% 459/509 [2:36:42<14:50, 17.81s/it]\n",
            "{'loss': 1.2677, 'learning_rate': 1.9878296146044626e-05, 'epoch': 0.9}\n",
            "{'loss': 1.1533, 'learning_rate': 1.5821501014198783e-05, 'epoch': 0.92}\n",
            "{'loss': 1.0981, 'learning_rate': 1.1764705882352942e-05, 'epoch': 0.94}\n",
            "{'loss': 1.0879, 'learning_rate': 7.707910750507099e-06, 'epoch': 0.96}\n",
            "{'loss': 1.2049, 'learning_rate': 3.6511156186612576e-06, 'epoch': 0.98}\n",
            "{'train_runtime': 10100.7315, 'train_samples_per_second': 6.46, 'train_steps_per_second': 0.05, 'train_loss': 1.1655611008230033, 'epoch': 1.0}\n",
            "100% 509/509 [2:48:20<00:00, 19.84s/it]\n",
            "repo is Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "100% 312/312 [04:36<00:00,  1.13it/s]\n",
            "adapter_model.safetensors: 100% 1.00G/1.00G [01:32<00:00, 10.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-2-13b-hf\" # sharded weights\n",
        "model_name = model_id.split('/')[-1]\n",
        "\n",
        "dataset_path = './data/ds_eli5_1024'\n",
        "ds_name = dataset_path.split('/')[-1]\n",
        "\n",
        "now = datetime.now()\n",
        "time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "\n",
        "#model_name = model_id.replace('/','-')\n",
        "#ds_name = dataset_path.split('/')[-1].replace('llama','combined_large').replace(':','-')\n",
        "\n",
        "output_dir = f'./{model_name}_{ds_name}/models'\n",
        "logging_dir = f'{output_dir}/logs'\n",
        "\n",
        "run_name = f'{model_name}_{ds_name}_{time_stamp}'\n",
        "optim = 'paged_adamw_8bit'\n",
        "\n",
        "from pathlib import Path\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "Path(logging_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "repo_id = f'{model_name}-{ds_name}'\n",
        "\n",
        "!python ./run_clm.py \\\n",
        "--output_dir {output_dir} \\\n",
        "--logging_dir {logging_dir} \\\n",
        "--model_id {model_id} \\\n",
        "--dataset_path {dataset_path} \\\n",
        "--run_name {run_name} \\\n",
        "--repo_id {repo_id} \\\n",
        "--report_to_wandb 1 \\\n",
        "--epochs 1 \\\n",
        "--max_steps -1 \\\n",
        "--per_device_train_batch_size 16 \\\n",
        "--per_device_eval_batch_size 16 \\\n",
        "--gradient_accumulation_steps 8 \\\n",
        "--lr 2e-4 \\\n",
        "--entity 'ft-llmmm' \\\n",
        "--project_name 'SFT_training_dm' \\\n",
        "--hub_strategy 'every_save' \\\n",
        "--torch_compile 0 \\\n",
        "--gradient_checkpointing 1 \\\n",
        "--optim 'paged_adamw_8bit' \\\n",
        "--group_by_length 1 \\\n",
        "--hf_token {hf_token} \\\n",
        "--wandb_token {wandb_token} \\\n",
        "--use_flash_attention 1 \\\n",
        "--logging_steps 10 \\\n",
        "--auto_find_batch_size 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBTbBw3QrE-m",
        "outputId": "c3710a81-e156-490c-d1e1-e4dd4ac4a161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-06 00:19:42.628354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "args is Namespace(model_id='meta-llama/Llama-2-13b-hf', repo_id='Llama-2-13b-hf-ds_eli5_1024', hub_strategy='every_save', output_dir='./Llama-2-13b-hf_ds_eli5_1024/models', output_data_dir=None, dataset_path='./data/ds_eli5_1024', hf_token='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', report_to_wandb=1, wandb_token='93b4fb1b729b939f257d7db15130b3710cad2ebb', epochs=1, max_steps=-1, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=8, max_seq_length=4096, logging_steps=10, optim='paged_adamw_8bit', lr=0.0002, lora_r=64, lora_alpha=16, weight_decay=0.1, lora_dropout=0.1, load_in_4bit=1, load_in_8bit=0, use_peft=1, gradient_checkpointing=1, bf16=1, group_by_length=1, merge_weights=0, seed=42, warmup_ratio=0.03, project_name='SFT_training_dm', entity='ft-llmmm', run_name='ds_eli5_1024_09.06.23-00.19.39', load_best_model_at_end=1, use_sagemaker=1, torch_compile=0, use_flash_attention=1, resume_from_checkpoint=0, auto_find_batch_size=0)\n",
            "extra is ['--logging_dir', './Llama-2-13b-hf_ds_eli5_1024/models/logs']\n",
            "Logging into the Hugging Face Hub with token hf_dZJsCiE...\n",
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Namespace(model_id='meta-llama/Llama-2-13b-hf', repo_id='Llama-2-13b-hf-ds_eli5_1024', hub_strategy='every_save', output_dir='./Llama-2-13b-hf_ds_eli5_1024/models', output_data_dir=None, dataset_path='./data/ds_eli5_1024', hf_token='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', report_to_wandb=1, wandb_token='93b4fb1b729b939f257d7db15130b3710cad2ebb', epochs=1, max_steps=-1, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=8, max_seq_length=4096, logging_steps=10, optim='paged_adamw_8bit', lr=0.0002, lora_r=64, lora_alpha=16, weight_decay=0.1, lora_dropout=0.1, load_in_4bit=1, load_in_8bit=0, use_peft=1, gradient_checkpointing=1, bf16=1, group_by_length=1, merge_weights=0, seed=42, warmup_ratio=0.03, project_name='SFT_training_dm', entity='ft-llmmm', run_name='ds_eli5_1024_09.06.23-00.19.39', load_best_model_at_end=1, use_sagemaker=1, torch_compile=0, use_flash_attention=1, resume_from_checkpoint=0, auto_find_batch_size=0)\n",
            "using flash attention\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmeltzer\u001b[0m (\u001b[33mft-llmmm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230906_001948-8kdeeqom\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mds_eli5_1024_09.06.23-00.19.39_r_64_alpha_16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ft-llmmm/SFT_training_dm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ft-llmmm/SFT_training_dm/runs/8kdeeqom\u001b[0m\n",
            "loading from ./data/ds_eli5_1024\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:479: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100% 3/3 [00:08<00:00,  2.68s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Found 7 modules to quantize: ['gate_proj', 'down_proj', 'up_proj', 'v_proj', 'q_proj', 'k_proj', 'o_proj']\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:640: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Using pad_token, but it is not set yet.\n",
            "TORCH DTYPE IS torch.bfloat16\n",
            "  0% 0/322 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "{'loss': 2.0601, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
            "{'loss': 2.0612, 'learning_rate': 0.0001935897435897436, 'epoch': 0.06}\n",
            "{'loss': 2.0837, 'learning_rate': 0.0001871794871794872, 'epoch': 0.09}\n",
            " 10% 33/322 [14:05<1:11:30, 14.84s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.30it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.20s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.29s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:17,  1.41s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:22,  1.56s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:18,  1.55s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.67s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:12,  1.49s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:24,  1.77s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:17,  1.66s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.61s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:13,  1.62s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.69s/it]\u001b[A\n",
            " 28% 17/60 [00:26<01:22,  1.91s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.89s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:10,  1.72s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.88s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:02,  1.61s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.80s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:03,  1.72s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.51s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.67s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:06,  1.94s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:01,  1.88s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.93s/it]\u001b[A\n",
            " 48% 29/60 [00:48<00:55,  1.78s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.77s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:50,  1.76s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.76s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.90s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:49,  1.88s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.90s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.89s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:46,  2.00s/it]\u001b[A\n",
            " 63% 38/60 [01:04<00:40,  1.86s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.08s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:44,  2.24s/it]\u001b[A\n",
            " 68% 41/60 [01:12<00:42,  2.21s/it]\u001b[A\n",
            " 70% 42/60 [01:13<00:36,  2.01s/it]\u001b[A\n",
            " 72% 43/60 [01:16<00:35,  2.08s/it]\u001b[A\n",
            " 73% 44/60 [01:18<00:36,  2.26s/it]\u001b[A\n",
            " 75% 45/60 [01:19<00:28,  1.89s/it]\u001b[A\n",
            " 77% 46/60 [01:21<00:27,  1.93s/it]\u001b[A\n",
            " 78% 47/60 [01:22<00:22,  1.70s/it]\u001b[A\n",
            " 80% 48/60 [01:24<00:20,  1.74s/it]\u001b[A\n",
            " 82% 49/60 [01:26<00:20,  1.84s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.69s/it]\u001b[A\n",
            " 85% 51/60 [01:29<00:14,  1.65s/it]\u001b[A\n",
            " 87% 52/60 [01:31<00:14,  1.76s/it]\u001b[A\n",
            " 88% 53/60 [01:34<00:13,  1.92s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:11,  1.99s/it]\u001b[A\n",
            " 92% 55/60 [01:38<00:09,  1.97s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.05s/it]\u001b[A\n",
            " 95% 57/60 [01:41<00:05,  1.81s/it]\u001b[A\n",
            " 97% 58/60 [01:43<00:03,  1.91s/it]\u001b[A\n",
            " 98% 59/60 [01:44<00:01,  1.68s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.1337697505950928, 'eval_runtime': 107.0731, 'eval_samples_per_second': 8.854, 'eval_steps_per_second': 0.56, 'epoch': 0.1}\n",
            " 10% 33/322 [15:52<1:11:30, 14.84s/it]\n",
            "100% 60/60 [01:45<00:00,  1.35s/it]\u001b[A\n",
            "{'loss': 2.0984, 'learning_rate': 0.00018076923076923077, 'epoch': 0.12}\n",
            "{'loss': 2.1576, 'learning_rate': 0.00017435897435897436, 'epoch': 0.15}\n",
            "{'loss': 2.0369, 'learning_rate': 0.00016794871794871796, 'epoch': 0.19}\n",
            " 20% 66/322 [28:26<1:43:54, 24.36s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.31it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.19s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.29s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:17,  1.41s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:22,  1.56s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:18,  1.55s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.67s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:12,  1.49s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:24,  1.77s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:17,  1.65s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.61s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:12,  1.62s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.68s/it]\u001b[A\n",
            " 28% 17/60 [00:26<01:21,  1.90s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.88s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:10,  1.72s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.88s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:02,  1.61s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.80s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:03,  1.72s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.51s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.67s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:06,  1.94s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:01,  1.88s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.92s/it]\u001b[A\n",
            " 48% 29/60 [00:47<00:55,  1.78s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.77s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:50,  1.75s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.75s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.90s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:48,  1.88s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.90s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.89s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:45,  2.00s/it]\u001b[A\n",
            " 63% 38/60 [01:04<00:40,  1.85s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.07s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:44,  2.24s/it]\u001b[A\n",
            " 68% 41/60 [01:12<00:42,  2.21s/it]\u001b[A\n",
            " 70% 42/60 [01:13<00:36,  2.01s/it]\u001b[A\n",
            " 72% 43/60 [01:15<00:35,  2.08s/it]\u001b[A\n",
            " 73% 44/60 [01:18<00:36,  2.26s/it]\u001b[A\n",
            " 75% 45/60 [01:19<00:28,  1.89s/it]\u001b[A\n",
            " 77% 46/60 [01:21<00:27,  1.93s/it]\u001b[A\n",
            " 78% 47/60 [01:22<00:22,  1.69s/it]\u001b[A\n",
            " 80% 48/60 [01:24<00:20,  1.74s/it]\u001b[A\n",
            " 82% 49/60 [01:26<00:20,  1.84s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.69s/it]\u001b[A\n",
            " 85% 51/60 [01:29<00:14,  1.65s/it]\u001b[A\n",
            " 87% 52/60 [01:31<00:14,  1.75s/it]\u001b[A\n",
            " 88% 53/60 [01:33<00:13,  1.91s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:11,  1.99s/it]\u001b[A\n",
            " 92% 55/60 [01:37<00:09,  1.97s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.05s/it]\u001b[A\n",
            " 95% 57/60 [01:41<00:05,  1.81s/it]\u001b[A\n",
            " 97% 58/60 [01:43<00:03,  1.91s/it]\u001b[A\n",
            " 98% 59/60 [01:44<00:01,  1.68s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.129304885864258, 'eval_runtime': 106.9278, 'eval_samples_per_second': 8.866, 'eval_steps_per_second': 0.561, 'epoch': 0.2}\n",
            " 20% 66/322 [30:13<1:43:54, 24.36s/it]\n",
            "100% 60/60 [01:45<00:00,  1.35s/it]\u001b[A\n",
            "{'loss': 2.0547, 'learning_rate': 0.00016153846153846155, 'epoch': 0.22}\n",
            "{'loss': 2.0778, 'learning_rate': 0.00015512820512820515, 'epoch': 0.25}\n",
            "{'loss': 2.0893, 'learning_rate': 0.00014871794871794872, 'epoch': 0.28}\n",
            " 31% 99/322 [38:22<35:51,  9.65s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.31it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.19s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.29s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:17,  1.42s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:22,  1.56s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:18,  1.55s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.67s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:12,  1.49s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:24,  1.77s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:17,  1.65s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.61s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:12,  1.62s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.68s/it]\u001b[A\n",
            " 28% 17/60 [00:26<01:21,  1.90s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.88s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:10,  1.72s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.88s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:02,  1.61s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.80s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:03,  1.72s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.51s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.67s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:05,  1.94s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:01,  1.88s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.92s/it]\u001b[A\n",
            " 48% 29/60 [00:47<00:55,  1.78s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.77s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:50,  1.75s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.75s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.90s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:48,  1.88s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.90s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.89s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:45,  2.00s/it]\u001b[A\n",
            " 63% 38/60 [01:04<00:40,  1.85s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.07s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:44,  2.24s/it]\u001b[A\n",
            " 68% 41/60 [01:12<00:42,  2.21s/it]\u001b[A\n",
            " 70% 42/60 [01:13<00:36,  2.01s/it]\u001b[A\n",
            " 72% 43/60 [01:15<00:35,  2.08s/it]\u001b[A\n",
            " 73% 44/60 [01:18<00:36,  2.26s/it]\u001b[A\n",
            " 75% 45/60 [01:19<00:28,  1.89s/it]\u001b[A\n",
            " 77% 46/60 [01:21<00:27,  1.93s/it]\u001b[A\n",
            " 78% 47/60 [01:22<00:22,  1.69s/it]\u001b[A\n",
            " 80% 48/60 [01:24<00:20,  1.73s/it]\u001b[A\n",
            " 82% 49/60 [01:26<00:20,  1.84s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.69s/it]\u001b[A\n",
            " 85% 51/60 [01:29<00:14,  1.65s/it]\u001b[A\n",
            " 87% 52/60 [01:31<00:14,  1.75s/it]\u001b[A\n",
            " 88% 53/60 [01:33<00:13,  1.91s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:11,  1.99s/it]\u001b[A\n",
            " 92% 55/60 [01:37<00:09,  1.97s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.05s/it]\u001b[A\n",
            " 95% 57/60 [01:41<00:05,  1.81s/it]\u001b[A\n",
            " 97% 58/60 [01:43<00:03,  1.91s/it]\u001b[A\n",
            " 98% 59/60 [01:44<00:01,  1.68s/it]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 2.1323657035827637, 'eval_runtime': 106.9218, 'eval_samples_per_second': 8.866, 'eval_steps_per_second': 0.561, 'epoch': 0.31}\n",
            " 31% 99/322 [40:09<35:51,  9.65s/it]\n",
            "100% 60/60 [01:45<00:00,  1.35s/it]\u001b[A\n",
            "{'loss': 2.1286, 'learning_rate': 0.0001423076923076923, 'epoch': 0.31}\n",
            "{'loss': 2.0278, 'learning_rate': 0.0001358974358974359, 'epoch': 0.34}\n",
            "{'loss': 2.0501, 'learning_rate': 0.0001294871794871795, 'epoch': 0.37}\n",
            "{'loss': 2.0821, 'learning_rate': 0.0001230769230769231, 'epoch': 0.4}\n",
            " 41% 132/322 [54:28<48:12, 15.22s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.31it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.20s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.29s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:17,  1.41s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:22,  1.56s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:18,  1.54s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.67s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:12,  1.49s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:24,  1.77s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:17,  1.65s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.61s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:12,  1.62s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.68s/it]\u001b[A\n",
            " 28% 17/60 [00:26<01:21,  1.90s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.88s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:10,  1.72s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.88s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:02,  1.61s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.80s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:03,  1.72s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.51s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.67s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:06,  1.94s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:01,  1.88s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.92s/it]\u001b[A\n",
            " 48% 29/60 [00:47<00:55,  1.78s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.77s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:50,  1.75s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.76s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.90s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:48,  1.88s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.90s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.89s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:46,  2.00s/it]\u001b[A\n",
            " 63% 38/60 [01:04<00:40,  1.86s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.08s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:44,  2.24s/it]\u001b[A\n",
            " 68% 41/60 [01:12<00:42,  2.21s/it]\u001b[A\n",
            " 70% 42/60 [01:13<00:36,  2.01s/it]\u001b[A\n",
            " 72% 43/60 [01:15<00:35,  2.08s/it]\u001b[A\n",
            " 73% 44/60 [01:18<00:36,  2.26s/it]\u001b[A\n",
            " 75% 45/60 [01:19<00:28,  1.89s/it]\u001b[A\n",
            " 77% 46/60 [01:21<00:27,  1.93s/it]\u001b[A\n",
            " 78% 47/60 [01:22<00:22,  1.69s/it]\u001b[A\n",
            " 80% 48/60 [01:24<00:20,  1.73s/it]\u001b[A\n",
            " 82% 49/60 [01:26<00:20,  1.84s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.69s/it]\u001b[A\n",
            " 85% 51/60 [01:29<00:14,  1.65s/it]\u001b[A\n",
            " 87% 52/60 [01:31<00:14,  1.75s/it]\u001b[A\n",
            " 88% 53/60 [01:33<00:13,  1.91s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:11,  1.99s/it]\u001b[A\n",
            " 92% 55/60 [01:37<00:09,  1.97s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.05s/it]\u001b[A\n",
            " 95% 57/60 [01:41<00:05,  1.81s/it]\u001b[A\n",
            " 97% 58/60 [01:43<00:03,  1.91s/it]\u001b[A\n",
            " 98% 59/60 [01:44<00:01,  1.68s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 2.1230714321136475, 'eval_runtime': 106.9608, 'eval_samples_per_second': 8.863, 'eval_steps_per_second': 0.561, 'epoch': 0.41}\n",
            " 41% 132/322 [56:15<48:12, 15.22s/it]\n",
            "100% 60/60 [01:45<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 2.0964, 'learning_rate': 0.00011666666666666668, 'epoch': 0.43}\n",
            "{'loss': 2.1395, 'learning_rate': 0.00011025641025641027, 'epoch': 0.46}\n",
            "{'loss': 2.0298, 'learning_rate': 0.00010384615384615386, 'epoch': 0.5}\n",
            " 51% 165/322 [1:08:39<1:06:25, 25.38s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.31it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.20s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.29s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:17,  1.41s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:22,  1.56s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:18,  1.54s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.67s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:12,  1.48s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:24,  1.77s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:17,  1.65s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.61s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:12,  1.62s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.69s/it]\u001b[A\n",
            " 28% 17/60 [00:26<01:22,  1.91s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.89s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:10,  1.72s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.88s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:02,  1.61s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.80s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:03,  1.73s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.52s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.68s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:06,  1.95s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:02,  1.88s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.93s/it]\u001b[A\n",
            " 48% 29/60 [00:48<00:55,  1.79s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.77s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:50,  1.76s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.76s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.90s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:49,  1.89s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.91s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.90s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:46,  2.01s/it]\u001b[A\n",
            " 63% 38/60 [01:04<00:40,  1.86s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.09s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:45,  2.25s/it]\u001b[A\n",
            " 68% 41/60 [01:12<00:42,  2.22s/it]\u001b[A\n",
            " 70% 42/60 [01:13<00:36,  2.02s/it]\u001b[A\n",
            " 72% 43/60 [01:16<00:35,  2.09s/it]\u001b[A\n",
            " 73% 44/60 [01:18<00:36,  2.28s/it]\u001b[A\n",
            " 75% 45/60 [01:19<00:28,  1.90s/it]\u001b[A\n",
            " 77% 46/60 [01:21<00:27,  1.94s/it]\u001b[A\n",
            " 78% 47/60 [01:23<00:22,  1.70s/it]\u001b[A\n",
            " 80% 48/60 [01:24<00:20,  1.74s/it]\u001b[A\n",
            " 82% 49/60 [01:26<00:20,  1.85s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.69s/it]\u001b[A\n",
            " 85% 51/60 [01:29<00:14,  1.66s/it]\u001b[A\n",
            " 87% 52/60 [01:31<00:14,  1.76s/it]\u001b[A\n",
            " 88% 53/60 [01:34<00:13,  1.93s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:11,  2.00s/it]\u001b[A\n",
            " 92% 55/60 [01:38<00:09,  1.98s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.06s/it]\u001b[A\n",
            " 95% 57/60 [01:41<00:05,  1.82s/it]\u001b[A\n",
            " 97% 58/60 [01:43<00:03,  1.92s/it]\u001b[A\n",
            " 98% 59/60 [01:45<00:01,  1.69s/it]\u001b[A\n",
            "                                         \n",
            "\u001b[A{'eval_loss': 2.1224777698516846, 'eval_runtime': 107.2978, 'eval_samples_per_second': 8.835, 'eval_steps_per_second': 0.559, 'epoch': 0.51}\n",
            " 51% 165/322 [1:10:27<1:06:25, 25.38s/it]\n",
            "100% 60/60 [01:45<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 2.0533, 'learning_rate': 9.743589743589744e-05, 'epoch': 0.53}\n",
            "{'loss': 2.0522, 'learning_rate': 9.102564102564103e-05, 'epoch': 0.56}\n",
            "{'loss': 2.0817, 'learning_rate': 8.461538461538461e-05, 'epoch': 0.59}\n",
            " 61% 198/322 [1:18:52<20:49, 10.08s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.30it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.20s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.30s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:17,  1.42s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:22,  1.57s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:19,  1.55s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.67s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:12,  1.49s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:25,  1.77s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:17,  1.66s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.62s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:13,  1.63s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.69s/it]\u001b[A\n",
            " 28% 17/60 [00:27<01:22,  1.91s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.89s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:10,  1.73s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.89s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:02,  1.61s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.80s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:04,  1.73s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.52s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.68s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:06,  1.95s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:02,  1.88s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.93s/it]\u001b[A\n",
            " 48% 29/60 [00:48<00:55,  1.79s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.78s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:51,  1.76s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.77s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.91s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:49,  1.90s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.91s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.90s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:46,  2.01s/it]\u001b[A\n",
            " 63% 38/60 [01:05<00:41,  1.86s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.09s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:45,  2.25s/it]\u001b[A\n",
            " 68% 41/60 [01:12<00:42,  2.22s/it]\u001b[A\n",
            " 70% 42/60 [01:14<00:36,  2.02s/it]\u001b[A\n",
            " 72% 43/60 [01:16<00:35,  2.09s/it]\u001b[A\n",
            " 73% 44/60 [01:19<00:36,  2.27s/it]\u001b[A\n",
            " 75% 45/60 [01:20<00:28,  1.90s/it]\u001b[A\n",
            " 77% 46/60 [01:22<00:27,  1.94s/it]\u001b[A\n",
            " 78% 47/60 [01:23<00:22,  1.70s/it]\u001b[A\n",
            " 80% 48/60 [01:25<00:20,  1.74s/it]\u001b[A\n",
            " 82% 49/60 [01:27<00:20,  1.85s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.70s/it]\u001b[A\n",
            " 85% 51/60 [01:30<00:14,  1.66s/it]\u001b[A\n",
            " 87% 52/60 [01:32<00:14,  1.76s/it]\u001b[A\n",
            " 88% 53/60 [01:34<00:13,  1.93s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:11,  2.00s/it]\u001b[A\n",
            " 92% 55/60 [01:38<00:09,  1.98s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.06s/it]\u001b[A\n",
            " 95% 57/60 [01:41<00:05,  1.82s/it]\u001b[A\n",
            " 97% 58/60 [01:44<00:03,  1.92s/it]\u001b[A\n",
            " 98% 59/60 [01:45<00:01,  1.69s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.1231021881103516, 'eval_runtime': 107.4775, 'eval_samples_per_second': 8.82, 'eval_steps_per_second': 0.558, 'epoch': 0.61}\n",
            " 61% 198/322 [1:20:40<20:49, 10.08s/it]\n",
            "100% 60/60 [01:45<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 2.1432, 'learning_rate': 7.820512820512821e-05, 'epoch': 0.62}\n",
            "{'loss': 2.0054, 'learning_rate': 7.17948717948718e-05, 'epoch': 0.65}\n",
            "{'loss': 2.0361, 'learning_rate': 6.538461538461539e-05, 'epoch': 0.68}\n",
            "{'loss': 2.0617, 'learning_rate': 5.897435897435898e-05, 'epoch': 0.71}\n",
            " 72% 231/322 [1:34:43<23:37, 15.57s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.30it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.20s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.30s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:18,  1.42s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:23,  1.57s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:19,  1.55s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.68s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:13,  1.49s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:25,  1.78s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:18,  1.66s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.62s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:13,  1.63s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.70s/it]\u001b[A\n",
            " 28% 17/60 [00:27<01:22,  1.92s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.90s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:11,  1.73s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.89s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:03,  1.62s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.81s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:04,  1.74s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.52s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.68s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:06,  1.96s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:02,  1.89s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.94s/it]\u001b[A\n",
            " 48% 29/60 [00:48<00:55,  1.79s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.78s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:51,  1.77s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.77s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.91s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:49,  1.90s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.91s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.90s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:46,  2.01s/it]\u001b[A\n",
            " 63% 38/60 [01:05<00:41,  1.87s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.09s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:45,  2.26s/it]\u001b[A\n",
            " 70% 42/60 [01:14<00:36,  2.03s/it]\u001b[A\n",
            " 72% 43/60 [01:16<00:35,  2.09s/it]\u001b[A\n",
            " 73% 44/60 [01:19<00:36,  2.28s/it]\u001b[A\n",
            " 75% 45/60 [01:20<00:28,  1.90s/it]\u001b[A\n",
            " 77% 46/60 [01:22<00:27,  1.94s/it]\u001b[A\n",
            " 78% 47/60 [01:23<00:22,  1.71s/it]\u001b[A\n",
            " 80% 48/60 [01:25<00:20,  1.75s/it]\u001b[A\n",
            " 82% 49/60 [01:27<00:20,  1.85s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.70s/it]\u001b[A\n",
            " 85% 51/60 [01:30<00:14,  1.66s/it]\u001b[A\n",
            " 87% 52/60 [01:32<00:14,  1.76s/it]\u001b[A\n",
            " 88% 53/60 [01:34<00:13,  1.93s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:12,  2.00s/it]\u001b[A\n",
            " 92% 55/60 [01:38<00:09,  1.98s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.07s/it]\u001b[A\n",
            " 95% 57/60 [01:42<00:05,  1.82s/it]\u001b[A\n",
            " 97% 58/60 [01:44<00:03,  1.93s/it]\u001b[A\n",
            " 98% 59/60 [01:45<00:01,  1.69s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.120473861694336, 'eval_runtime': 107.6662, 'eval_samples_per_second': 8.805, 'eval_steps_per_second': 0.557, 'epoch': 0.72}\n",
            " 72% 231/322 [1:36:30<23:37, 15.57s/it]\n",
            "100% 60/60 [01:46<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 2.1282, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.77}\n",
            "{'loss': 2.0323, 'learning_rate': 3.974358974358974e-05, 'epoch': 0.81}\n",
            " 82% 264/322 [1:48:47<25:40, 26.57s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.30it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.20s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.30s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:17,  1.42s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:22,  1.56s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:19,  1.55s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.67s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:13,  1.49s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:25,  1.78s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:18,  1.66s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.62s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:13,  1.63s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.69s/it]\u001b[A\n",
            " 28% 17/60 [00:27<01:22,  1.92s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.90s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:10,  1.73s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.89s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:02,  1.61s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.81s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:04,  1.73s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.52s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.68s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:06,  1.95s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:02,  1.88s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.93s/it]\u001b[A\n",
            " 48% 29/60 [00:48<00:55,  1.79s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.78s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:51,  1.76s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.76s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.91s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:49,  1.90s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.91s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.90s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:46,  2.01s/it]\u001b[A\n",
            " 63% 38/60 [01:05<00:41,  1.86s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.09s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:45,  2.25s/it]\u001b[A\n",
            " 68% 41/60 [01:12<00:42,  2.22s/it]\u001b[A\n",
            " 70% 42/60 [01:14<00:36,  2.02s/it]\u001b[A\n",
            " 72% 43/60 [01:16<00:35,  2.09s/it]\u001b[A\n",
            " 73% 44/60 [01:19<00:36,  2.28s/it]\u001b[A\n",
            " 75% 45/60 [01:20<00:28,  1.90s/it]\u001b[A\n",
            " 77% 46/60 [01:22<00:27,  1.94s/it]\u001b[A\n",
            " 78% 47/60 [01:23<00:22,  1.70s/it]\u001b[A\n",
            " 80% 48/60 [01:25<00:20,  1.74s/it]\u001b[A\n",
            " 82% 49/60 [01:27<00:20,  1.85s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.70s/it]\u001b[A\n",
            " 85% 51/60 [01:30<00:14,  1.66s/it]\u001b[A\n",
            " 87% 52/60 [01:32<00:14,  1.76s/it]\u001b[A\n",
            " 88% 53/60 [01:34<00:13,  1.93s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:11,  2.00s/it]\u001b[A\n",
            " 92% 55/60 [01:38<00:09,  1.98s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.06s/it]\u001b[A\n",
            " 95% 57/60 [01:42<00:05,  1.82s/it]\u001b[A\n",
            " 97% 58/60 [01:44<00:03,  1.92s/it]\u001b[A\n",
            " 98% 59/60 [01:45<00:01,  1.69s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.1205925941467285, 'eval_runtime': 107.5228, 'eval_samples_per_second': 8.817, 'eval_steps_per_second': 0.558, 'epoch': 0.82}\n",
            " 82% 264/322 [1:50:34<25:40, 26.57s/it]\n",
            "100% 60/60 [01:45<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 2.0407, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.84}\n",
            "{'loss': 2.0445, 'learning_rate': 2.6923076923076923e-05, 'epoch': 0.87}\n",
            "{'loss': 2.0552, 'learning_rate': 2.0512820512820512e-05, 'epoch': 0.9}\n",
            " 92% 297/322 [1:59:11<04:20, 10.44s/it]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "  0% 0/60 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/60 [00:01<00:44,  1.30it/s]\u001b[A\n",
            "  5% 3/60 [00:03<01:08,  1.20s/it]\u001b[A\n",
            "  7% 4/60 [00:04<01:12,  1.30s/it]\u001b[A\n",
            "  8% 5/60 [00:06<01:17,  1.42s/it]\u001b[A\n",
            " 10% 6/60 [00:07<01:14,  1.38s/it]\u001b[A\n",
            " 12% 7/60 [00:09<01:22,  1.56s/it]\u001b[A\n",
            " 13% 8/60 [00:11<01:18,  1.51s/it]\u001b[A\n",
            " 15% 9/60 [00:12<01:19,  1.55s/it]\u001b[A\n",
            " 17% 10/60 [00:14<01:23,  1.68s/it]\u001b[A\n",
            " 18% 11/60 [00:15<01:13,  1.49s/it]\u001b[A\n",
            " 20% 12/60 [00:18<01:25,  1.78s/it]\u001b[A\n",
            " 22% 13/60 [00:19<01:18,  1.66s/it]\u001b[A\n",
            " 23% 14/60 [00:21<01:14,  1.62s/it]\u001b[A\n",
            " 25% 15/60 [00:22<01:13,  1.63s/it]\u001b[A\n",
            " 27% 16/60 [00:24<01:14,  1.69s/it]\u001b[A\n",
            " 28% 17/60 [00:27<01:22,  1.91s/it]\u001b[A\n",
            " 30% 18/60 [00:28<01:19,  1.90s/it]\u001b[A\n",
            " 32% 19/60 [00:30<01:10,  1.73s/it]\u001b[A\n",
            " 33% 20/60 [00:32<01:15,  1.89s/it]\u001b[A\n",
            " 35% 21/60 [00:33<01:02,  1.61s/it]\u001b[A\n",
            " 37% 22/60 [00:35<01:08,  1.81s/it]\u001b[A\n",
            " 38% 23/60 [00:37<01:04,  1.73s/it]\u001b[A\n",
            " 40% 24/60 [00:38<00:54,  1.52s/it]\u001b[A\n",
            " 42% 25/60 [00:40<00:58,  1.68s/it]\u001b[A\n",
            " 43% 26/60 [00:42<01:06,  1.95s/it]\u001b[A\n",
            " 45% 27/60 [00:44<01:02,  1.89s/it]\u001b[A\n",
            " 47% 28/60 [00:46<01:01,  1.93s/it]\u001b[A\n",
            " 48% 29/60 [00:48<00:55,  1.79s/it]\u001b[A\n",
            " 50% 30/60 [00:49<00:53,  1.78s/it]\u001b[A\n",
            " 52% 31/60 [00:51<00:51,  1.76s/it]\u001b[A\n",
            " 53% 32/60 [00:53<00:49,  1.76s/it]\u001b[A\n",
            " 55% 33/60 [00:55<00:51,  1.91s/it]\u001b[A\n",
            " 57% 34/60 [00:57<00:49,  1.89s/it]\u001b[A\n",
            " 58% 35/60 [00:59<00:47,  1.91s/it]\u001b[A\n",
            " 60% 36/60 [01:01<00:45,  1.90s/it]\u001b[A\n",
            " 62% 37/60 [01:03<00:46,  2.01s/it]\u001b[A\n",
            " 63% 38/60 [01:05<00:41,  1.87s/it]\u001b[A\n",
            " 65% 39/60 [01:07<00:43,  2.09s/it]\u001b[A\n",
            " 67% 40/60 [01:10<00:45,  2.25s/it]\u001b[A\n",
            " 68% 41/60 [01:12<00:42,  2.22s/it]\u001b[A\n",
            " 70% 42/60 [01:14<00:36,  2.02s/it]\u001b[A\n",
            " 72% 43/60 [01:16<00:35,  2.09s/it]\u001b[A\n",
            " 73% 44/60 [01:19<00:36,  2.28s/it]\u001b[A\n",
            " 75% 45/60 [01:20<00:28,  1.90s/it]\u001b[A\n",
            " 77% 46/60 [01:22<00:27,  1.94s/it]\u001b[A\n",
            " 78% 47/60 [01:23<00:22,  1.70s/it]\u001b[A\n",
            " 80% 48/60 [01:25<00:20,  1.74s/it]\u001b[A\n",
            " 82% 49/60 [01:27<00:20,  1.85s/it]\u001b[A\n",
            " 83% 50/60 [01:28<00:16,  1.69s/it]\u001b[A\n",
            " 85% 51/60 [01:30<00:14,  1.66s/it]\u001b[A\n",
            " 87% 52/60 [01:32<00:14,  1.76s/it]\u001b[A\n",
            " 88% 53/60 [01:34<00:13,  1.93s/it]\u001b[A\n",
            " 90% 54/60 [01:36<00:11,  2.00s/it]\u001b[A\n",
            " 92% 55/60 [01:38<00:09,  1.98s/it]\u001b[A\n",
            " 93% 56/60 [01:40<00:08,  2.06s/it]\u001b[A\n",
            " 95% 57/60 [01:42<00:05,  1.82s/it]\u001b[A\n",
            " 97% 58/60 [01:44<00:03,  1.92s/it]\u001b[A\n",
            " 98% 59/60 [01:45<00:01,  1.69s/it]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 2.119617462158203, 'eval_runtime': 107.5249, 'eval_samples_per_second': 8.817, 'eval_steps_per_second': 0.558, 'epoch': 0.92}\n",
            " 92% 297/322 [2:00:58<04:20, 10.44s/it]\n",
            "100% 60/60 [01:45<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 2.1338, 'learning_rate': 1.4102564102564104e-05, 'epoch': 0.93}\n",
            "{'loss': 2.0158, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.96}\n",
            "{'loss': 2.071, 'learning_rate': 1.282051282051282e-06, 'epoch': 0.99}\n",
            "{'train_runtime': 7768.7384, 'train_samples_per_second': 5.317, 'train_steps_per_second': 0.041, 'train_loss': 2.0728155752146464, 'epoch': 1.0}\n",
            "100% 322/322 [2:09:28<00:00, 24.13s/it]\n",
            "repo is Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "100% 60/60 [01:45<00:00,  1.76s/it]\n",
            "adapter_model.safetensors: 100% 1.00G/1.00G [01:42<00:00, 9.76MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "NJSXRw7nsE8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j31XAdZcO2Dc"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eye6_JRiBVAZ"
      },
      "source": [
        "### Computing Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jdCwgyHDxdOL"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "from peft import PeftModel\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "MeF1RPfNxfYS"
      },
      "outputs": [],
      "source": [
        "def inference_formatting(example):\n",
        "    return f\"### Human: {example}\\n ### Assistant:\"\n",
        "\n",
        "def generate_examples(model,\n",
        "                      tokenizer,\n",
        "                      data,\n",
        "                      padding=True):\n",
        "    generation_config = transformers.GenerationConfig(num_beams = 1,\n",
        "                                         max_new_tokens = 256,\n",
        "                                         do_sample = True,\n",
        "                                         temperature = .6,\n",
        "                                         top_p = 0.9,\n",
        "                                         repetition_penalty = 1.2,\n",
        "                                         #pad_token_id = model.config.eos_token_id\n",
        "                                        )\n",
        "\n",
        "    prompts = data['prompt']\n",
        "\n",
        "    #pipe = pipeline('text-generation',model,tokenizer=tokenizer)\n",
        "\n",
        "    #predictions = pipe(prompts,generation_config = generation_config)\n",
        "\n",
        "    input = tokenizer(prompts, return_tensors = 'pt', padding = padding).to('cuda')\n",
        "\n",
        "    output_ids = model.generate(input_ids = input['input_ids'],\n",
        "                                attention_mask = input['attention_mask'],\n",
        "                                generation_config = generation_config,\n",
        "                                pad_token_id = model.config.eos_token_id,\n",
        "                                )\n",
        "\n",
        "    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def generate_df_predictions(model_ids,\n",
        "                            ds,\n",
        "                            output_dir,\n",
        "                            batch_size=16,\n",
        "                            seed = 50,\n",
        "                            size = 100,\n",
        "                            padding=True):\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        #load_in_8bit=True,\n",
        "        #load_in_4bit=True,\n",
        "        #bnb_4bit_use_double_quant=True,\n",
        "        #bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    ds_small = {}\n",
        "\n",
        "    for base_model, model_id in model_ids:\n",
        "        print(f'working on model {model_id.split(\"/\")[-1]}')\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.padding_side = \"left\"\n",
        "        predictions = defaultdict(list)\n",
        "\n",
        "        if base_model:\n",
        "\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model,\n",
        "                device_map=\"auto\",\n",
        "                load_in_4bit = True,\n",
        "                quantization_config=bnb_config\n",
        "                )\n",
        "\n",
        "            model = PeftModel.from_pretrained(model = model,\n",
        "                            model_id = model_id,\n",
        "                            torch_dtype = torch.bfloat16,\n",
        "                            is_trainable = False)\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                device_map=\"auto\",\n",
        "                load_in_4bit = True,\n",
        "                quantization_config=bnb_config\n",
        "                )\n",
        "\n",
        "        model.eval()\n",
        "        model_name = model_id.split('/')[-1]\n",
        "\n",
        "        for ds_name in ds:\n",
        "\n",
        "            ds_small = ds[ds_name]['validation'].map(lambda x: {'prompt':inference_formatting(x['question'])})\n",
        "            ds_small = ds_small.shuffle(seed=seed)\n",
        "            ds_small = ds_small.select(range(size))\n",
        "\n",
        "            print(f'working on dataset {ds_name}')\n",
        "\n",
        "            for k in tqdm(range(0,len(ds_small),batch_size)):\n",
        "                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\n",
        "                predictions[model_name,ds_name].append(prediction)\n",
        "\n",
        "            with open(f'./val_results_new_merge/{model_name}_{ds_name}.pkl', 'wb') as f:\n",
        "                pickle.dump(predictions[model_name,ds_name], f)\n",
        "\n",
        "            rouge_scores = {}\n",
        "            bert_scores = {}\n",
        "\n",
        "        for model_name, _ in predictions:\n",
        "\n",
        "            rouge_scores[(model_name,ds_name)] = rouge.compute(\n",
        "                predictions = results[model_name,ds_name],\n",
        "                references = ds_small['QA']\n",
        "            )\n",
        "\n",
        "            bert_scores[(model_name,ds_name)] = bertscore.compute(\n",
        "                predictions = results[model_name,ds_name],\n",
        "                references = ds_small['QA'],\n",
        "                lang='en')\n",
        "        del model\n",
        "\n",
        "    df_preds = pd.DataFrame(predictions)\n",
        "    df_rouge = pd.DataFrame(rouge_scores)\n",
        "    df_bert = pd.DataFrame(bert_scores)\n",
        "\n",
        "    df_preds.to_csv(output_dir+'/predictions.csv',index=False)\n",
        "    df_rouge.to_csv(output_dir+'/rouge.csv',index=False)\n",
        "    df_bert.to_csv(output_dir+'/bertscore.csv',index=False)\n",
        "\n",
        "\n",
        "\n",
        "    return df_preds, df_rouge, df_bert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lGyYcQ0u2AZ1"
      },
      "outputs": [],
      "source": [
        "ds = {}\n",
        "ds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\n",
        "ds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\n",
        "ds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ids = []\n",
        "model_ids.append((None,'meta-llama/Llama-2-7b-hf'))\n",
        "model_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\n",
        "model_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\n",
        "model_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\n",
        "#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\n",
        "#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\n",
        "#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\n",
        "#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))"
      ],
      "metadata": {
        "id": "K-iWExc1GKrg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp5DRLHaxVP4",
        "outputId": "bcbe6a8c-4789-4a56-a7aa-79bddbe9d067"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'=0.13.3'\n",
            "'=0.3.1'\n",
            " \u001b[0m\u001b[01;34martifacts\u001b[0m/\n",
            " \u001b[01;34mdata\u001b[0m/\n",
            " \u001b[01;34mdistilgpt2_ds_wiki_1024_full\u001b[0m/\n",
            " \u001b[01;34mEleutherAI-pythia-70m-deduped_ds_wiki_1024_full\u001b[0m/\n",
            " \u001b[01;34mLlama-2-13b-hf_ds_eli5_1024\u001b[0m/\n",
            " \u001b[01;34mLlama-2-13b-hf_ds_wiki_1024_full\u001b[0m/\n",
            " \u001b[01;34mLlama-2-13b-hf_eli5-wiki-1024\u001b[0m/\n",
            " \u001b[01;34mLlama-2-7b-hf_combined_large_QA_tokenized_1024-v1\u001b[0m/\n",
            " \u001b[01;34mLlama-2-7b-hf_combined_tokenized_1024\u001b[0m/\n",
            " \u001b[01;34mLlama-2-7b-hf_ds_wiki_1024\u001b[0m/\n",
            " \u001b[01;34mLlama-2-7b-hf_ds_wiki_1024_full\u001b[0m/\n",
            " \u001b[01;34mLlama-2-7b-hf_wiki_r_64_alpha_16_wiki\u001b[0m/\n",
            " \u001b[01;34mNone\u001b[0m/\n",
            " \u001b[01;34mNousResearch-Llama-2-7b-hf_ds_wiki_1024_full\u001b[0m/\n",
            " \u001b[01;34mresults\u001b[0m/\n",
            " run_clm.py\n",
            " SFT_QA.ipynb\n",
            " \u001b[01;34mutils\u001b[0m/\n",
            " \u001b[01;34mval_results\u001b[0m/\n",
            " \u001b[01;34mval_results_new_merge\u001b[0m/\n",
            " \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_df_predictions(model_ids,\n",
        "                        ds,\n",
        "                        './llama-2-7b-inference',\n",
        "                        batch_size=2,\n",
        "                        padding=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "173d52575d2f42f08c808846ace321cd",
            "bcb6a39cfcb947d2bddd7772660aad99",
            "981be7d8d9ef40e0b5483979065deace",
            "694770111fda4b50861a5e8723d02ef3",
            "084c7f98c36c4aa19b1f7dfe9dd3cd1a",
            "b32a0b3690234e6ba0bdb7779b801f67",
            "a9b188d7b9a34e0e88db6c0ae0c79bf5",
            "3a91315a01764109a7014c70690bb463",
            "743bf494d11845a7968d7550e2b14b2f",
            "4c5e9325bf52435da9fd3c77bafb5503",
            "1665505e7880470fb90ccec3182f147a",
            "d4ea089f48f2455ca6f4c2625343afb1",
            "e89f9f54ae754db0b6d5a99f165c24a4",
            "8ff2f94113af4c7491675b3745f74e9f",
            "96361f99d2684ac9b25a8e7488e581cd",
            "bae2607fc3f0445b89f8c9bb9a9807b8",
            "39aedbfcf5ad4083b662d43a75dcaef8",
            "384b2ee5d6904acd8cd3491682554a8e",
            "76c0cfed990245d1937db3619189bc3e",
            "f98c5b11f31a4122b58f20fe953e9682",
            "c677aa4ed00b469bbf571042390f300a",
            "85e831e7c1964dca992b9fa3a52ceff7"
          ]
        },
        "id": "75cJDnIoHm8T",
        "outputId": "7cceb274-1733-4b61-ce25-7cd7942cee22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working on model Llama-2-7b-hf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "173d52575d2f42f08c808846ace321cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working on dataset full\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [09:59<00:00, 12.00s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4991 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4ea089f48f2455ca6f4c2625343afb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working on dataset wiki\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 41/50 [08:07<01:48, 12.06s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "c-w9t7mGgjBT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "output_ids = model.generate(input_ids = input['input_ids'],\n",
        "                            attention_mask = input['attention_mask'],\n",
        "                            #pad_token_id = model.config.eos_token_id,\n",
        "                            )\n",
        "\n",
        "predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADOdOb75O5pY",
        "outputId": "0f4d482c-2a57-4855-93b1-29821d406977"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['complete this sentence: I am a ']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "locals()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQK2exUEOMeO",
        "outputId": "b09ef101-8961-4fa4-bc38-79ab6e5687fb"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'__name__': '__main__',\n",
              " '__doc__': 'Automatically created module for IPython interactive environment',\n",
              " '__package__': None,\n",
              " '__loader__': None,\n",
              " '__spec__': None,\n",
              " '__builtin__': <module 'builtins' (built-in)>,\n",
              " '__builtins__': <module 'builtins' (built-in)>,\n",
              " '_ih': ['',\n",
              "  'from google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\nget_ipython().run_line_magic(\\'cd\\', \\'drive/MyDrive/LLMs/Fine-tuning/SFT\\')\\n\\n# installations\\n#!pip install detoxify\\n\\nget_ipython().system(\\'pip install peft==0.4.0\\')\\nget_ipython().system(\\'pip install bitsandbytes==0.41.1\\')\\nget_ipython().system(\\'pip install safetensors>=0.3.1\\')\\nget_ipython().system(\\'pip install trl\\')\\nget_ipython().system(\\'pip install wandb\\')\\nget_ipython().system(\\'pip install tokenizers>=0.13.3\\')\\nget_ipython().system(\\'pip install -U transformers\\')\\nget_ipython().system(\\'pip install accelerate==0.21.0\\')\\nget_ipython().system(\\'pip install datasets\\')\\nget_ipython().system(\\'pip install -U torch\\')\\nget_ipython().system(\\'pip install evaluate\\')\\nget_ipython().system(\\'pip install rouge_score\\')\\nget_ipython().system(\\'pip install nltk\\')\\nget_ipython().system(\\'pip install bert_score\\')\\n\\nget_ipython().system(\\'python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, \\\\\\'Hardware not supported for Flash Attention\\\\\\'\"\\')\\nget_ipython().system(\\'pip install ninja packaging\\')\\nget_ipython().system(\\'pip install flash-attn --no-build-isolation\\')',\n",
              "  'import gc\\n\\nimport os\\nimport torch\\nfrom google.colab import runtime\\nimport pandas as pd\\n\\nimport datasets\\nimport accelerate\\nimport transformers\\nfrom transformers import (AutoTokenizer,\\n                          AutoModelForCausalLM,\\n                          Trainer,\\n                          TrainingArguments,\\n                          DataCollatorForLanguageModeling,\\n                          BitsAndBytesConfig,\\n                          TrainerCallback)\\nimport bitsandbytes as bnb\\nimport wandb\\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\\nfrom datetime import datetime\\nfrom huggingface_hub import login\\n\\nfrom peft.tuners.lora import LoraLayer\\nimport evaluate\\n\\n#from getpass import getpass\\n#hf_token = getpass()\\n#wandb_token = getpass()',\n",
              "  'from getpass import getpass\\nhf_token = getpass()\\nwandb_token = getpass()\\n\\nlogin(hf_token)\\nwandb.login(key=wandb_token)',\n",
              "  '# setup collator\\n\\ndef formatting_prompts_func(example):\\n    output_texts = []\\n    for i in range(len(example[\\'question\\'])):\\n        text = f\"### Human: {example[\\'question\\'][i]}\\\\n ### Assistant: {example[\\'answer\\'][i]}\"\\n        output_texts.append(text)\\n    return output_texts\\n\\ndef sft_collator(tokenizer, response_template = \" ### Assistant:\"):\\n\\n    return DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)\\n\\ndef combine_question_answer(ds,formatting_func):\\n\\n    if \\'QA\\' not in ds[\\'train\\']:\\n        ds = ds.map(lambda x: {\\'QA\\':formatting_func(x)},\\n                    batched=True)\\n    return ds\\n\\ndef prepare_dataset(ds,\\n                    tokenizer,\\n                    formatting_func,\\n                    max_seq_length=\\'auto\\'):\\n\\n    if max_seq_length == \\'auto\\':\\n        max_seq_length = tokenizer.model_max_length\\n\\n    ds = combine_question_answer(ds,formatting_func)\\n\\n    ds = ds.map(lambda x: {\\'tokens\\':tokenizer(x[\\'QA\\'],\\n                                              return_length=False)})\\n\\n    ds = ds.filter(lambda x: len(x[\\'tokens\\'][\\'input_ids\\'])<=max_seq_length)\\n\\n    return ds',\n",
              "  'from huggingface_hub import login\\nfrom collections import defaultdict\\nfrom transformers import AutoTokenizer\\nfrom tqdm import tqdm\\nfrom peft import PeftModel\\nimport pickle\\nimport os\\nimport pandas as pd',\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                #pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  'from huggingface_hub import login\\nfrom collections import defaultdict\\nfrom transformers import AutoTokenizer\\nfrom tqdm import tqdm\\nfrom peft import PeftModel\\nimport pickle\\nimport os\\nimport pandas as pd\\nfrom transformers import pipeline',\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    pipe = pipeline(\\'text-generation\\',model)\\n\\n    predictions = pipe(prompts,generation_config = generation_config) \\n\\n    #input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n#\\n#    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n#                                attention_mask = input[\\'attention_mask\\'],\\n#                                generation_config = generation_config,\\n#                                #pad_token_id = model.config.eos_token_id,\\n#                                )\\n#\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    predictions = pipe(prompts,generation_config = generation_config) \\n\\n    #input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n#\\n#    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n#                                attention_mask = input[\\'attention_mask\\'],\\n#                                generation_config = generation_config,\\n#                                #pad_token_id = model.config.eos_token_id,\\n#                                )\\n#\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_8bit=True\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              "  'bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n                    \\'NousResearch/Llama-2-7b-hf\\',\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\ntokenizer = AutoTokenizer.from_pretrained(\\'meta-llama/Llama-2-7b-hf\\')',\n",
              "  \"tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\",\n",
              "  \"tokenizer.add_tokens(['<unk>'])\",\n",
              "  \"tokenizer.add_tokens(['PAD'])\",\n",
              "  \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\",\n",
              "  'pad_token',\n",
              "  \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a '], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              "  \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a ','this is a test'], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              "  'input',\n",
              "  \"tokenizer.decode(input['input_ids'])\",\n",
              "  'tokenizer.decode(inpu)',\n",
              "  'tokenizer.decode(input)',\n",
              "  \"tokenizer.decode(input['input_ids'][1])\",\n",
              "  \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a ','this is a test.'], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              "  \"tokenizer.decode(input['input_ids'][1])\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        #load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ndel tokenizer\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ndel model\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'locals()',\n",
              "  'dir()',\n",
              "  'import gc\\n\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import transformers',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=4,\\n                        padding=True)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        #load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        with open(f\\'./val_results/{model_name}_{ds_name}.pkl\\', \\'wb\\') as f:\\n            pickle.dump(predictions[model_name,ds_name], f)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  'import gc\\ndel transformers\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ndel bnb_config, ds, input\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'del model_ids, wandb_token\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'locals()'],\n",
              " '_oh': {3: True,\n",
              "  35: 0,\n",
              "  36: 1,\n",
              "  41: {'input_ids': tensor([[    1,  4866,   445, 10541, 29901,   306,   626,   263, 29871],\n",
              "          [    1,   445,   338,   263,  1243, 32000, 32000, 32000, 32000]],\n",
              "         device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "          [1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0')},\n",
              "  45: '<s> this is a testPADPADPADPAD',\n",
              "  47: '<s> this is a test.PADPADPAD',\n",
              "  59: {...},\n",
              "  60: ['AutoModelForCausalLM',\n",
              "   'AutoTokenizer',\n",
              "   'BitsAndBytesConfig',\n",
              "   'DataCollatorForCompletionOnlyLM',\n",
              "   'DataCollatorForLanguageModeling',\n",
              "   'In',\n",
              "   'LoraConfig',\n",
              "   'LoraLayer',\n",
              "   'Out',\n",
              "   'PeftModel',\n",
              "   'SFTTrainer',\n",
              "   'Trainer',\n",
              "   'TrainerCallback',\n",
              "   'TrainingArguments',\n",
              "   '_',\n",
              "   '_3',\n",
              "   '_35',\n",
              "   '_36',\n",
              "   '_41',\n",
              "   '_45',\n",
              "   '_47',\n",
              "   '_59',\n",
              "   '__',\n",
              "   '___',\n",
              "   '__builtin__',\n",
              "   '__builtins__',\n",
              "   '__doc__',\n",
              "   '__loader__',\n",
              "   '__name__',\n",
              "   '__package__',\n",
              "   '__spec__',\n",
              "   '_dh',\n",
              "   '_exit_code',\n",
              "   '_i',\n",
              "   '_i1',\n",
              "   '_i10',\n",
              "   '_i11',\n",
              "   '_i12',\n",
              "   '_i13',\n",
              "   '_i14',\n",
              "   '_i15',\n",
              "   '_i16',\n",
              "   '_i17',\n",
              "   '_i18',\n",
              "   '_i19',\n",
              "   '_i2',\n",
              "   '_i20',\n",
              "   '_i21',\n",
              "   '_i22',\n",
              "   '_i23',\n",
              "   '_i24',\n",
              "   '_i25',\n",
              "   '_i26',\n",
              "   '_i27',\n",
              "   '_i28',\n",
              "   '_i29',\n",
              "   '_i3',\n",
              "   '_i30',\n",
              "   '_i31',\n",
              "   '_i32',\n",
              "   '_i33',\n",
              "   '_i34',\n",
              "   '_i35',\n",
              "   '_i36',\n",
              "   '_i37',\n",
              "   '_i38',\n",
              "   '_i39',\n",
              "   '_i4',\n",
              "   '_i40',\n",
              "   '_i41',\n",
              "   '_i42',\n",
              "   '_i43',\n",
              "   '_i44',\n",
              "   '_i45',\n",
              "   '_i46',\n",
              "   '_i47',\n",
              "   '_i48',\n",
              "   '_i49',\n",
              "   '_i5',\n",
              "   '_i50',\n",
              "   '_i51',\n",
              "   '_i52',\n",
              "   '_i53',\n",
              "   '_i54',\n",
              "   '_i55',\n",
              "   '_i56',\n",
              "   '_i57',\n",
              "   '_i58',\n",
              "   '_i59',\n",
              "   '_i6',\n",
              "   '_i60',\n",
              "   '_i7',\n",
              "   '_i8',\n",
              "   '_i9',\n",
              "   '_ih',\n",
              "   '_ii',\n",
              "   '_iii',\n",
              "   '_oh',\n",
              "   'accelerate',\n",
              "   'bnb',\n",
              "   'bnb_config',\n",
              "   'combine_question_answer',\n",
              "   'datasets',\n",
              "   'datetime',\n",
              "   'defaultdict',\n",
              "   'drive',\n",
              "   'ds',\n",
              "   'evaluate',\n",
              "   'exit',\n",
              "   'formatting_prompts_func',\n",
              "   'gc',\n",
              "   'generate_df_predictions',\n",
              "   'generate_examples',\n",
              "   'get_ipython',\n",
              "   'get_peft_model',\n",
              "   'getpass',\n",
              "   'hf_token',\n",
              "   'inference_formatting',\n",
              "   'input',\n",
              "   'login',\n",
              "   'model_ids',\n",
              "   'os',\n",
              "   'pd',\n",
              "   'pickle',\n",
              "   'pipeline',\n",
              "   'prepare_dataset',\n",
              "   'prepare_model_for_kbit_training',\n",
              "   'quit',\n",
              "   'runtime',\n",
              "   'sft_collator',\n",
              "   'torch',\n",
              "   'tqdm',\n",
              "   'transformers',\n",
              "   'wandb',\n",
              "   'wandb_token']},\n",
              " '_dh': ['/content', '/content/drive/MyDrive/LLMs/Fine-tuning/SFT'],\n",
              " 'In': ['',\n",
              "  'from google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\nget_ipython().run_line_magic(\\'cd\\', \\'drive/MyDrive/LLMs/Fine-tuning/SFT\\')\\n\\n# installations\\n#!pip install detoxify\\n\\nget_ipython().system(\\'pip install peft==0.4.0\\')\\nget_ipython().system(\\'pip install bitsandbytes==0.41.1\\')\\nget_ipython().system(\\'pip install safetensors>=0.3.1\\')\\nget_ipython().system(\\'pip install trl\\')\\nget_ipython().system(\\'pip install wandb\\')\\nget_ipython().system(\\'pip install tokenizers>=0.13.3\\')\\nget_ipython().system(\\'pip install -U transformers\\')\\nget_ipython().system(\\'pip install accelerate==0.21.0\\')\\nget_ipython().system(\\'pip install datasets\\')\\nget_ipython().system(\\'pip install -U torch\\')\\nget_ipython().system(\\'pip install evaluate\\')\\nget_ipython().system(\\'pip install rouge_score\\')\\nget_ipython().system(\\'pip install nltk\\')\\nget_ipython().system(\\'pip install bert_score\\')\\n\\nget_ipython().system(\\'python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, \\\\\\'Hardware not supported for Flash Attention\\\\\\'\"\\')\\nget_ipython().system(\\'pip install ninja packaging\\')\\nget_ipython().system(\\'pip install flash-attn --no-build-isolation\\')',\n",
              "  'import gc\\n\\nimport os\\nimport torch\\nfrom google.colab import runtime\\nimport pandas as pd\\n\\nimport datasets\\nimport accelerate\\nimport transformers\\nfrom transformers import (AutoTokenizer,\\n                          AutoModelForCausalLM,\\n                          Trainer,\\n                          TrainingArguments,\\n                          DataCollatorForLanguageModeling,\\n                          BitsAndBytesConfig,\\n                          TrainerCallback)\\nimport bitsandbytes as bnb\\nimport wandb\\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\\nfrom datetime import datetime\\nfrom huggingface_hub import login\\n\\nfrom peft.tuners.lora import LoraLayer\\nimport evaluate\\n\\n#from getpass import getpass\\n#hf_token = getpass()\\n#wandb_token = getpass()',\n",
              "  'from getpass import getpass\\nhf_token = getpass()\\nwandb_token = getpass()\\n\\nlogin(hf_token)\\nwandb.login(key=wandb_token)',\n",
              "  '# setup collator\\n\\ndef formatting_prompts_func(example):\\n    output_texts = []\\n    for i in range(len(example[\\'question\\'])):\\n        text = f\"### Human: {example[\\'question\\'][i]}\\\\n ### Assistant: {example[\\'answer\\'][i]}\"\\n        output_texts.append(text)\\n    return output_texts\\n\\ndef sft_collator(tokenizer, response_template = \" ### Assistant:\"):\\n\\n    return DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)\\n\\ndef combine_question_answer(ds,formatting_func):\\n\\n    if \\'QA\\' not in ds[\\'train\\']:\\n        ds = ds.map(lambda x: {\\'QA\\':formatting_func(x)},\\n                    batched=True)\\n    return ds\\n\\ndef prepare_dataset(ds,\\n                    tokenizer,\\n                    formatting_func,\\n                    max_seq_length=\\'auto\\'):\\n\\n    if max_seq_length == \\'auto\\':\\n        max_seq_length = tokenizer.model_max_length\\n\\n    ds = combine_question_answer(ds,formatting_func)\\n\\n    ds = ds.map(lambda x: {\\'tokens\\':tokenizer(x[\\'QA\\'],\\n                                              return_length=False)})\\n\\n    ds = ds.filter(lambda x: len(x[\\'tokens\\'][\\'input_ids\\'])<=max_seq_length)\\n\\n    return ds',\n",
              "  'from huggingface_hub import login\\nfrom collections import defaultdict\\nfrom transformers import AutoTokenizer\\nfrom tqdm import tqdm\\nfrom peft import PeftModel\\nimport pickle\\nimport os\\nimport pandas as pd',\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                #pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  'from huggingface_hub import login\\nfrom collections import defaultdict\\nfrom transformers import AutoTokenizer\\nfrom tqdm import tqdm\\nfrom peft import PeftModel\\nimport pickle\\nimport os\\nimport pandas as pd\\nfrom transformers import pipeline',\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    pipe = pipeline(\\'text-generation\\',model)\\n\\n    predictions = pipe(prompts,generation_config = generation_config) \\n\\n    #input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n#\\n#    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n#                                attention_mask = input[\\'attention_mask\\'],\\n#                                generation_config = generation_config,\\n#                                #pad_token_id = model.config.eos_token_id,\\n#                                )\\n#\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    predictions = pipe(prompts,generation_config = generation_config) \\n\\n    #input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n#\\n#    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n#                                attention_mask = input[\\'attention_mask\\'],\\n#                                generation_config = generation_config,\\n#                                #pad_token_id = model.config.eos_token_id,\\n#                                )\\n#\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_8bit=True\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              "  'bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n                    \\'NousResearch/Llama-2-7b-hf\\',\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\ntokenizer = AutoTokenizer.from_pretrained(\\'meta-llama/Llama-2-7b-hf\\')',\n",
              "  \"tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\",\n",
              "  \"tokenizer.add_tokens(['<unk>'])\",\n",
              "  \"tokenizer.add_tokens(['PAD'])\",\n",
              "  \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\",\n",
              "  'pad_token',\n",
              "  \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a '], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              "  \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a ','this is a test'], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              "  'input',\n",
              "  \"tokenizer.decode(input['input_ids'])\",\n",
              "  'tokenizer.decode(inpu)',\n",
              "  'tokenizer.decode(input)',\n",
              "  \"tokenizer.decode(input['input_ids'][1])\",\n",
              "  \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a ','this is a test.'], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              "  \"tokenizer.decode(input['input_ids'][1])\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        #load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              "  \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ndel tokenizer\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ndel model\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'locals()',\n",
              "  'dir()',\n",
              "  'import gc\\n\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import transformers',\n",
              "  \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=4,\\n                        padding=True)\",\n",
              "  'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        #load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        with open(f\\'./val_results/{model_name}_{ds_name}.pkl\\', \\'wb\\') as f:\\n            pickle.dump(predictions[model_name,ds_name], f)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              "  'import gc\\ndel transformers\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ndel bnb_config, ds, input\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'del model_ids, wandb_token\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              "  'locals()'],\n",
              " 'Out': {3: True,\n",
              "  35: 0,\n",
              "  36: 1,\n",
              "  41: {'input_ids': tensor([[    1,  4866,   445, 10541, 29901,   306,   626,   263, 29871],\n",
              "          [    1,   445,   338,   263,  1243, 32000, 32000, 32000, 32000]],\n",
              "         device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "          [1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0')},\n",
              "  45: '<s> this is a testPADPADPADPAD',\n",
              "  47: '<s> this is a test.PADPADPAD',\n",
              "  59: {...},\n",
              "  60: ['AutoModelForCausalLM',\n",
              "   'AutoTokenizer',\n",
              "   'BitsAndBytesConfig',\n",
              "   'DataCollatorForCompletionOnlyLM',\n",
              "   'DataCollatorForLanguageModeling',\n",
              "   'In',\n",
              "   'LoraConfig',\n",
              "   'LoraLayer',\n",
              "   'Out',\n",
              "   'PeftModel',\n",
              "   'SFTTrainer',\n",
              "   'Trainer',\n",
              "   'TrainerCallback',\n",
              "   'TrainingArguments',\n",
              "   '_',\n",
              "   '_3',\n",
              "   '_35',\n",
              "   '_36',\n",
              "   '_41',\n",
              "   '_45',\n",
              "   '_47',\n",
              "   '_59',\n",
              "   '__',\n",
              "   '___',\n",
              "   '__builtin__',\n",
              "   '__builtins__',\n",
              "   '__doc__',\n",
              "   '__loader__',\n",
              "   '__name__',\n",
              "   '__package__',\n",
              "   '__spec__',\n",
              "   '_dh',\n",
              "   '_exit_code',\n",
              "   '_i',\n",
              "   '_i1',\n",
              "   '_i10',\n",
              "   '_i11',\n",
              "   '_i12',\n",
              "   '_i13',\n",
              "   '_i14',\n",
              "   '_i15',\n",
              "   '_i16',\n",
              "   '_i17',\n",
              "   '_i18',\n",
              "   '_i19',\n",
              "   '_i2',\n",
              "   '_i20',\n",
              "   '_i21',\n",
              "   '_i22',\n",
              "   '_i23',\n",
              "   '_i24',\n",
              "   '_i25',\n",
              "   '_i26',\n",
              "   '_i27',\n",
              "   '_i28',\n",
              "   '_i29',\n",
              "   '_i3',\n",
              "   '_i30',\n",
              "   '_i31',\n",
              "   '_i32',\n",
              "   '_i33',\n",
              "   '_i34',\n",
              "   '_i35',\n",
              "   '_i36',\n",
              "   '_i37',\n",
              "   '_i38',\n",
              "   '_i39',\n",
              "   '_i4',\n",
              "   '_i40',\n",
              "   '_i41',\n",
              "   '_i42',\n",
              "   '_i43',\n",
              "   '_i44',\n",
              "   '_i45',\n",
              "   '_i46',\n",
              "   '_i47',\n",
              "   '_i48',\n",
              "   '_i49',\n",
              "   '_i5',\n",
              "   '_i50',\n",
              "   '_i51',\n",
              "   '_i52',\n",
              "   '_i53',\n",
              "   '_i54',\n",
              "   '_i55',\n",
              "   '_i56',\n",
              "   '_i57',\n",
              "   '_i58',\n",
              "   '_i59',\n",
              "   '_i6',\n",
              "   '_i60',\n",
              "   '_i7',\n",
              "   '_i8',\n",
              "   '_i9',\n",
              "   '_ih',\n",
              "   '_ii',\n",
              "   '_iii',\n",
              "   '_oh',\n",
              "   'accelerate',\n",
              "   'bnb',\n",
              "   'bnb_config',\n",
              "   'combine_question_answer',\n",
              "   'datasets',\n",
              "   'datetime',\n",
              "   'defaultdict',\n",
              "   'drive',\n",
              "   'ds',\n",
              "   'evaluate',\n",
              "   'exit',\n",
              "   'formatting_prompts_func',\n",
              "   'gc',\n",
              "   'generate_df_predictions',\n",
              "   'generate_examples',\n",
              "   'get_ipython',\n",
              "   'get_peft_model',\n",
              "   'getpass',\n",
              "   'hf_token',\n",
              "   'inference_formatting',\n",
              "   'input',\n",
              "   'login',\n",
              "   'model_ids',\n",
              "   'os',\n",
              "   'pd',\n",
              "   'pickle',\n",
              "   'pipeline',\n",
              "   'prepare_dataset',\n",
              "   'prepare_model_for_kbit_training',\n",
              "   'quit',\n",
              "   'runtime',\n",
              "   'sft_collator',\n",
              "   'torch',\n",
              "   'tqdm',\n",
              "   'transformers',\n",
              "   'wandb',\n",
              "   'wandb_token']},\n",
              " 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x78cb2020e080>>,\n",
              " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x78cb2020e620>,\n",
              " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x78cb2020e620>,\n",
              " '_': ['AutoModelForCausalLM',\n",
              "  'AutoTokenizer',\n",
              "  'BitsAndBytesConfig',\n",
              "  'DataCollatorForCompletionOnlyLM',\n",
              "  'DataCollatorForLanguageModeling',\n",
              "  'In',\n",
              "  'LoraConfig',\n",
              "  'LoraLayer',\n",
              "  'Out',\n",
              "  'PeftModel',\n",
              "  'SFTTrainer',\n",
              "  'Trainer',\n",
              "  'TrainerCallback',\n",
              "  'TrainingArguments',\n",
              "  '_',\n",
              "  '_3',\n",
              "  '_35',\n",
              "  '_36',\n",
              "  '_41',\n",
              "  '_45',\n",
              "  '_47',\n",
              "  '_59',\n",
              "  '__',\n",
              "  '___',\n",
              "  '__builtin__',\n",
              "  '__builtins__',\n",
              "  '__doc__',\n",
              "  '__loader__',\n",
              "  '__name__',\n",
              "  '__package__',\n",
              "  '__spec__',\n",
              "  '_dh',\n",
              "  '_exit_code',\n",
              "  '_i',\n",
              "  '_i1',\n",
              "  '_i10',\n",
              "  '_i11',\n",
              "  '_i12',\n",
              "  '_i13',\n",
              "  '_i14',\n",
              "  '_i15',\n",
              "  '_i16',\n",
              "  '_i17',\n",
              "  '_i18',\n",
              "  '_i19',\n",
              "  '_i2',\n",
              "  '_i20',\n",
              "  '_i21',\n",
              "  '_i22',\n",
              "  '_i23',\n",
              "  '_i24',\n",
              "  '_i25',\n",
              "  '_i26',\n",
              "  '_i27',\n",
              "  '_i28',\n",
              "  '_i29',\n",
              "  '_i3',\n",
              "  '_i30',\n",
              "  '_i31',\n",
              "  '_i32',\n",
              "  '_i33',\n",
              "  '_i34',\n",
              "  '_i35',\n",
              "  '_i36',\n",
              "  '_i37',\n",
              "  '_i38',\n",
              "  '_i39',\n",
              "  '_i4',\n",
              "  '_i40',\n",
              "  '_i41',\n",
              "  '_i42',\n",
              "  '_i43',\n",
              "  '_i44',\n",
              "  '_i45',\n",
              "  '_i46',\n",
              "  '_i47',\n",
              "  '_i48',\n",
              "  '_i49',\n",
              "  '_i5',\n",
              "  '_i50',\n",
              "  '_i51',\n",
              "  '_i52',\n",
              "  '_i53',\n",
              "  '_i54',\n",
              "  '_i55',\n",
              "  '_i56',\n",
              "  '_i57',\n",
              "  '_i58',\n",
              "  '_i59',\n",
              "  '_i6',\n",
              "  '_i60',\n",
              "  '_i7',\n",
              "  '_i8',\n",
              "  '_i9',\n",
              "  '_ih',\n",
              "  '_ii',\n",
              "  '_iii',\n",
              "  '_oh',\n",
              "  'accelerate',\n",
              "  'bnb',\n",
              "  'bnb_config',\n",
              "  'combine_question_answer',\n",
              "  'datasets',\n",
              "  'datetime',\n",
              "  'defaultdict',\n",
              "  'drive',\n",
              "  'ds',\n",
              "  'evaluate',\n",
              "  'exit',\n",
              "  'formatting_prompts_func',\n",
              "  'gc',\n",
              "  'generate_df_predictions',\n",
              "  'generate_examples',\n",
              "  'get_ipython',\n",
              "  'get_peft_model',\n",
              "  'getpass',\n",
              "  'hf_token',\n",
              "  'inference_formatting',\n",
              "  'input',\n",
              "  'login',\n",
              "  'model_ids',\n",
              "  'os',\n",
              "  'pd',\n",
              "  'pickle',\n",
              "  'pipeline',\n",
              "  'prepare_dataset',\n",
              "  'prepare_model_for_kbit_training',\n",
              "  'quit',\n",
              "  'runtime',\n",
              "  'sft_collator',\n",
              "  'torch',\n",
              "  'tqdm',\n",
              "  'transformers',\n",
              "  'wandb',\n",
              "  'wandb_token'],\n",
              " '__': {...},\n",
              " '___': '<s> this is a test.PADPADPAD',\n",
              " '_i': 'del model_ids, wandb_token\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_ii': 'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_iii': 'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i1': 'from google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\n%cd drive/MyDrive/LLMs/Fine-tuning/SFT\\n\\n# installations\\n#!pip install detoxify\\n\\n!pip install peft==0.4.0\\n!pip install bitsandbytes==0.41.1\\n!pip install safetensors>=0.3.1\\n!pip install trl\\n!pip install wandb\\n!pip install tokenizers>=0.13.3\\n!pip install -U transformers\\n!pip install accelerate==0.21.0\\n!pip install datasets\\n!pip install -U torch\\n!pip install evaluate\\n!pip install rouge_score\\n!pip install nltk\\n!pip install bert_score\\n\\n!python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, \\'Hardware not supported for Flash Attention\\'\"\\n!pip install ninja packaging\\n!pip install flash-attn --no-build-isolation',\n",
              " 'drive': <module 'google.colab.drive' from '/usr/local/lib/python3.10/dist-packages/google/colab/drive.py'>,\n",
              " '_exit_code': 0,\n",
              " '_i2': 'import gc\\n\\nimport os\\nimport torch\\nfrom google.colab import runtime\\nimport pandas as pd\\n\\nimport datasets\\nimport accelerate\\nimport transformers\\nfrom transformers import (AutoTokenizer,\\n                          AutoModelForCausalLM,\\n                          Trainer,\\n                          TrainingArguments,\\n                          DataCollatorForLanguageModeling,\\n                          BitsAndBytesConfig,\\n                          TrainerCallback)\\nimport bitsandbytes as bnb\\nimport wandb\\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\\nfrom trl import SFTTrainer, DataCollatorForCompletionOnlyLM\\nfrom datetime import datetime\\nfrom huggingface_hub import login\\n\\nfrom peft.tuners.lora import LoraLayer\\nimport evaluate\\n\\n#from getpass import getpass\\n#hf_token = getpass()\\n#wandb_token = getpass()',\n",
              " 'gc': <module 'gc' (built-in)>,\n",
              " 'os': <module 'os' from '/usr/lib/python3.10/os.py'>,\n",
              " 'torch': <module 'torch' from '/usr/local/lib/python3.10/dist-packages/torch/__init__.py'>,\n",
              " 'runtime': <module 'google.colab.runtime' from '/usr/local/lib/python3.10/dist-packages/google/colab/runtime.py'>,\n",
              " 'pd': <module 'pandas' from '/usr/local/lib/python3.10/dist-packages/pandas/__init__.py'>,\n",
              " 'datasets': <module 'datasets' from '/usr/local/lib/python3.10/dist-packages/datasets/__init__.py'>,\n",
              " 'accelerate': <module 'accelerate' from '/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py'>,\n",
              " 'AutoTokenizer': transformers.models.auto.tokenization_auto.AutoTokenizer,\n",
              " 'AutoModelForCausalLM': transformers.models.auto.modeling_auto.AutoModelForCausalLM,\n",
              " 'Trainer': transformers.trainer.Trainer,\n",
              " 'TrainingArguments': transformers.training_args.TrainingArguments,\n",
              " 'DataCollatorForLanguageModeling': transformers.data.data_collator.DataCollatorForLanguageModeling,\n",
              " 'BitsAndBytesConfig': transformers.utils.quantization_config.BitsAndBytesConfig,\n",
              " 'TrainerCallback': transformers.trainer_callback.TrainerCallback,\n",
              " 'bnb': <module 'bitsandbytes' from '/usr/local/lib/python3.10/dist-packages/bitsandbytes/__init__.py'>,\n",
              " 'wandb': <module 'wandb' from '/usr/local/lib/python3.10/dist-packages/wandb/__init__.py'>,\n",
              " 'LoraConfig': peft.tuners.lora.LoraConfig,\n",
              " 'get_peft_model': <function peft.mapping.get_peft_model(model: 'PreTrainedModel', peft_config: 'PeftConfig', adapter_name: 'str' = 'default') -> 'PeftModel'>,\n",
              " 'prepare_model_for_kbit_training': <function peft.utils.other.prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)>,\n",
              " 'SFTTrainer': trl.trainer.sft_trainer.SFTTrainer,\n",
              " 'DataCollatorForCompletionOnlyLM': trl.trainer.utils.DataCollatorForCompletionOnlyLM,\n",
              " 'datetime': datetime.datetime,\n",
              " 'login': <function huggingface_hub._login.login(token: Optional[str] = None, add_to_git_credential: bool = False, new_session: bool = True, write_permission: bool = False) -> None>,\n",
              " 'LoraLayer': peft.tuners.lora.LoraLayer,\n",
              " 'evaluate': <module 'evaluate' from '/usr/local/lib/python3.10/dist-packages/evaluate/__init__.py'>,\n",
              " '_i3': 'from getpass import getpass\\nhf_token = getpass()\\nwandb_token = getpass()\\n\\nlogin(hf_token)\\nwandb.login(key=wandb_token)',\n",
              " 'getpass': <bound method Kernel.getpass of <google.colab._kernel.Kernel object at 0x78cb2020dff0>>,\n",
              " 'hf_token': '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',\n",
              " '_3': True,\n",
              " '_i4': '# setup collator\\n\\ndef formatting_prompts_func(example):\\n    output_texts = []\\n    for i in range(len(example[\\'question\\'])):\\n        text = f\"### Human: {example[\\'question\\'][i]}\\\\n ### Assistant: {example[\\'answer\\'][i]}\"\\n        output_texts.append(text)\\n    return output_texts\\n\\ndef sft_collator(tokenizer, response_template = \" ### Assistant:\"):\\n\\n    return DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)\\n\\ndef combine_question_answer(ds,formatting_func):\\n\\n    if \\'QA\\' not in ds[\\'train\\']:\\n        ds = ds.map(lambda x: {\\'QA\\':formatting_func(x)},\\n                    batched=True)\\n    return ds\\n\\ndef prepare_dataset(ds,\\n                    tokenizer,\\n                    formatting_func,\\n                    max_seq_length=\\'auto\\'):\\n\\n    if max_seq_length == \\'auto\\':\\n        max_seq_length = tokenizer.model_max_length\\n\\n    ds = combine_question_answer(ds,formatting_func)\\n\\n    ds = ds.map(lambda x: {\\'tokens\\':tokenizer(x[\\'QA\\'],\\n                                              return_length=False)})\\n\\n    ds = ds.filter(lambda x: len(x[\\'tokens\\'][\\'input_ids\\'])<=max_seq_length)\\n\\n    return ds',\n",
              " 'formatting_prompts_func': <function __main__.formatting_prompts_func(example)>,\n",
              " 'sft_collator': <function __main__.sft_collator(tokenizer, response_template=' ### Assistant:')>,\n",
              " 'combine_question_answer': <function __main__.combine_question_answer(ds, formatting_func)>,\n",
              " 'prepare_dataset': <function __main__.prepare_dataset(ds, tokenizer, formatting_func, max_seq_length='auto')>,\n",
              " '_i5': 'from huggingface_hub import login\\nfrom collections import defaultdict\\nfrom transformers import AutoTokenizer\\nfrom tqdm import tqdm\\nfrom peft import PeftModel\\nimport pickle\\nimport os\\nimport pandas as pd',\n",
              " 'defaultdict': collections.defaultdict,\n",
              " 'tqdm': tqdm.std.tqdm,\n",
              " 'PeftModel': peft.peft_model.PeftModel,\n",
              " 'pickle': <module 'pickle' from '/usr/lib/python3.10/pickle.py'>,\n",
              " '_i6': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                #pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " 'inference_formatting': <function __main__.inference_formatting(example)>,\n",
              " 'generate_examples': <function __main__.generate_examples(model, tokenizer, data, padding=True)>,\n",
              " 'generate_df_predictions': <function __main__.generate_df_predictions(model_ids, ds, output_dir, batch_size=16, seed=50, size=100, padding=True)>,\n",
              " '_i7': 'from huggingface_hub import login\\nfrom collections import defaultdict\\nfrom transformers import AutoTokenizer\\nfrom tqdm import tqdm\\nfrom peft import PeftModel\\nimport pickle\\nimport os\\nimport pandas as pd\\nfrom transformers import pipeline',\n",
              " 'pipeline': <function transformers.pipelines.pipeline(task: str = None, model: Union[str, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel'), NoneType] = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, ForwardRef('PreTrainedTokenizerFast'), NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, image_processor: Union[str, transformers.image_processing_utils.BaseImageProcessor, NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, token: Union[bool, str, NoneType] = None, device: Union[int, str, ForwardRef('torch.device'), NoneType] = None, device_map=None, torch_dtype=None, trust_remote_code: Optional[bool] = None, model_kwargs: Dict[str, Any] = None, pipeline_class: Optional[Any] = None, **kwargs) -> transformers.pipelines.base.Pipeline>,\n",
              " '_i8': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    pipe = pipeline(\\'text-generation\\',model)\\n\\n    predictions = pipe(prompts,generation_config = generation_config) \\n\\n    #input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n#\\n#    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n#                                attention_mask = input[\\'attention_mask\\'],\\n#                                generation_config = generation_config,\\n#                                #pad_token_id = model.config.eos_token_id,\\n#                                )\\n#\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i9': \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              " '_i10': \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\nmodel_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              " '_i11': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              " '_i12': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    predictions = pipe(prompts,generation_config = generation_config) \\n\\n    #input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n#\\n#    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n#                                attention_mask = input[\\'attention_mask\\'],\\n#                                generation_config = generation_config,\\n#                                #pad_token_id = model.config.eos_token_id,\\n#                                )\\n#\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i13': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              " '_i14': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              " '_i15': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              " '_i16': \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              " '_i17': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n#    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i18': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i19': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama2-inference',\\n                        batch_size=1,\\n                        padding=False)\",\n",
              " '_i20': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),1)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+1],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i21': \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              " '_i22': \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              " '_i23': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              " '_i24': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i25': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=False)\",\n",
              " '_i26': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              " '_i27': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_8bit=True\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i28': \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              " '_i29': \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              " '_i30': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              " '_i31': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i32': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              " '_i33': 'bnb_config = BitsAndBytesConfig(\\n        load_in_4bit=True,\\n        bnb_4bit_use_double_quant=True,\\n        bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n                    \\'NousResearch/Llama-2-7b-hf\\',\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\ntokenizer = AutoTokenizer.from_pretrained(\\'meta-llama/Llama-2-7b-hf\\')',\n",
              " '_i34': \"tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\",\n",
              " '_i35': \"tokenizer.add_tokens(['<unk>'])\",\n",
              " '_35': 0,\n",
              " '_i36': \"tokenizer.add_tokens(['PAD'])\",\n",
              " '_36': 1,\n",
              " '_i37': \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\",\n",
              " '_i38': 'pad_token',\n",
              " '_i39': \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a '], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              " '_i40': \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a ','this is a test'], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              " '_i41': 'input',\n",
              " '_41': {'input_ids': tensor([[    1,  4866,   445, 10541, 29901,   306,   626,   263, 29871],\n",
              "         [    1,   445,   338,   263,  1243, 32000, 32000, 32000, 32000]],\n",
              "        device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 0, 0, 0, 0]], device='cuda:0')},\n",
              " '_i42': \"tokenizer.decode(input['input_ids'])\",\n",
              " '_i43': 'tokenizer.decode(inpu)',\n",
              " '_i44': 'tokenizer.decode(input)',\n",
              " '_i45': \"tokenizer.decode(input['input_ids'][1])\",\n",
              " '_45': '<s> this is a testPADPADPADPAD',\n",
              " '_i46': \"tokenizer.add_tokens(['PAD'])\\ntokenizer.pad_token = 'PAD'\\n\\ninput = tokenizer(['complete this sentence: I am a ','this is a test.'], return_tensors = 'pt', padding = True).to('cuda')\",\n",
              " '_i47': \"tokenizer.decode(input['input_ids'][1])\",\n",
              " '_47': '<s> this is a test.PADPADPAD',\n",
              " '_i48': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        #load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i49': \"ds = {}\\nds['full'] = datasets.load_from_disk('../data/SFT_QA_dataset_llama')\\nds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\\nds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')\",\n",
              " '_i50': \"model_ids = []\\nmodel_ids.append((None,'meta-llama/Llama-2-7b-hf'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\\nmodel_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\\n#model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16'))\\n#model_ids.append(('meta-llama/Llama-2-13b-hf', 'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'))\",\n",
              " '_i51': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=16,\\n                        padding=True)\",\n",
              " '_i52': 'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i53': 'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i54': 'import gc\\ndel tokenizer\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i55': 'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i56': 'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i57': 'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i58': 'import gc\\ndel model\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i59': 'locals()',\n",
              " '_59': {...},\n",
              " '_i60': 'dir()',\n",
              " '_60': ['AutoModelForCausalLM',\n",
              "  'AutoTokenizer',\n",
              "  'BitsAndBytesConfig',\n",
              "  'DataCollatorForCompletionOnlyLM',\n",
              "  'DataCollatorForLanguageModeling',\n",
              "  'In',\n",
              "  'LoraConfig',\n",
              "  'LoraLayer',\n",
              "  'Out',\n",
              "  'PeftModel',\n",
              "  'SFTTrainer',\n",
              "  'Trainer',\n",
              "  'TrainerCallback',\n",
              "  'TrainingArguments',\n",
              "  '_',\n",
              "  '_3',\n",
              "  '_35',\n",
              "  '_36',\n",
              "  '_41',\n",
              "  '_45',\n",
              "  '_47',\n",
              "  '_59',\n",
              "  '__',\n",
              "  '___',\n",
              "  '__builtin__',\n",
              "  '__builtins__',\n",
              "  '__doc__',\n",
              "  '__loader__',\n",
              "  '__name__',\n",
              "  '__package__',\n",
              "  '__spec__',\n",
              "  '_dh',\n",
              "  '_exit_code',\n",
              "  '_i',\n",
              "  '_i1',\n",
              "  '_i10',\n",
              "  '_i11',\n",
              "  '_i12',\n",
              "  '_i13',\n",
              "  '_i14',\n",
              "  '_i15',\n",
              "  '_i16',\n",
              "  '_i17',\n",
              "  '_i18',\n",
              "  '_i19',\n",
              "  '_i2',\n",
              "  '_i20',\n",
              "  '_i21',\n",
              "  '_i22',\n",
              "  '_i23',\n",
              "  '_i24',\n",
              "  '_i25',\n",
              "  '_i26',\n",
              "  '_i27',\n",
              "  '_i28',\n",
              "  '_i29',\n",
              "  '_i3',\n",
              "  '_i30',\n",
              "  '_i31',\n",
              "  '_i32',\n",
              "  '_i33',\n",
              "  '_i34',\n",
              "  '_i35',\n",
              "  '_i36',\n",
              "  '_i37',\n",
              "  '_i38',\n",
              "  '_i39',\n",
              "  '_i4',\n",
              "  '_i40',\n",
              "  '_i41',\n",
              "  '_i42',\n",
              "  '_i43',\n",
              "  '_i44',\n",
              "  '_i45',\n",
              "  '_i46',\n",
              "  '_i47',\n",
              "  '_i48',\n",
              "  '_i49',\n",
              "  '_i5',\n",
              "  '_i50',\n",
              "  '_i51',\n",
              "  '_i52',\n",
              "  '_i53',\n",
              "  '_i54',\n",
              "  '_i55',\n",
              "  '_i56',\n",
              "  '_i57',\n",
              "  '_i58',\n",
              "  '_i59',\n",
              "  '_i6',\n",
              "  '_i60',\n",
              "  '_i7',\n",
              "  '_i8',\n",
              "  '_i9',\n",
              "  '_ih',\n",
              "  '_ii',\n",
              "  '_iii',\n",
              "  '_oh',\n",
              "  'accelerate',\n",
              "  'bnb',\n",
              "  'bnb_config',\n",
              "  'combine_question_answer',\n",
              "  'datasets',\n",
              "  'datetime',\n",
              "  'defaultdict',\n",
              "  'drive',\n",
              "  'ds',\n",
              "  'evaluate',\n",
              "  'exit',\n",
              "  'formatting_prompts_func',\n",
              "  'gc',\n",
              "  'generate_df_predictions',\n",
              "  'generate_examples',\n",
              "  'get_ipython',\n",
              "  'get_peft_model',\n",
              "  'getpass',\n",
              "  'hf_token',\n",
              "  'inference_formatting',\n",
              "  'input',\n",
              "  'login',\n",
              "  'model_ids',\n",
              "  'os',\n",
              "  'pd',\n",
              "  'pickle',\n",
              "  'pipeline',\n",
              "  'prepare_dataset',\n",
              "  'prepare_model_for_kbit_training',\n",
              "  'quit',\n",
              "  'runtime',\n",
              "  'sft_collator',\n",
              "  'torch',\n",
              "  'tqdm',\n",
              "  'transformers',\n",
              "  'wandb',\n",
              "  'wandb_token'],\n",
              " '_i61': 'import gc\\n\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i62': 'import transformers',\n",
              " '_i63': \"generate_df_predictions(model_ids,\\n                        ds,\\n                        './llama-2-7b-inference',\\n                        batch_size=4,\\n                        padding=True)\",\n",
              " '_i64': 'def inference_formatting(example):\\n    return f\"### Human: {example}\\\\n ### Assistant:\"\\n\\ndef generate_examples(model,\\n                      tokenizer,\\n                      data,\\n                      padding=True):\\n    generation_config = transformers.GenerationConfig(num_beams = 1,\\n                                         max_new_tokens = 256,\\n                                         do_sample = True,\\n                                         temperature = .6,\\n                                         top_p = 0.9,\\n                                         repetition_penalty = 1.2,\\n                                         #pad_token_id = model.config.eos_token_id\\n                                        )\\n\\n    prompts = data[\\'prompt\\']\\n\\n    #pipe = pipeline(\\'text-generation\\',model,tokenizer=tokenizer)\\n\\n    #predictions = pipe(prompts,generation_config = generation_config) \\n\\n    input = tokenizer(prompts, return_tensors = \\'pt\\', padding = padding).to(\\'cuda\\')\\n\\n    output_ids = model.generate(input_ids = input[\\'input_ids\\'],\\n                                attention_mask = input[\\'attention_mask\\'],\\n                                generation_config = generation_config,\\n                                pad_token_id = model.config.eos_token_id,\\n                                )\\n\\n    predictions =  [tokenizer.decode(ids, skip_special_tokens = True) for ids in output_ids]\\n\\n    return predictions\\n\\ndef generate_df_predictions(model_ids,\\n                            ds,\\n                            output_dir,\\n                            batch_size=16,\\n                            seed = 50,\\n                            size = 100,\\n                            padding=True):\\n        \\n    bnb_config = BitsAndBytesConfig(\\n        #load_in_8bit=True,\\n        #load_in_4bit=True,\\n        #bnb_4bit_use_double_quant=True,\\n        #bnb_4bit_quant_type=\"nf4\",\\n        bnb_4bit_compute_dtype=torch.bfloat16,\\n    )\\n\\n    ds_small = {}\\n    for ds_name in ds:\\n        \\n        ds_small = ds[ds_name][\\'validation\\'].map(lambda x: {\\'prompt\\':inference_formatting(x[\\'question\\'])})\\n        ds_small = ds_small.shuffle(seed=seed)\\n        ds_small = ds_small.select(range(size))\\n\\n        print(f\\'working on dataset {ds_name}\\')\\n\\n        for base_model, model_id in model_ids:\\n            tokenizer = AutoTokenizer.from_pretrained(model_id)\\n            \\n            tokenizer.pad_token = tokenizer.eos_token\\n            tokenizer.padding_side = \"left\"\\n            predictions = defaultdict(list)\\n\\n            if base_model:\\n\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    base_model,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n\\n                model = PeftModel.from_pretrained(model = model,\\n                                model_id = model_id,\\n                                torch_dtype = torch.bfloat16,\\n                                is_trainable = False)\\n            else:\\n                model = AutoModelForCausalLM.from_pretrained(\\n                    model_id,\\n                    device_map=\"auto\",\\n                    load_in_4bit = True,\\n                    quantization_config=bnb_config\\n                    )\\n                \\n            model.eval()\\n            model_name = model_id.split(\\'/\\')[-1]\\n                \\n            for k in tqdm(range(0,len(ds_small),batch_size)):\\n                prediction = generate_examples(model,tokenizer, ds_small[k:k+batch_size],padding=padding)\\n                predictions[model_name,ds_name].append(prediction)\\n\\n        with open(f\\'./val_results/{model_name}_{ds_name}.pkl\\', \\'wb\\') as f:\\n            pickle.dump(predictions[model_name,ds_name], f)\\n\\n        rouge_scores = {}\\n        bert_scores = {}\\n        \\n        for model_name, _ in predictions:\\n\\n            rouge_scores[(model_name,ds_name)] = rouge.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\']\\n            )\\n\\n            bert_scores[(model_name,ds_name)] = bertscore.compute(\\n                predictions = results[model_name,ds_name],\\n                references = ds_small[\\'QA\\'],\\n                lang=\\'en\\')\\n        \\n    df_preds = pd.DataFrame(predictions)\\n    df_rouge = pd.DataFrame(rouge_scores)\\n    df_bert = pd.DataFrame(bert_scores)\\n        \\n    df_preds.to_csv(output_dir+\\'/predictions.csv\\',index=False)\\n    df_rouge.to_csv(output_dir+\\'/rouge.csv\\',index=False)\\n    df_bert.to_csv(output_dir+\\'/bertscore.csv\\',index=False)\\n\\n\\n    return df_preds, df_rouge, df_bert',\n",
              " '_i65': 'import gc\\ndel transformers\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              " '_i66': 'import gc\\ndel bnb_config, ds, input\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              " '_i67': 'import gc\\ngc.collect()\\ngc.enable()\\ntorch.cuda.empty_cache()',\n",
              " '_i68': 'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i69': 'import gc\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i70': 'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i71': 'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i72': 'gc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i73': 'del model_ids, wandb_token\\ngc.enable()\\ngc.collect()\\ntorch.cuda.empty_cache()',\n",
              " '_i74': 'locals()'}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y96ir55bBXj0"
      },
      "source": [
        "### Fixing results formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0B3WzmPxUYX"
      },
      "outputs": [],
      "source": [
        "df_results = pd.read_csv('./data/df_predictions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA9mJfm2yzdR"
      },
      "outputs": [],
      "source": [
        "results = defaultdict(list)\n",
        "\n",
        "for col in df_results.columns[1:]:\n",
        "    if '.' in col:\n",
        "        model_name = col.split('.')[0]\n",
        "    else:\n",
        "        model_name = col\n",
        "    ds_name = df_results[col][0]\n",
        "\n",
        "    for i in range(1,8):\n",
        "        results[(model_name,ds_name)].extend(eval(df_results[col][i]))\n",
        "\n",
        "df_results_fixed = pd.DataFrame(results).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN-K-gSjtnod"
      },
      "outputs": [],
      "source": [
        "df_results_fixed.to_csv('./results/df_results_fixed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "7ae676caf3524c1d9a10c49dbe83fbc8",
            "4abbd932ee124462b6fc77236667ea54",
            "1e230e44421d4c7796a09a8bd2d3daf7",
            "f185b14a21ad4ff5b0560f6545b20d6d",
            "1f95e69df31243eab1d881944b6ca7e3",
            "b001d09cfd2341fcba7272b5dfbbb3fc",
            "fc586e9bbf5e436f899a5408a14f35a8",
            "b8148a9b2cf9450aa7461c1cbab41fa6"
          ]
        },
        "id": "rEHUMyYsug4t",
        "outputId": "094a6389-5118-4933-f4fd-8039f632119a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666997038333117, max=1.0)…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ae676caf3524c1d9a10c49dbe83fbc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230831_182848-88g9xv9h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/inference/runs/88g9xv9h' target=\"_blank\">wild-deluge-5</a></strong> to <a href='https://wandb.ai/ft-llmmm/inference' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ft-llmmm/inference' target=\"_blank\">https://wandb.ai/ft-llmmm/inference</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ft-llmmm/inference/runs/88g9xv9h' target=\"_blank\">https://wandb.ai/ft-llmmm/inference/runs/88g9xv9h</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(entity='ft-llmmm',project='inference')\n",
        "run.log({'Val_Predictions':wandb.Table(dataframe=df_results_fixed.iloc[:,:99])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4i9-FHyu_rq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "fc56238f-39b8-403d-b987-8b84c126f533"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">revived-flower-4</strong> at: <a href='https://wandb.ai/ft-llmmm/inference/runs/n0uytzjn' target=\"_blank\">https://wandb.ai/ft-llmmm/inference/runs/n0uytzjn</a><br/>Synced 4 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230831_180836-n0uytzjn/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD9fQDwAKv_E"
      },
      "outputs": [],
      "source": [
        "df_results_fixed=pd.read_csv('./results/df_results_fixed.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_fixed.iloc[i:i+1,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "2oF_lufg3n_S",
        "outputId": "c46f4bc7-c101-4c1c-a375-111eb7afa993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          model dataset  \\\n",
              "0  llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16    full   \n",
              "\n",
              "                                                   0  \\\n",
              "0  ### Human: What was the purpose of Apollo 10?\\...   \n",
              "\n",
              "                                                   1  \\\n",
              "0  ### Human: Who is Hervé Barulea, also known as...   \n",
              "\n",
              "                                                   2  \\\n",
              "0  ### Human: Who was Danny Murphy and what were ...   \n",
              "\n",
              "                                                   3  \\\n",
              "0  ### Human: Who was David Azulai and when did h...   \n",
              "\n",
              "                                                   4  \\\n",
              "0  ### Human: What is a song and what are some di...   \n",
              "\n",
              "                                                   5  \\\n",
              "0  ### Human: What is the origin of the Chenab Ri...   \n",
              "\n",
              "                                                   6  \\\n",
              "0  ### Human: Who was Richard Pryor and what awar...   \n",
              "\n",
              "                                                   7  ...  \\\n",
              "0  ### Human: What is the defense mechanism of th...  ...   \n",
              "\n",
              "                                                  90  \\\n",
              "0  ### Human: Who was Carlos Amadeu?\\n ### Assist...   \n",
              "\n",
              "                                                  91  \\\n",
              "0  ### Human: What is Pau and where is it located...   \n",
              "\n",
              "                                                  92  \\\n",
              "0  ### Human: What is the black sapote also known...   \n",
              "\n",
              "                                                  93  \\\n",
              "0  ### Human: What is the title of the movie Litt...   \n",
              "\n",
              "                                                  94  \\\n",
              "0  ### Human: Who are the Twelve Olympians in Gre...   \n",
              "\n",
              "                                                  95  \\\n",
              "0  ### Human: Who is Michael Blunden?\\n ### Assis...   \n",
              "\n",
              "                                                  96  \\\n",
              "0  ### Human: What is a damson and how is it diff...   \n",
              "\n",
              "                                                  97  \\\n",
              "0  ### Human: Who was Nat King Cole?\\n ### Assist...   \n",
              "\n",
              "                                                  98  \\\n",
              "0  ### Human: What is the official currency of Si...   \n",
              "\n",
              "                                                  99  \n",
              "0  ### Human: What is the Klondike?\\n ### Assista...  \n",
              "\n",
              "[1 rows x 102 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9efa8310-937c-44ef-a812-a5c26ed3008f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>dataset</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16</td>\n",
              "      <td>full</td>\n",
              "      <td>### Human: What was the purpose of Apollo 10?\\...</td>\n",
              "      <td>### Human: Who is Hervé Barulea, also known as...</td>\n",
              "      <td>### Human: Who was Danny Murphy and what were ...</td>\n",
              "      <td>### Human: Who was David Azulai and when did h...</td>\n",
              "      <td>### Human: What is a song and what are some di...</td>\n",
              "      <td>### Human: What is the origin of the Chenab Ri...</td>\n",
              "      <td>### Human: Who was Richard Pryor and what awar...</td>\n",
              "      <td>### Human: What is the defense mechanism of th...</td>\n",
              "      <td>...</td>\n",
              "      <td>### Human: Who was Carlos Amadeu?\\n ### Assist...</td>\n",
              "      <td>### Human: What is Pau and where is it located...</td>\n",
              "      <td>### Human: What is the black sapote also known...</td>\n",
              "      <td>### Human: What is the title of the movie Litt...</td>\n",
              "      <td>### Human: Who are the Twelve Olympians in Gre...</td>\n",
              "      <td>### Human: Who is Michael Blunden?\\n ### Assis...</td>\n",
              "      <td>### Human: What is a damson and how is it diff...</td>\n",
              "      <td>### Human: Who was Nat King Cole?\\n ### Assist...</td>\n",
              "      <td>### Human: What is the official currency of Si...</td>\n",
              "      <td>### Human: What is the Klondike?\\n ### Assista...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 102 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9efa8310-937c-44ef-a812-a5c26ed3008f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9efa8310-937c-44ef-a812-a5c26ed3008f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9efa8310-937c-44ef-a812-a5c26ed3008f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "series_pred['model'].values[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2vS2Rhtm4RBO",
        "outputId": "74c363e2-2e82-472c-b17b-1c7a3b295ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "    series_pred = df_results_fixed.iloc[i:i+1,:]\n",
        "    model_name = series_pred['model'].values[0]\n",
        "    ds_name = series_pred['dataset'].values[0]\n",
        "\n",
        "    run.log({f'{model_name}_{ds_name}_eval':wandb.Table(dataframe=series_pred.iloc[:1,2:])})"
      ],
      "metadata": {
        "id": "bdMLFxnexyRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_results_fixed= df_results_fixed.rename(columns={'Unnamed: 0':'model'})"
      ],
      "metadata": {
        "id": "ytzZqKVcxT8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiKzb744KyMy"
      },
      "outputs": [],
      "source": [
        "df_results_fixed = df_results_fixed.set_index(['Unnamed: 0','Unnamed: 1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC245FSALIgq"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-HWZ9kcLFs7"
      },
      "outputs": [],
      "source": [
        "llama_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZrgnbEwK-A0"
      },
      "outputs": [],
      "source": [
        "df_results_split = df_results_fixed.applymap(lambda x:x.split('Assistant:')[-1])\n",
        "df_results_split = df_results_split.applymap(lambda x: len(llama_tokenizer(x)['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkZFdlWuL2t5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "uG8cMufcMbf_",
        "outputId": "121d86e0-cec3-454d-e9b7-ed3fcf7edeff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-694f655f-c7de-411e-a755-050cf2b19fcf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>mean</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16</th>\n",
              "      <th>full</th>\n",
              "      <td>512.01</td>\n",
              "      <td>366.0</td>\n",
              "      <td>514.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16</th>\n",
              "      <th>full</th>\n",
              "      <td>505.15</td>\n",
              "      <td>312.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16</th>\n",
              "      <th>full</th>\n",
              "      <td>513.95</td>\n",
              "      <td>513.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama2-7b</th>\n",
              "      <th>full</th>\n",
              "      <td>513.98</td>\n",
              "      <td>513.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16</th>\n",
              "      <th>wiki</th>\n",
              "      <td>511.28</td>\n",
              "      <td>378.0</td>\n",
              "      <td>516.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16</th>\n",
              "      <th>wiki</th>\n",
              "      <td>510.96</td>\n",
              "      <td>350.0</td>\n",
              "      <td>516.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16</th>\n",
              "      <th>wiki</th>\n",
              "      <td>511.45</td>\n",
              "      <td>261.0</td>\n",
              "      <td>514.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama2-7b</th>\n",
              "      <th>wiki</th>\n",
              "      <td>513.95</td>\n",
              "      <td>513.0</td>\n",
              "      <td>514.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16</th>\n",
              "      <th>eli5</th>\n",
              "      <td>513.39</td>\n",
              "      <td>462.0</td>\n",
              "      <td>514.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16</th>\n",
              "      <th>eli5</th>\n",
              "      <td>502.45</td>\n",
              "      <td>334.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16</th>\n",
              "      <th>eli5</th>\n",
              "      <td>513.96</td>\n",
              "      <td>513.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama2-7b</th>\n",
              "      <th>eli5</th>\n",
              "      <td>509.66</td>\n",
              "      <td>182.0</td>\n",
              "      <td>515.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-694f655f-c7de-411e-a755-050cf2b19fcf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-694f655f-c7de-411e-a755-050cf2b19fcf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-694f655f-c7de-411e-a755-050cf2b19fcf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dd5d6e15-b967-47b1-8754-0e71b1e8811c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd5d6e15-b967-47b1-8754-0e71b1e8811c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dd5d6e15-b967-47b1-8754-0e71b1e8811c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                           mean    min    max\n",
              "Unnamed: 0                                   Unnamed: 1                      \n",
              "llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16 full        512.01  366.0  514.0\n",
              "llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16   full        505.15  312.0  515.0\n",
              "llama-7b-SFT_ds_eli5_1024_r_64_alpha_16      full        513.95  513.0  515.0\n",
              "llama2-7b                                    full        513.98  513.0  515.0\n",
              "llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16 wiki        511.28  378.0  516.0\n",
              "llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16   wiki        510.96  350.0  516.0\n",
              "llama-7b-SFT_ds_eli5_1024_r_64_alpha_16      wiki        511.45  261.0  514.0\n",
              "llama2-7b                                    wiki        513.95  513.0  514.0\n",
              "llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16 eli5        513.39  462.0  514.0\n",
              "llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16   eli5        502.45  334.0  515.0\n",
              "llama-7b-SFT_ds_eli5_1024_r_64_alpha_16      eli5        513.96  513.0  515.0\n",
              "llama2-7b                                    eli5        509.66  182.0  515.0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_results_split.agg(func=['mean','min','max'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgwo2wa21iyO"
      },
      "outputs": [],
      "source": [
        "adapter_model_ids = ['dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16',\n",
        "          'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16',\n",
        "          'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16']\n",
        "\n",
        "model_names = [model_id.split('/')[-1] for model_id in adapter_model_ids]\n",
        "\n",
        "model_names.append('llama2-7b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0-Ju3MUBaBd"
      },
      "source": [
        "### Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbfTb2Wq1OZj"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TJfGXBErEANy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAUEtTH11Z9o"
      },
      "outputs": [],
      "source": [
        "rouge_scores = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f'working on model {model_name}')\n",
        "    for ds_name in ds:\n",
        "        print(f'working on dataset {ds_name}')\n",
        "\n",
        "        data = ds[ds_name]['validation']\n",
        "        data = data.shuffle(seed=50)\n",
        "        data_small = data.select(range(100))\n",
        "\n",
        "        if (model_name,ds_name) in rouge_scores:\n",
        "            continue\n",
        "\n",
        "        rouge_scores[(model_name,ds_name)] = rouge.compute(\n",
        "            predictions = results[model_name,ds_name],\n",
        "            references = data_small['QA']\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G92gbuJZ6AcH"
      },
      "outputs": [],
      "source": [
        "indices_relabel = {\n",
        "    'llama-7b-SFT_ds_eli5_1024_r_64_alpha_16':'llama2-7b-eli5',\n",
        "    'llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16':'llama2-7b-wiki',\n",
        "    'llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16':'llama2-7b-eli5-wiki'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLIOp4VT-QMT"
      },
      "outputs": [],
      "source": [
        "df_dict = {}\n",
        "\n",
        "for ds_name in ['eli5','wiki','full']:\n",
        "\n",
        "    df_dict[ds_name] = pd.concat([pd.Series(rouge_scores[(model_name,ds_name)])\n",
        "            for model_name in model_names],axis=1).T\n",
        "    df_dict[ds_name].index = model_names\n",
        "    df_dict[ds_name] = df_dict[ds_name].rename(index = indices_relabel)\n",
        "    df_dict[ds_name] = df_dict[ds_name].loc[['llama2-7b',\n",
        "                        'llama2-7b-eli5',\n",
        "                        'llama2-7b-wiki',\n",
        "                        'llama2-7b-eli5-wiki']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "y2KtZWHBAMD4",
        "outputId": "24683925-19dd-4f38-b8aa-40d033c9b7ac"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230826_040139-ad283uwj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/inference/runs/ad283uwj' target=\"_blank\">summer-glitter-1</a></strong> to <a href='https://wandb.ai/ft-llmmm/inference' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ft-llmmm/inference' target=\"_blank\">https://wandb.ai/ft-llmmm/inference</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ft-llmmm/inference/runs/ad283uwj' target=\"_blank\">https://wandb.ai/ft-llmmm/inference/runs/ad283uwj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(entity='ft-llmmm',\n",
        "                 project='inference')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_6MaWe5__b3"
      },
      "outputs": [],
      "source": [
        "for ds_name in ['eli5','wiki','full']:\n",
        "    table = wandb.Table(dataframe=df_dict[ds_name].reset_index())\n",
        "    run.log({f'llama2-7b_{ds_name}':table})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1WAQu_14Nqm"
      },
      "outputs": [],
      "source": [
        "rouge1 = pd.DataFrame(rouge_scores).loc['rouge1'].unstack()\n",
        "\n",
        "rouge1_v2 = rouge1.reindex(['llama2-7b',\n",
        "                         'llama-7b-SFT_ds_eli5_1024_r_64_alpha_16',\n",
        "                         'llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16',\n",
        "                         'llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16'])\n",
        "\n",
        "rouge1_v2 = rouge1_v2.rename(index = indices_relabel)\n",
        "rouge1_v2 = rouge1_v2[['eli5','wiki','full']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDhRXpaO55SS"
      },
      "outputs": [],
      "source": [
        "df_combined = pd.DataFrame(rouge_scores).stack().rename(columns=indices_relabel)\n",
        "df_combined = df_combined.swaplevel().unstack().loc[['eli5','wiki','full']]\n",
        "df_combined = df_combined.T.unstack()\n",
        "df_combined = df_combined.loc[['llama2-7b',\n",
        "                               'llama2-7b-eli5',\n",
        "                               'llama2-7b-wiki',\n",
        "                               'llama2-7b-eli5-wiki']]\n",
        "#df_combined.stack().stack().swaplevel(i=0,j=2).unstack().unstack()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZrYAFxHJ2dQ"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qc1cQBHN8ZBR"
      },
      "outputs": [],
      "source": [
        "df_combined.to_csv('./results/df_combined.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miyXPk3HBdgp"
      },
      "source": [
        "### Bert-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2JleeuQLtLq"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2519667bcc7e43518b16999f15d1212a"
          ]
        },
        "id": "oD_X_EHWBjbp",
        "outputId": "ab1fd25e-eb9e-48f3-8350-876cc9624710"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2519667bcc7e43518b16999f15d1212a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bertscore = evaluate.load(\"bertscore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN8F31jnBxuc"
      },
      "outputs": [],
      "source": [
        "bert_scores = {}\n",
        "\n",
        "for model_name in model_names:\n",
        "    print(f'working on model {model_name}')\n",
        "    for ds_name in ds:\n",
        "        print(f'working on dataset {ds_name}')\n",
        "\n",
        "        data = ds[ds_name]['validation']\n",
        "        data = data.shuffle(seed=50)\n",
        "        data_small = data.select(range(100))\n",
        "\n",
        "        bert_scores[(model_name,ds_name)] = bertscore.compute(\n",
        "            predictions = results[model_name,ds_name],\n",
        "            references = data_small['QA'],\n",
        "            lang='en'\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXjcHvfYEO67"
      },
      "outputs": [],
      "source": [
        "df_bert_scores_v0 = pd.DataFrame(bert_scores)\n",
        "df_bert_scores_v0.to_csv('./results/df_bert_scores_v0.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mumamP9hGPdS"
      },
      "outputs": [],
      "source": [
        "models_relabel = {\n",
        "    'llama-7b-SFT_ds_eli5_1024_r_64_alpha_16':'llama2-7b-eli5',\n",
        "    'llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16':'llama2-7b-wiki',\n",
        "    'llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16':'llama2-7b-eli5-wiki'\n",
        "}\n",
        "\n",
        "def fix_names(model_name):\n",
        "    if '.' in model_name:\n",
        "        model_name = model_name.split('.')[0]\n",
        "    if model_name in models_relabel:\n",
        "        return models_relabel[model_name]\n",
        "    else:\n",
        "        return model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnhUHOGbGKE8"
      },
      "outputs": [],
      "source": [
        "df_bert_scores_v0 = pd.read_csv('./results/df_bert_scores_v0.csv').T\n",
        "df_bert_scores_v0.columns=['dataset','precision','recall','f1','hashcode']\n",
        "df_bert_scores_v0 = df_bert_scores_v0.iloc[1:,:-1].reset_index()\n",
        "df_bert_scores_v0['index'] = df_bert_scores_v0['index'].apply(fix_names)\n",
        "#df_bert_scores_v0 = df_bert_scores_v0.set_index(['index','dataset'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5BEcPhHH9IM"
      },
      "outputs": [],
      "source": [
        "bert_score_summary = {}\n",
        "for ds_name in ['eli5','wiki','full']:\n",
        "    bert_score_summary[ds_name] = df_bert_scores_v0[df_bert_scores_v0['dataset']==\n",
        "                                                    ds_name][['index','precision','recall','f1']]\n",
        "    bert_score_summary[ds_name] = bert_score_summary[ds_name].set_index('index')\n",
        "    bert_score_summary[ds_name] = bert_score_summary[ds_name].loc[['llama2-7b',\n",
        "                                                                   'llama2-7b-eli5',\n",
        "                                                                   'llama2-7b-wiki',\n",
        "                                                                   'llama2-7b-eli5-wiki']]\n",
        "    bert_score_summary[ds_name] = bert_score_summary[ds_name].applymap(lambda x:\n",
        "                                                                       np.mean(eval(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "EgaUVIReMB43",
        "outputId": "56fad2e7-f1ea-472f-af7e-5980274cb79a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/LLMs/Fine-tuning/SFT/wandb/run-20230826_045331-1qznwbtn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ft-llmmm/inference/runs/1qznwbtn' target=\"_blank\">likely-haze-2</a></strong> to <a href='https://wandb.ai/ft-llmmm/inference' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/ft-llmmm/inference' target=\"_blank\">https://wandb.ai/ft-llmmm/inference</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/ft-llmmm/inference/runs/1qznwbtn' target=\"_blank\">https://wandb.ai/ft-llmmm/inference/runs/1qznwbtn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(entity='ft-llmmm',\n",
        "                 project='inference')\n",
        "\n",
        "for ds_name in ['eli5','wiki','full']:\n",
        "    table = wandb.Table(dataframe=bert_score_summary[ds_name].reset_index())\n",
        "    run.log({f'llama2-7b_bertscore_{ds_name}':table})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151,
          "referenced_widgets": [
            "c1f9256d8c814beb849fc7a3a89520e5"
          ]
        },
        "id": "W4xvdJjpMWju",
        "outputId": "ae26ced8-496f-4422-ba7d-4c927b4a42b5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1f9256d8c814beb849fc7a3a89520e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.005 MB of 0.016 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.299669…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">likely-haze-2</strong> at: <a href='https://wandb.ai/ft-llmmm/inference/runs/1qznwbtn' target=\"_blank\">https://wandb.ai/ft-llmmm/inference/runs/1qznwbtn</a><br/>Synced 4 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230826_045331-1qznwbtn/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merging Weights"
      ],
      "metadata": {
        "id": "WInFDBrsVoU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import peft\n",
        "import json\n",
        "import shutil\n",
        "from peft.utils import _get_submodules\n",
        "import os\n",
        "import bitsandbytes as bnb\n",
        "from bitsandbytes.functional import dequantize_4bit\n",
        "from peft import PeftModel\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          LlamaForCausalLM,\n",
        "                          LlamaTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          AutoTokenizer\n",
        ")\n",
        "import gc\n",
        "import copy\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "YE35tkQkX446"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii67UTsRrkOQ"
      },
      "outputs": [],
      "source": [
        "def dequantize_model(model, tokenizer, dtype=torch.bfloat16, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    'model': the peftmodel you loaded with qlora.\n",
        "    'tokenizer': the model's corresponding hf's tokenizer.\n",
        "    'to': directory to save the dequantized model\n",
        "    'dtype': dtype that the model was trained using\n",
        "    'device': device to load the model to\n",
        "    \"\"\"\n",
        "\n",
        "    cls = bnb.nn.Linear4bit\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, cls):\n",
        "                print(f\"Dequantizing `{name}`...\")\n",
        "                quant_state = copy.deepcopy(module.weight.quant_state)\n",
        "\n",
        "                quant_state[2] = dtype\n",
        "\n",
        "                weights = dequantize_4bit(module.weight.data, quant_state=quant_state, quant_type=\"nf4\").to(dtype)\n",
        "\n",
        "                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=None, dtype=dtype)\n",
        "                new_module.weight = torch.nn.Parameter(weights)\n",
        "                new_module.to(device=device, dtype=dtype)\n",
        "\n",
        "                parent, target, target_name = _get_submodules(model, name)\n",
        "                setattr(parent, target_name, new_module)\n",
        "\n",
        "        # a hack, setting this to avoid hf's saving error because hf\n",
        "        # itself does not support saving a model that is registered to be loaded in 4bit.\n",
        "        model.is_loaded_in_4bit = False\n",
        "        return model\n",
        "\n",
        "def merge_weights(base_model_id,\n",
        "                  adapter_model_id,\n",
        "                  hf_token,\n",
        "                  dtype=torch.bfloat16,\n",
        "                  device=\"cuda\"):\n",
        "\n",
        "    repo_id = adapter_model_id+'_merged'\n",
        "\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_id,\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map={\"\": 0},\n",
        "        use_auth_token=hf_token\n",
        "        )\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(base_model_id,\n",
        "                                         use_auth_token=hf_token\n",
        "                                        )\n",
        "    model = dequantize_model(model, tok)\n",
        "    model = PeftModel.from_pretrained(model = model, model_id = adapter_model_id)\n",
        "    model = model.merge_and_unload()\n",
        "\n",
        "    model.push_to_hub(repo_id,safe_serialization=True)\n",
        "    tok.push_to_hub(repo_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onuYf1c0Quhj"
      },
      "outputs": [],
      "source": [
        "adapter_models = [\n",
        "    'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16',\n",
        "    'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16',\n",
        "    'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16'\n",
        "]\n",
        "base_model_id = 'meta-llama/Llama-2-7b-hf'\n",
        "\n",
        "for adapter_model in adapter_models:\n",
        "    merge_weights(base_model_id,\n",
        "                  adapter_model,\n",
        "                  hf_token,\n",
        "                  dtype=torch.bfloat16,\n",
        "                  device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = 'meta-llama/Llama-2-13b-hf'\n",
        "adapter_model ='dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16'\n",
        "\n",
        "merge_weights(base_model_id,\n",
        "                adapter_model,\n",
        "                hf_token,\n",
        "                dtype=torch.bfloat16,\n",
        "                device=\"cuda\")"
      ],
      "metadata": {
        "id": "7FcLgZzbW3CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNSSMGyo5zCJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Gjygy89vQITv",
        "Z5dtefRJQcWe",
        "xVQPqQvfiP_j",
        "UvhKLq9bXygs",
        "rP1GQbTiT1P1",
        "7XD5fUFsT21h",
        "uElMmvz9X1Px",
        "gbNAxtQqQMN2",
        "gTapsiF6W_wZ",
        "Y96ir55bBXj0",
        "WInFDBrsVoU5"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fff54b74b3740dd91a93914b77d414e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13e5289477db410cb394a8a3fc46ef95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24eeaa6ad3a24b34a75ef63c23770e15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2872be6249e742ecaaa7528434584a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24eeaa6ad3a24b34a75ef63c23770e15",
            "placeholder": "​",
            "style": "IPY_MODEL_13e5289477db410cb394a8a3fc46ef95",
            "value": " 5939/5939 [00:00&lt;00:00, 7009.45 examples/s]"
          }
        },
        "2d932962f1df4cb78560ff2909a7e53d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3334eb1e16604ff0a8c28c6ed38dfd9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c4923ea45184b8f843af340b8c5103e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fff54b74b3740dd91a93914b77d414e",
            "placeholder": "​",
            "style": "IPY_MODEL_558728111107485d8a356f726f0ede23",
            "value": "Filter: 100%"
          }
        },
        "4aa1d7e657f1483f990e5116c3b0b8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dff5b5617dd3426e98743ab828517a39",
            "placeholder": "​",
            "style": "IPY_MODEL_b2ebdfde6f8e4045959377101c65f3d6",
            "value": "Filter: 100%"
          }
        },
        "4b3f4137f26f4ebc891cd8cfbe3362ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c4923ea45184b8f843af340b8c5103e",
              "IPY_MODEL_f7d02bc9e0e74cb49e9ec3e390e1814a",
              "IPY_MODEL_2872be6249e742ecaaa7528434584a58"
            ],
            "layout": "IPY_MODEL_8986504bfec64133a9392319fe68e2a3"
          }
        },
        "558728111107485d8a356f726f0ede23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5746d68c6e324cfcbcd02c4b26d44138": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d3d425474244d5ba1774c33ac49a6a1",
            "max": 7247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b45b4f9f716f42ebb5d2f78bca8d8020",
            "value": 7247
          }
        },
        "66db49cef7644066ac851f42b3d855f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d3d425474244d5ba1774c33ac49a6a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812f198b05104ad28ae9d29399b4d77d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8986504bfec64133a9392319fe68e2a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a7fb3f1296542bcb34e51dbedca929c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc7c44726b514729a3b91c8734dc6e94",
            "placeholder": "​",
            "style": "IPY_MODEL_8ac4e9af3bfb478192e33ecc429ead4c",
            "value": "Filter: 100%"
          }
        },
        "8ac4e9af3bfb478192e33ecc429ead4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d61ae0b338048a2979ce5ba42613528": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ccb486b2604e24a718eeb7e7551467": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3d1a2bd3fde49139935cdde49bcf6a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5fc78d74ac946a6bd85f14cdff3fd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a71d6e6c71694cbba52dfb3cb5d9f00b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_812f198b05104ad28ae9d29399b4d77d",
            "max": 106557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9d442ee43e647169cbf4f862ebfeb9d",
            "value": 106557
          }
        },
        "b2ebdfde6f8e4045959377101c65f3d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b45b4f9f716f42ebb5d2f78bca8d8020": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc7c44726b514729a3b91c8734dc6e94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb1da06817d4adeae0c0b777596706f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a7fb3f1296542bcb34e51dbedca929c",
              "IPY_MODEL_5746d68c6e324cfcbcd02c4b26d44138",
              "IPY_MODEL_da90c785ffce4a8c90b47ec8b2367c7d"
            ],
            "layout": "IPY_MODEL_66db49cef7644066ac851f42b3d855f8"
          }
        },
        "da90c785ffce4a8c90b47ec8b2367c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0ccb486b2604e24a718eeb7e7551467",
            "placeholder": "​",
            "style": "IPY_MODEL_a3d1a2bd3fde49139935cdde49bcf6a2",
            "value": " 7247/7247 [00:01&lt;00:00, 6369.39 examples/s]"
          }
        },
        "dff5b5617dd3426e98743ab828517a39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e45cfe61e04545c69218d7dda297944b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9d442ee43e647169cbf4f862ebfeb9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9f7c216ddff49609c41c777ece24742": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4aa1d7e657f1483f990e5116c3b0b8a2",
              "IPY_MODEL_a71d6e6c71694cbba52dfb3cb5d9f00b",
              "IPY_MODEL_edc366d9a09d45688bbe4bf176db7f12"
            ],
            "layout": "IPY_MODEL_e45cfe61e04545c69218d7dda297944b"
          }
        },
        "edc366d9a09d45688bbe4bf176db7f12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d61ae0b338048a2979ce5ba42613528",
            "placeholder": "​",
            "style": "IPY_MODEL_a5fc78d74ac946a6bd85f14cdff3fd16",
            "value": " 106557/106557 [00:17&lt;00:00, 6266.74 examples/s]"
          }
        },
        "f7d02bc9e0e74cb49e9ec3e390e1814a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d932962f1df4cb78560ff2909a7e53d",
            "max": 5939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3334eb1e16604ff0a8c28c6ed38dfd9a",
            "value": 5939
          }
        },
        "9ec95d5a6cf24809b6ae7fb85e5a3f99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42a3e3570c384ba6a91ea56d8a99b751",
              "IPY_MODEL_bc5eabe736534126909b208a693f1a10",
              "IPY_MODEL_c80f5f234a2c4a37b2f9b430dd02fdb7"
            ],
            "layout": "IPY_MODEL_9b7133276d1e4dbfb1803bfbd83fe5e4"
          }
        },
        "42a3e3570c384ba6a91ea56d8a99b751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b05afb6f0b15442fac736afa9410da88",
            "placeholder": "​",
            "style": "IPY_MODEL_666c21662ae94824b6242d5af12d44c1",
            "value": "Filter: 100%"
          }
        },
        "bc5eabe736534126909b208a693f1a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68ea2cbad2db4dfd9528a4f317cdec01",
            "max": 106557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_606732276e90460fabf45d4ec54613b0",
            "value": 106557
          }
        },
        "c80f5f234a2c4a37b2f9b430dd02fdb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b720af7d88d46e5ad406b547759eba2",
            "placeholder": "​",
            "style": "IPY_MODEL_df95bb6035e94182935681acf9a93be3",
            "value": " 106557/106557 [00:55&lt;00:00, 1728.07 examples/s]"
          }
        },
        "9b7133276d1e4dbfb1803bfbd83fe5e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b05afb6f0b15442fac736afa9410da88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "666c21662ae94824b6242d5af12d44c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68ea2cbad2db4dfd9528a4f317cdec01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "606732276e90460fabf45d4ec54613b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b720af7d88d46e5ad406b547759eba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df95bb6035e94182935681acf9a93be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75023618498a44329e7a38eaa9772bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3e9078cdad44d2a8a7f8a6cef5c4458",
              "IPY_MODEL_eb7e7af87c2b4c0f91ec6bc95632b6f3",
              "IPY_MODEL_ff3a8a39a5c34daa929e669162fa4d65"
            ],
            "layout": "IPY_MODEL_13386ffd2b4e4d38803536d66d7d6b3d"
          }
        },
        "f3e9078cdad44d2a8a7f8a6cef5c4458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8abea99cbac644c3876b45ad50b5f11c",
            "placeholder": "​",
            "style": "IPY_MODEL_cabdab99c5ed417c846b595a5c1094d6",
            "value": "Filter: 100%"
          }
        },
        "eb7e7af87c2b4c0f91ec6bc95632b6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c28490aa7ceb40a8a936d54453e600ce",
            "max": 5939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f74e1901adb4bf28130214976670467",
            "value": 5939
          }
        },
        "ff3a8a39a5c34daa929e669162fa4d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e252551667f44ae580364a9a7d4ace32",
            "placeholder": "​",
            "style": "IPY_MODEL_60391b6e40d4428f9c460a4fade75ea1",
            "value": " 5939/5939 [00:02&lt;00:00, 2341.30 examples/s]"
          }
        },
        "13386ffd2b4e4d38803536d66d7d6b3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8abea99cbac644c3876b45ad50b5f11c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cabdab99c5ed417c846b595a5c1094d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c28490aa7ceb40a8a936d54453e600ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f74e1901adb4bf28130214976670467": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e252551667f44ae580364a9a7d4ace32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60391b6e40d4428f9c460a4fade75ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "288fac6a7d034e76a5da608797d0fb6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa9164f1428049b5b9bc03ed4a29eb95",
              "IPY_MODEL_5874e02e0c7b4d23a56c1ffc6e72fbcf",
              "IPY_MODEL_56888d59d0174cf9a8b2122effa52bcc"
            ],
            "layout": "IPY_MODEL_ec94041036ee4c409fcdccfe9ad79884"
          }
        },
        "aa9164f1428049b5b9bc03ed4a29eb95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88165582cf674f7893c9cc78b0ce0401",
            "placeholder": "​",
            "style": "IPY_MODEL_58d710e8f7d74ac0bc175af2a00dc698",
            "value": "Filter: 100%"
          }
        },
        "5874e02e0c7b4d23a56c1ffc6e72fbcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b3e03685ae54ddab2757f760dfb76e2",
            "max": 7247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e6a791e7c0b4db38cf3b0b6779a7ab9",
            "value": 7247
          }
        },
        "56888d59d0174cf9a8b2122effa52bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c10f27cef72444f9ddc9d1eec135cd2",
            "placeholder": "​",
            "style": "IPY_MODEL_be39eae6a08e45afbebbdc8d56cdcc06",
            "value": " 7247/7247 [00:04&lt;00:00, 1794.31 examples/s]"
          }
        },
        "ec94041036ee4c409fcdccfe9ad79884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88165582cf674f7893c9cc78b0ce0401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58d710e8f7d74ac0bc175af2a00dc698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b3e03685ae54ddab2757f760dfb76e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e6a791e7c0b4db38cf3b0b6779a7ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c10f27cef72444f9ddc9d1eec135cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be39eae6a08e45afbebbdc8d56cdcc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42489c6d5e3c428eac79ad465d0dd970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bdb9b1ea66643498c5cfe76864b8c1e",
              "IPY_MODEL_03a49488cd684ccaa0244031a110f2d3",
              "IPY_MODEL_df512da945fa4e1d87f1c7c7c08ceddc"
            ],
            "layout": "IPY_MODEL_c022cd1feeda480db3055be21f2671d1"
          }
        },
        "5bdb9b1ea66643498c5cfe76864b8c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0fbbe5c878d4e89a66269a1572fc899",
            "placeholder": "​",
            "style": "IPY_MODEL_10fcf3a0aa63491a90023d63d6bced19",
            "value": "Filter: 100%"
          }
        },
        "03a49488cd684ccaa0244031a110f2d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27d1eb662b25400d9b89027bbe49f38f",
            "max": 106557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf2771a8882f45d0b239ac79b582c49b",
            "value": 106557
          }
        },
        "df512da945fa4e1d87f1c7c7c08ceddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_364a5b1389bd4649a63f8f6ae2d7c297",
            "placeholder": "​",
            "style": "IPY_MODEL_f49e4ee8209f4b0f86022f262fff27a9",
            "value": " 106557/106557 [00:42&lt;00:00, 3276.39 examples/s]"
          }
        },
        "c022cd1feeda480db3055be21f2671d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0fbbe5c878d4e89a66269a1572fc899": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10fcf3a0aa63491a90023d63d6bced19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27d1eb662b25400d9b89027bbe49f38f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf2771a8882f45d0b239ac79b582c49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "364a5b1389bd4649a63f8f6ae2d7c297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49e4ee8209f4b0f86022f262fff27a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2190899e0f4c48ae8bcb2674a93e66c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dce546672faf4e218518fba3d02cf321",
              "IPY_MODEL_d48eb69797f84e9185e54b3bb7022cdd",
              "IPY_MODEL_e2a3f56aa92343dcb52f3d073ede3d47"
            ],
            "layout": "IPY_MODEL_d5546eb8d76f4573a6921e4148e42c1b"
          }
        },
        "dce546672faf4e218518fba3d02cf321": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ce796f7c9cd4a9ca1f9c4c353fc64c0",
            "placeholder": "​",
            "style": "IPY_MODEL_27669cf20b484194aa5220b37613b0dc",
            "value": "Filter: 100%"
          }
        },
        "d48eb69797f84e9185e54b3bb7022cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a9c3c12236c4b73b2859c02268a010a",
            "max": 5939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_717f7e2573aa42e787a366091d3350e0",
            "value": 5939
          }
        },
        "e2a3f56aa92343dcb52f3d073ede3d47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44e834ca788248778048b46df0e0b95e",
            "placeholder": "​",
            "style": "IPY_MODEL_620757e2cd0f4be4a67019bd625886d5",
            "value": " 5939/5939 [00:01&lt;00:00, 3083.54 examples/s]"
          }
        },
        "d5546eb8d76f4573a6921e4148e42c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ce796f7c9cd4a9ca1f9c4c353fc64c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27669cf20b484194aa5220b37613b0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a9c3c12236c4b73b2859c02268a010a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "717f7e2573aa42e787a366091d3350e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44e834ca788248778048b46df0e0b95e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "620757e2cd0f4be4a67019bd625886d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e793cefdd32d4564aaa3b91c9519c9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43aa35559c34432fb0d9e89eb0982ab5",
              "IPY_MODEL_b26d952ba80b4bb9a6fbdbea8643dc13",
              "IPY_MODEL_99603bda2be7404982ebb3d8fab91fc6"
            ],
            "layout": "IPY_MODEL_a62baebee6c04deaa0c1b6910b6a5c13"
          }
        },
        "43aa35559c34432fb0d9e89eb0982ab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e86bc23ebd724ec1ab63aaffb1ee4ade",
            "placeholder": "​",
            "style": "IPY_MODEL_22e8157835994bf5ab79beada908ef0a",
            "value": "Filter: 100%"
          }
        },
        "b26d952ba80b4bb9a6fbdbea8643dc13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b72e4ad90ed14b23882e17e3c5ea0e19",
            "max": 7247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ef56e1c98f647fc9ba384f46420b963",
            "value": 7247
          }
        },
        "99603bda2be7404982ebb3d8fab91fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c33056afcac74799a427aa0181d6ba4f",
            "placeholder": "​",
            "style": "IPY_MODEL_564509a103f8467d9ac8237c9b546165",
            "value": " 7247/7247 [00:02&lt;00:00, 3063.93 examples/s]"
          }
        },
        "a62baebee6c04deaa0c1b6910b6a5c13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e86bc23ebd724ec1ab63aaffb1ee4ade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22e8157835994bf5ab79beada908ef0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b72e4ad90ed14b23882e17e3c5ea0e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ef56e1c98f647fc9ba384f46420b963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c33056afcac74799a427aa0181d6ba4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564509a103f8467d9ac8237c9b546165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ae676caf3524c1d9a10c49dbe83fbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4abbd932ee124462b6fc77236667ea54",
              "IPY_MODEL_1e230e44421d4c7796a09a8bd2d3daf7"
            ],
            "layout": "IPY_MODEL_f185b14a21ad4ff5b0560f6545b20d6d"
          }
        },
        "4abbd932ee124462b6fc77236667ea54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f95e69df31243eab1d881944b6ca7e3",
            "placeholder": "​",
            "style": "IPY_MODEL_b001d09cfd2341fcba7272b5dfbbb3fc",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "1e230e44421d4c7796a09a8bd2d3daf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc586e9bbf5e436f899a5408a14f35a8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8148a9b2cf9450aa7461c1cbab41fa6",
            "value": 1
          }
        },
        "f185b14a21ad4ff5b0560f6545b20d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f95e69df31243eab1d881944b6ca7e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b001d09cfd2341fcba7272b5dfbbb3fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc586e9bbf5e436f899a5408a14f35a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8148a9b2cf9450aa7461c1cbab41fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "173d52575d2f42f08c808846ace321cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bcb6a39cfcb947d2bddd7772660aad99",
              "IPY_MODEL_981be7d8d9ef40e0b5483979065deace",
              "IPY_MODEL_694770111fda4b50861a5e8723d02ef3"
            ],
            "layout": "IPY_MODEL_084c7f98c36c4aa19b1f7dfe9dd3cd1a"
          }
        },
        "bcb6a39cfcb947d2bddd7772660aad99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32a0b3690234e6ba0bdb7779b801f67",
            "placeholder": "​",
            "style": "IPY_MODEL_a9b188d7b9a34e0e88db6c0ae0c79bf5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "981be7d8d9ef40e0b5483979065deace": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a91315a01764109a7014c70690bb463",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_743bf494d11845a7968d7550e2b14b2f",
            "value": 2
          }
        },
        "694770111fda4b50861a5e8723d02ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c5e9325bf52435da9fd3c77bafb5503",
            "placeholder": "​",
            "style": "IPY_MODEL_1665505e7880470fb90ccec3182f147a",
            "value": " 2/2 [00:11&lt;00:00,  5.23s/it]"
          }
        },
        "084c7f98c36c4aa19b1f7dfe9dd3cd1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b32a0b3690234e6ba0bdb7779b801f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9b188d7b9a34e0e88db6c0ae0c79bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a91315a01764109a7014c70690bb463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "743bf494d11845a7968d7550e2b14b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c5e9325bf52435da9fd3c77bafb5503": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1665505e7880470fb90ccec3182f147a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4ea089f48f2455ca6f4c2625343afb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e89f9f54ae754db0b6d5a99f165c24a4",
              "IPY_MODEL_8ff2f94113af4c7491675b3745f74e9f",
              "IPY_MODEL_96361f99d2684ac9b25a8e7488e581cd"
            ],
            "layout": "IPY_MODEL_bae2607fc3f0445b89f8c9bb9a9807b8"
          }
        },
        "e89f9f54ae754db0b6d5a99f165c24a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39aedbfcf5ad4083b662d43a75dcaef8",
            "placeholder": "​",
            "style": "IPY_MODEL_384b2ee5d6904acd8cd3491682554a8e",
            "value": "Map: 100%"
          }
        },
        "8ff2f94113af4c7491675b3745f74e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76c0cfed990245d1937db3619189bc3e",
            "max": 4991,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f98c5b11f31a4122b58f20fe953e9682",
            "value": 4991
          }
        },
        "96361f99d2684ac9b25a8e7488e581cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c677aa4ed00b469bbf571042390f300a",
            "placeholder": "​",
            "style": "IPY_MODEL_85e831e7c1964dca992b9fa3a52ceff7",
            "value": " 4991/4991 [00:00&lt;00:00, 7105.75 examples/s]"
          }
        },
        "bae2607fc3f0445b89f8c9bb9a9807b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39aedbfcf5ad4083b662d43a75dcaef8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "384b2ee5d6904acd8cd3491682554a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76c0cfed990245d1937db3619189bc3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f98c5b11f31a4122b58f20fe953e9682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c677aa4ed00b469bbf571042390f300a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e831e7c1964dca992b9fa3a52ceff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}