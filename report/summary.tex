\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{hhline}
%SetFonts

\usepackage{multirow}
\usepackage{booktabs}

\usepackage{multirow}
\usepackage{tabularx}
\usepackage{ragged2e}

%SetFonts


\title{Training Llama-2 on Simple, Safe Text}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\section{Introduction}

In this report we'll summarize the main results we have obtained on training a Llama-2 model to produce safe and simple text.
Please feel free to add the results of any experiments. 

\section{Dataset Analysis}



\subsection{Filtering}
\subsection{EDA}
\section{Model and Training}
\section{SFT Inference Results}
In this section we will summarize the results of Llama-2 models fine-tuned using supervised fine-tuning.

\subsection{HuggingFace Leaderboard}
First I'll summarize the results of our fine-tuned models versus the Meta-Llama models on the Huggingface Open LLM leaderboard.
The leaderboard is a wrapper for the ``Eleuther AI Language Model Evaluation Harness". 
Specifically, the leaderboard measures the 25-shot performance of LLMs on the arc-challenge dataset, the 10-shot performance on the HellaSwag dataset, the 0-shot performance on TurthfulQA, and the 5-shot performance on MMLU. For arc-challenge and HellaSwag performance is measured using the acc$\_$norm metric of EleutherAI, for TruthfulQA they use the mc2 metric, and for MMLU they average the accuracy of the model across tasks in the MMLU dataset.
The nice thing about the leaderboard is one can submit either the full model or submit just the adapter layers and give the base model separately.
The problem is, sometimes models disappear from the leaderboard and I'm not sure why.
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Model &          Average & Arc & HellaSwag & MMLU & TruthfulQA \\
\hline
Llama-2-7b&                  53.40               &   53.07            & 77.74              &  43.80                &  38.98 \\
Llama-2-7b-chat  &          \textbf{56.34} &  52.90              & 78.55             &  \textbf{48.32}    &  \textbf{45.57} \\
Llama-2-7b-eli5  &           53.92             &  53.41              & 77.90             &  43.56                 & 40.81\\
Llama-2-7b-wiki &           53.72              &  \textbf{54.35} & 78.06             &  45.35                 & 37.11\\
Llama-2-7b-eli5-wiki&    55.46               &  53.75              & \textbf{78.76}&  46.02                   & 43.31\\
\hhline{|=|=|=|=|=|=|}
Llama-2-13b &               56.90              &   58.11             & 80.97            &  54.34            & 34.17\\
Llama-2-13b-chat&        59.93              & 59.04               & 81.94            &  54.64            & \textbf{44.12}\\
Llama-2-13b-eli5&          \textbf{60.61} &  \textbf{60.41}  & \textbf{82.58}&  \textbf{55.86}& 43.61\\
Llama-2-13b-wiki&	        58.12             &  59.04               & 82.33            &  55.36            & 35.75\\
Llama-2-13b-eli5-wiki&   59.43             &  59.98               & 82.43            &  55.41            & 39.90 \\
\hline
\end{tabular}
\end{center}
\caption{Results for Llama-2 models on Huggingface Open LLM Leaderboard}.
\label{table:hfleaderboard}
\end{table}
Here Llama-2-7b is the base 7 billion model and Llama-2-7b-chat is the Llama-2 model which has undergone RLHF.
The remaining models are fine-tuned versions of the base-model. 
Specifically, they are trained using SFT and QLORA on the ELI5 SFT dataset, the Simple Wikipedia Instruct dataset, or their combination. The second half of the table is the same, except for the 13B parameter model.

When defining the model there are subtleties about how to merge the LoRA adapter weights with the rest of the model.
The subtlety arises because in QLoRA we quantize the base model to 4-bit, but need to dequantize these weights to bfloat16 when performing back-propagation for the LoRA adapter layers.
When we merge the adapter layers with the base model we have two natural options: either quantize the model to bfloat16 and then merge or quantize the model to 4-bit, dequantize to bfloat16, and then merge.
The second option is arguably more natural since during training we are quantizing and then dequantizing, so we want the model at inference to be as close as possible to the model during training.
However, when we quantize and then dequantize we risk losing precision and degrading the model in the process.
It is not clear which choice is better and this likely depends on how strong of an effect the LoRA layers have and how sensitive they are to the exact form of weights. 
On automated benchmarks we have not seen one method give reliably better results than the other.

All that said, to get the above results we quantized and dequantized the 7B model before merging, while for the 13B model we quantized the model to bfloat16 and then merged. 
For the 7B model we could perform the quantization and dequantization on a 40GB A100.
For the 13B model we directly submitted the adapter layers to the HuggingFace leaderboard and used the ungated Llama-2-7b-hf model from NousResearch (the Meta-Llama model is gated and although it can be downloaded, we were not able to use it as a base model on the leaderboard).

In Table \ref{table:hfleaderboard} we see that of the 7B models, the Meta-Llama-2-7b-chat performs the best on average with the Llama-2 model trained on ELI5 and Simple Wikipedia performing the second best.
One surprising thing is the chat model actually performs worse than the base model on the Arc-Challenge dataset, and here the model trained on just Simple Wikipedia QA pairs performs the best. 
On the HellaSwag dataset the Llama-2-7B model trained on ELI5 and Simple Wikipedia marginally outperforms the chat model, but the difference is likely too small to be statistically significant.
Finally, on MMLU and TruthfulQA the 7B chat model performs significantly better than the other models.

Once we go up to 13B parameters we see that the Llama-2 model trained on the ELI5 SFT dataset performs the best on average with the 13B chat model and 13B model fine-tuned on ELI5 + Simple Wikipedia close behind.
It is surprising that the model trained on just ELI5 model performs the best, and this result is driven primarily by its improved performance on the TruthfulQA dataset. 
On the other datasets it barely improves over the model trained on the combined ELI5 and Simple Wikipedia dataset.
We are not sure what the cause of this effect is, somehow training the model more on Reddit data makes the model more honest!
This result could also be an artifact of a poor choice of hyperparameters, and perhaps with a different learning rate and/or after averaging over initializations the difference would go away or the model trained on the combined dataset would perform better.

\subsection{ROUGE and BERTScore}
In this section we will investigate how supervised fine-tuning effects the models ROUGE and BERTScores.
ROUGE is a well-known automated benchmark that measures n-gam overlap between generated text and the reference text.
Since it just looks at n-gram overlaps, and does not take into account semantic content, ROUGE is effectively measuring to what extent the trained model is adopting the vocabulary of the reference text.
One advantage of BERTScore is it uses pre-trained encoder models, such as BERT or RoBERTa, to encode the generated and reference text in some high-dimensional vector space and then measures the cosine-similarity between the two vectors.
Of course, this also means that BERTScore is more computationally intensive to compute.

In the tables below we present the ROUGE and BERTScore metrics for the Llama-2 base model, as well as our fine-tuned models, on the validation sets for our three datasets: ELI5, Simple Wikipedia, and combined dataset.
We include the original Llama-2 model as a baseline to measure how much fine-tuning changes the Rouge and BERTScores.
In addition, we also include ``off-diagonal" elements, where we train a model on one dataset and measure its ROUGE and BERTScores on the validation split of a different dataset.
For example, we include cases where we train the model on the ELI5 SFT dataset and then evaluate it on the validation split of the Simple Wikipedia QA dataset.
We included these results to serve as additional baselines to see to what extend fine-tuning on \textit{any} QA dataset changes the evaluation metrics.
In all cases, the model produces at most 256 new tokens.

\begin{table}[htbp]
\centering
\begin{tabular}{cccccc}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{rouge1} & \textbf{rouge2} & \textbf{rougeL} & \textbf{rougeLsum} \\
\midrule
\multirow{4}{*}{ELI5} & Llama-2-7b & \textbf{0.3796} & \textbf{0.2432} & \textbf{0.3000} & \textbf{0.3222} \\
& Llama-2-7b-ELI5 & 0.3701 & 0.2140 & 0.2736 & 0.2821 \\
& Llama-2-7b-wiki & 0.3575 & 0.2083 & 0.2660 & 0.2762 \\
& Llama-2-7b-ELI5-wiki & 0.3702 & 0.2126 & 0.2733 & 0.2819 \\
\midrule
\multirow{4}{*}{wiki} & Llama-2-7b-hf & 0.1923 & 0.0103 & 0.0937 & \textbf{0.1392} \\
& Llama-2-7b-ELI5 & \textbf{0.2203} & \textbf{0.0125} & \textbf{0.0964} & 0.1271 \\
& Llama-2-7b-wiki & 0.1826 & 0.0073 & 0.0879 & 0.1165 \\
& Llama-2-7b-ELI5-wiki & 0.1811 & 0.0076 & 0.0885 & 0.1153 \\
\midrule
\multirow{4}{*}{full} & Llama-2-7b & 0.1944 & 0.0087 & 0.0905 & \textbf{0.1400} \\
& Llama-2-7b-ELI5 & \textbf{0.2243} & \textbf{0.0118} & \textbf{0.0971} & 0.1312 \\
& Llama-2-7b-wiki & 0.1905 & 0.0079 & 0.0889 & 0.1198 \\
&Llama-2-7b-ELI5-wiki & 0.1907 & 0.0080 & 0.0894 & 0.1193 \\
\bottomrule
\end{tabular}
\caption{Rouge Scores}
\label{tab:rouge_scores}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{ccccc}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
\multirow{4}{*}{eli5} & Llama-2-7b & \textbf{0.8429} & 0.8754 & 0.8583 \\
& Llama-7b-ELI5 & 0.8372 & \textbf{0.8799} & 0.8575 \\
& Llama-2-7b-wiki & 0.8415 & 0.8796 & \textbf{0.8598} \\
& Llama-2-7b-ELI5-wiki & 0.8399 & 0.8798 & 0.8590 \\
\midrule
\multirow{4}{*}{wiki} & Llama-2-7b & \textbf{0.7879} & 0.8067 & 0.7970 \\
& Llama-7b-ELI5 & 0.7859 & \textbf{0.8093} & \textbf{0.7971} \\
& Llama-2-7b-wiki & 0.7782 & 0.8019 & 0.7898 \\
& Llama-2-7b-ELI5-wiki & 0.7783 & 0.8014 & 0.7896 \\
\midrule
\multirow{4}{*}{full} & Llama-2-7b & \textbf{0.7940} & 0.8092 & 0.8014 \\
& Llama-7b-ELI5 & 0.7933 & \textbf{0.8116} & \textbf{0.8022} \\
& Llama-2-7b-wiki  & 0.7861 & 0.8048 & 0.7952 \\
& Llama-2-7b-ELI5-wiki & 0.7841 & 0.8046 & 0.7941 \\
\bottomrule
\end{tabular}
\caption{Precision, Recall, and F1 BERTScores}
\label{tab:scores}
\end{table}

It's difficult to interpret or make sense of these results, somehow the base Llama-2 model often outperforms the fine-tuned models and the model trained on just the ELI5 dataset often performs very well on the Simple Wikipedia validation set!
It is probably best to take these numbers with a grain of salt. The BERTScore metrics tend to differ by very small amounts, e.g. the difference in F1 scores between the base model and the model trained on ELI5 for the Simple Wikipedia validation set differ by just 0.001.
In addition, we of course do not know on what data the original Llama-2 model was trained and if there is data leakage.
Finally, long-form question answering remains a difficult task to evaluate given the diversity of topics and possible answers a given question may have!

\subsection{MT(S)-Bench}



\end{document}  