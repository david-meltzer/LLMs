\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps -> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{hhline}
\usepackage{amsmath}
%SetFonts

\usepackage{multirow}
\usepackage{booktabs}

\usepackage{multirow}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage[colorlinks]{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor  = blue,citecolor = blue}

%SetFonts


\title{Training LLMs on Simple, Safe Text}
%\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle
\tableofcontents
\section{Introduction}

In this project we will explore how to build a LLM that answers questions in a simple and safe ($S^{2}$) way.
That is, we want an LLM which answers questions in a way which is understandable to schoolchildren and/or adults who are learning English while also not producing any potentially toxic or offensive content.
To train the LLM we will use a combination of imitation learning and preference modeling.
The two main algorithms we will explore are Reinforcement Learning from Human Feedback (RLHF) \cite{ouyang2022training,bai2022training} and Direct Preference Optimization (DPO) \cite{rafailov2023direct}. 

In the remainder of this introduction we will briefly review these methods.
We will attempt to be light on the mathematical details of the algorithms and instead focus on the overall challenges and benefits of each approach.
 
 \subsection{RLHF}
RLHF is the most well-known method used to align LLMs with human interests and is behind the success of chatbots such as ChatGPT and Claude.
RLHF is roughly given by the following three step procedure:
\begin{enumerate}
\item We start by training the model using supervised fine-tuning (SFT). At this step we simply prompt the model with a question in the form ``\#\#\# Human: \{question\} \#\#\#Assistant:" and ask the model to autoregressively generate new text. This is called supervised fine-tuning because we have a reference human-written answer and want to minimize the KL-divergence between the reference text and the model generated text. Despite its name, SFT is essentially the same as pretraining, except we do not compute the loss on the prompt.

\item Next, we perform reward modeling (RM), which is a preference modeling task.
In this step, we take another neural net and present it with a question and two answers, one of which is labelled ``chosen" and the other one is labelled ``rejected".
At this step, the goal is to teach the model to identify which answer is preferred over the other, i.e. to learn human preferences via sequence classification. 
Typically the reward model is taken to be the LLM from step (1), but we replace the head of the SFTed model with a layer to compute the reward function $r(Q,A)$ and train the model with a loss function such that the model assigns higher rewards to the chosen answer and lower rewards to the rejected answer. 

\item Finally, we use the reward model from step (2) to train the SFTed model from step (1) using reinforcement learning.
Specifically, the model is trained using Proximal Policy Optimization (PPO).
At this step it helps to add a term to the reward modeling objective proportional to the KL-divergence between the SFT model we are training and the original SFT from step (1).
The addition of a KL penalty helps ensure that the model trained with RL does not deviate too strongly from the original model and produce low-quality text that simply ``hacks" the reward model.
\end{enumerate}

To be more precise, during step (2) we typically assume that human preferences are captured by the Bradley-Terry model:
\begin{align}
p^{*}(y_1\succ y_1|x)=\frac{e^{r^{*}(x,y_1)}}{e^{r^{*}(x,y_1)}+e^{r^{*}(x,y_2)}}
\end{align}
where $r^{*}$ is the gold reward model.
To find the optimal reward model we can minimize the following loss function:
\begin{align}
\mathcal{L}(r_{\phi},\mathcal{D}) = - \mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\left[\log(\sigma(r_{\phi}(x,y_w)-r_{\phi}(x,y_l)))\right]
\label{eq:reward_optimization}
\end{align}
where $\phi$ are the parameters of the reward model, $y_w$ and $y_l$ are the chosen/rejected answers, respectively, $\mathcal{D}$ is our dataset, and $\sigma$ is the logistic function. 

During the RL step the goal is to find a policy (LLM) $\pi_{\theta}$ which solves the following optimization problems:
\begin{align}
\pi_{\theta^{*}} = \text{argmax}_{\pi_{\theta}}\mathbb{E}_{x\sim \mathcal{D},y\sim \pi_{\theta}(y|x)}[r_{\phi}(x,y)]-\beta \mathcal{D}_{KL}[\pi_{\theta}(y|x)||\pi_{\text{\text{ref}}}(y|x)]
\label{eq:constrained_optimization}
\end{align}
where $r_{\phi}$ is the previous learned reward model, $\mathcal{D}_{KL}$ is the KL-divergence, and $\pi_{\text{\text{ref}}}$ is the SFT model from step (1).
The parameter $\beta$ is a hyperparameter which controls how much we penalize deviations from the original reference model.
The optimal policy is then found using PPO.

RLHF is clearly a non-trivial algorithm to implement in practice. 
The simplest step to implement is the SFT since the training objective is essentially identical to the pretraining objective. 
In addition, if we have a dataset of high enough quality, we arguably do not need to perform many steps of supervised fine-tuning, e.g. in the LIMA paper they achieved remarkable after only training the model with SFT on 1000 examples!

The second step, reward modeling, is simple to implement in principle, but in practice can be very subtle and difficult to get right.
For one, we are trying to teach a model what human preferences are by simply ranking two answers.
This may be too rough of a metric to actually determine human preferences, e.g. what answer is preferred over the another may depend on how the question is written and who the intended audience is.
In addition, what answer is considered the ``best" will be biased by the preferences of whoever is ranking the answer, and their preferences may not be universal.
%On internet sites, such as Reddit, one answer may also receive more votes than another simply because it appeared earlier, and not because it is of a genuinely higher quality.
In theory, it may also be better to have multiple reward models to judge different aspects of an answer, as opposed to having one reward model which is supposed to determine the quality of the answer in full.\footnote{We may want separate reward models to separately judge the simplicity, helpfulness, and harmfulness of the answer.}
However, despite its simplicity, learning human preferences by ranking answers has proven remarkably successful and is also a natural starting point for solving the alignment problem.
%All that being said, one reason why training a reward model on data from sites such as Reddit or StackExchange is useful is because actually getting human preference data is non-trivial and these sites give us access to both a wide scope of data on which we can get started.
%In addition, by using the ``Wisdom of the crowd" of users who vote on answers, we can potentially avoid biasing the data by the preferences of a small group of labellers (of course the demographics of users on these sites is also biased towards certain groups and this should also be taken into account).

To successfully perform reward modeling, we also need to ensure that our preference dataset is of a high-enough quality such that we can use our trained reward model during the reinforcement learning step.
The general wisdom currently is that the reward model needs to be achieving accuracies of around $70\%$ (and ideally higher) before we can use it for reinforcement learning.
The fact these accuracies are fairly low is a testament to the fact that reward modeling by itself is a non-trivial task!
We also need to ensure the preference modeling dataset is of a wide enough scope such that the reward model knows how to properly judge answers that are generated during reinforcement learning.
For example, a model trained only to judge answers about math may not accurately judge answers about other topics, such as history or politics.

The final step of RLHF, implementing PPO, is more technically challenging than the previous steps.
The PPO algorithm requires that we have a value function, a reward model, the policy function (the LLM currently being trained), and the initial SFTed model (which serves as our reference model). 
Therefore, during PPO we have to keep four separate models in models, and they are all LLMs!
For this reason, the PPO step can be fairly memory intensive.
To avoid out-of-memory errors, and also spending too much money on GPUs, we either need to use relatively small LLMs and/or use parameter efficient methods such as QLoRA.
In addition, it is well-known that training a model with RL is more difficult than training a model using supervised learning and that RL algorithms tend to be more sensitive to hyperparameters and choices of initialization.
For this reason, we RL typically requires more hyperparameter optimization than supervised or unsupervised learning.

\subsection{DPO}
DPO is a newer algorithm designed to achieve the same objective as RLHF using only supervised learning.
DPO is a supervised learning algorithm that optimizes the same objective as RLHF, except without needing to train a separate reward model (hence the subtitle of the paper, ``Your Language Model is Secretly a Reward Model") and without needing to perform reinforcement learning.
To do this, they use an exact map between reward functions to optimal policies such that the loss function on the reward model becomes a loss function on the policy model.
Therefore, step (2) of the RLHF algorithm, performing supervised learning on the reward model, turns into a supervised learning problem on the policy function directly, which in our case is the SFTed LLM.

DPO is therefore a two step procedure:
\begin{enumerate}
\item Perform supervised fine-tuning on a pre-trained LLM in the same exact way as step one of RLHF.
\item Take the model from step (1) and train it to learn human preferences directly using the DPO loss function.  
\end{enumerate} 

To proof that the DPO is equivalent to RLHF, they first show that the optimal policy which satisfies equation \ref{eq:constrained_optimization} is:
\begin{align}
\pi_{r}(y|x)=\frac{1}{Z(x)}\pi_{\text{ref}}(y|x)e^{\frac{r(x,y)}{\beta}}
\end{align} 
where the partition function is:
\begin{align}
Z(x)=\sum\limits_{y}\pi_{\text{ref}}(y|x)e^{\frac{r(x,y)}{\beta}}.
\end{align}
After making this identification, the reward modeling step, equation \ref{eq:reward_optimization}, becomes equivalent to minimizing the following loss function:
\begin{align}
\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}}) = -\mathbb{E}_{(x,y_w,y_l)}\sim\mathcal{D}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_{\text{ref}}(y_w|x)}-\beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]
\end{align}

DPO is overall a simpler algorithm to implement and use since it just involves supervised learning.
For this reason, we will start by using DPO to align our LLMs and use the results from DPO as a benchmark for RLHF. 
In the original DPO paper it was also shown that DPO achieves comparable or better results than RLHF, so we expect DPO will serve as a strong benchmark for RLHF, or any other algorithm to align LLMs. 

Are there any downsides to DPO?
One potential downside, given its current incarnation, is that there may be cases where we want a separate reward model.
This is brought up in the DPO paper, where they mention that having a separate reward model may be useful to label currently unlabelled prompts, and it is less clear if performing self-labelling using the LLM (which is also being trained) is equally effective.
In addition, as brought up earlier, it may be useful to have multiple reward models to capture different aspects of an answer, e.g. helpfulness vs simplicity. 
This theoretically can be implemented in RLHF by simply training more reward models, but it is not clear how to do this using DPO directly since in DPO our LLM is also the reward model.
Finally, like most implementations of RLHF, the DPO algorithm assigns a single score to an entire answer and does judge individual parts of the answer.

\section{Datasets}

\subsection{Sources}
The two sources of our datasets are the ELI5 dataset and Simple Wikipedia.
In this section we will go into detail on the form of these datasets and in the next section we will explain how we filtered them. 

\subsubsection*{ELI5}

The ELI5 dataset gets its name from the subreddit Explain Like I'm Five (r/ELI5) on reddit.com.
The stated goal of the ELI5 subreddit is to provide a place where people can ask questions on a wide variety of topics and receive a layperson-friendly explanation.
Commentators are requested to answer a question without assuming ``knowledge beyond a typical secondary education program" and in general to keep answers clear and simple.
The ELI5 dataset also contains question/answer pairs from the r/AskHistorians and r/AskScience subreddits. The r/AskHistorians is well-known to be a well-moderated community where answers are particularly in-depth and clear. 
Low quality answers, e.g. ones that do not answer the original question or do not properly cite sources, are typically removed.
The quality of posts on r/AskScience is also high and users generally receive clear answers to fairly technical questions, although the moderation appears to be less strict than on r/AskHistorians.
We will use the ELI5 dataset to train a question/answer model by using the title and body of the post as the question and highly-voted comments as the answers.

The original ELI5 dataset was created by researchers at Facebook \cite{fan-etal-2019-eli5} and the original ELI5 dataset can be found \href{https://huggingface.co/datasets/eli5}{here}.
However, one potential issue with this dataset is that there is a large amount of data leakage between the train/validation/test splits \cite{krishna2021hurdles,mahapatra2021new}.
For this reason, we will use a cleaned version of the ELI5 dataset which was constructed by the second group and can be found \href{https://huggingface.co/datasets/vblagoje/lfqa}{here}.
This dataset will also require some cleaning due to data leakage, but overall its cleaner than the previous version of the ELI5 dataset.


\subsubsection*{Wikipedia}
\href{https://simple.wikipedia.org/wiki/Main_Page}{Simple Wikipedia} is a version of Wikipedia where users are encouraged to write articles using basic English.
On the Simple Wikipedia homepage they write: ``The Simple English Wikipedia is for everyone, such as children and adults who are learning English."
They also encourage users to write shorter sentences, while not necessarily requiring that the articles also be short.
In particular, users are told to write articles using simple words and grammar, but to not necessarily only use ``basic information".
Unlike the ELI5 dataset, we cannot use the Simple Wikipedia dataset out of the box to train a question-answering model since the articles are not written in that style.
Instead, we will use GPT-3.5 to generate questions whose answer is contained in the first few paragraphs of the Simple Wikipedia article \cite{koksal2023longform}. 
Specifically, we give GPT-3.5 the following system message and prompt:
\\[5pt]
\indent \textbf{System Message}: ``You are a helpful assistant that generates questions from text."
\indent \textbf{Prompt}: ``Question: X
\newline
\indent\indent\indent\indent Answer: \{answer\} 
\\
\indent\indent\indent\indent What kind of question, X, could this be an answer to?\newline \indent\indent\indent\indent X:"
\\[5pt]
Here the text in \{answer\} is the first few paragraphs of the Simple Wikipedia article.

This is a very general and powerful idea which allows us to use large LLMs to generate whole new, synthetic datasets from existing text.
A nice feature of this method is that only the question is AI-generated, the answer itself is still human-written text which has been repurposed for a new task.
A dataset of Simple Wikipedia articles is hosted on HuggingFace \href{https://huggingface.co/datasets/wikipedia/viewer/20220301.simple/train}{here} which we will use to generate the questions.

\subsection{Creating and Filtering the Datasets}
RLHF involves three separate training steps, supervised fine-tuning, reward modeling, and reinforcement learning, so we therefore need three separate datasets.
Each dataset also has its own train, validation, and test sets.
To perform DPO we also need separate datasets for supervised fine-tuning and reward modeling.
In this section we will summarize how we filter and split our initial dataset into these three datasets.
\subsubsection{ELI5}
The initial training, validation, and test splits of the \href{https://huggingface.co/datasets/vblagoje/lfqa}{ELI5} dataset contains 226147, 3020, and 10000 separate posts. 
However, each post can contain multiple question/answer pairs.
If we count each QA pair as a separate datapoint then the train, validation, and test splits contain 669139, 22636, and 41650 datapoints, respectively.

Before we split this dataset up into SFT/RM/RL subsets, we first performed some data cleaning.
To form the SFT and RM datasets we used the following procedure:
\begin{enumerate}
\item First we used \href{https://pypi.org/project/redditcleaner/}{redditcleaner} to remove Reddit-specific markdown formatting.
\item Next we removed any quoted texts in the answers. Although quoting the original question is not problematic in and of itself, we did this to shorten the answer and ensure the entire answer can fit in the context length.
\item We also removed any extra whitespaces from the text.
\item The ELI5 dataset uses the string ``\_url\_i\_" as a placeholder for URLs, which are stored elsewhere in the dataset. 
We decided to remove URLs given the propensity of LLMs to hallucinate fake sources.
Instead, we intend to use retrieval augmented generation (RAG) so our LLMs can properly cite sources.
\item We removed any answers which are less than 20 words long, since these tend to be uninformative. 
\item We removed any posts that did not fit the question answer paradigm, e.g. if the title/post did not actually contain a question.
There are many posts on these subreddits of these forms, e.g. r/AskScience has \href{https://www.reddit.com/r/askscience/comments/10wyuf9/ask_anything_wednesday_engineering_mathematics/}{``Ask Anything Wednesday"} posts where users ask questions and receive answers in the comments and r/AskHistorians has had meta posts on the state of the subreddit, see e.g. this \href{https://www.reddit.com/r/AskHistorians/comments/yfs7uh/askhistorians_has_hit_15_million_subscribers_to/}{post} celebrating the subreddit hitting 1.5 million subscribers. We manually went through the ELI5 dataset to identify and remove such posts.
\item We removed answers which did not meet our simplicity criteria according to the \href{https://en.wikipedia.org/wiki/Flesch\%E2\%80\%93Kincaid_readability_tests}{Flesch-Kincaid readability metrics}. Specifically, we measured the simplicity of an answer by computing its Flesch reading ease (FRE) and Flesch-Kincaid grade level (FKG) scores. We only kept posts with FRE$\geq 60$, which corresponds to ``simple English" and FKG$< 9$, which corresponds to text which should be understandable by middle-school students and younger (the FKG score approximately corresponds to US school grade level). 
\end{enumerate}

There is of course freedom to change the above data cleaning procedure and it would be interesting to explore alternatives. 
In particular, it may be useful to keep the quoted text in the answer so that our LLM learns that it's useful to quote the original text when replying.
In addition, keeping the URLs in the answer may not cause anymore hallucination than normal. 
Both options are certainly worth exploring in the future.

Of all of the above filtering steps, the last one is likely the most unique to our project. 
The Flesch-Kincaid readability metrics are automatic metrics developed in the 1970s to measure the difficulty of the text.
They are computed as follows:
\begin{align}
\text{FRE} &= 206.835-1.015\left(\frac{\text{total words}}{\text{total sentences}}\right)-84.6\left(\frac{\text{total syllables}}{\text{total words}}\right)
\\
\text{FKG} &= 0.39\left(\frac{\text{total words}}{\text{total sentences}}\right)+11.8\left(\frac{\text{total syllables}}{\text{total words}}\right)-15.59
\end{align}
It would of course be interesting to consider more advanced methods to determine the simplicity or complexity of a text.
For example, we can try to train a model to classify texts into different categories or we could simply prompt GPT-4.
We decided to use Flesch-Readability metrics because they both serve as a strong baseline to compare against more complicated models and also because they gave us a fast and easy way to filter our large datasets. 
We expect that more advanced methods may be more useful to perform further pruning of our dataset.

After applying the above filtering steps, we split the resulting dataset into the SFT and RM dataset.
If we want to use a post for reward modeling we of course need to ensure that it contains more than one answer!
For this reason, we put any post that only has one answer into the SFT dataset.
In addition, even if we have multiple answers, we also need clear human preferences between those answers.
Therefore, we only keep answers in the RM dataset if they have unique scores such that we can rank them.
If two answers have the same score, we move one of the answers (with the corresponding question) to the SFT dataset.  
Finally, we place any question which has not been placed into either the SFT or RM dataset into the reinforcement learning dataset.
Note that this includes questions which were filtered out in the above pre-processing steps since for RL we only need the questions, and not the answers, since the model's generation will be judged solely by the reward model and will not be compared to the human-written text. 
For DPO we will only use the SFT and RM datasets.

Once we have split the dataset into the SFT, RM, and RL datasets, we next need to ensure that each dataset does not have any data leakage.
To catch data leakage, we used the ``all-mpnet-base-v2" \href{https://www.sbert.net/}{SentenceTransformers} model to embed each question into a 768-dimensional vector space.\footnote{Specifically, we combined the title and body of the post into one string applied the SentenceTransformer model to that text.}
We then computed the cosine-similarities between the embedding vectors in the train, validation, and test sets. As a reminder, the cosine similarity is defined by:
\begin{align}
\text{cos-similarity}(v_1,v_2)=\frac{v_1\cdot v_2}{|\!|v_1|\!|_2 |\!|v_2|\!|_2},
\end{align}
where $|\!|\ |\!|_2$ is the standard $L_2$ norm.
After inspecting various questions and their cosine-similarity, we decided to use 0.6 as our cutoff for detecting data-leakage, i.e. if \\ $\text{cos-similarity}(v_1,v_2)\geq 0.6$ we would remove either $v_1$ or $v_2$.
If a training example overlapped significantly with an example in the validation or test sets, we removed it from the training set since the training dataset is much larger than the other two datasets.
For the same reason, if an example in the test set overlapped significantly with a vector in the validation set, we removed it from the test set. After removing data leakage, the cleaning process for the SFT and RM datasets starts to differ slightly, so we will consider these two datasets separately for the remained of this section.
\\[10pt]
{\large \textbf{Supervised Fine-Tuning}}
\\[10pt]
During supervised fine-tuning we are essentially teaching our model to imitate how humans (or more advanced LLMs) answer questions.
To avoid our model imitating bad behavior, we need to perform extra filtering steps.
As a first, basic filtering step, we only kept answer which received a score of at least 4 from the Reddit community. 
The original ELI5 dataset was constructed by only keeping posts that had a score of at least 2 and also contained answers with a score of at least two.

Next, to prevent the model from outputting toxic or offensive content, we used \href{https://github.com/unitaryai/detoxify}{Detoxify} from UnitaryAI to detect toxic content.
Specifically, we use the ``unbiased" model which has a RoBERTa-Base model fine-tuned on the dataset from the following \href{https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification}{Kaggle competition}. This competition defined toxic content to be anything that is ``rude, disrespectful or otherwise likely to make someone leave a discussion." 
The model from Detoxify scores assigns a score of 0-1 on the following 6 metrics: severe\_toxicity, obscene, threat, insult, identity\_attack, sexual\_explicit".
Higher scores are associated to toxic text while lower scores are associated with safer text.
We filtered by SFT dataset to only keep answers which received a score of $\leq 0.1$ on all six metrics.


After performing the above cleaning steps, we trained Llama-2 models on the SFT dataset.
However, we noticed some issues with the model's generated text at inference time.
In particular, the model had a tendency to end its posts with strings of text of the form: ``Edit: typos, Edit 2: Sorry caught more typos etc.". 
Evidently, the model picked up on the fact that many Reddit posts by adding edits either correcting information in the post, adding additional sources, or thanking other users.
To avoid our model learning this behavior, we filtered the SFT dataset to remove any answers which ended in this way. 

The final ELI5-SFT dataset contains a total of 41568 questions where the train\/validation\/test split is 38595/880/2093. 
\\[10pt]
{\large \textbf{Reward Modeling}}
\\[10pt]
For reward modeling we do not need to perform the same filtering steps as in SFT since we are now teaching the model to distinguish between good and bad responses.
Therefore, we do not necessarily need to filter out all answers with a low score or which contain toxic content, we just need to ensure these answers are always in the ``rejected" column. 
Similarly, we can also keep answers which end in "edit: caught typos" as long as they are also restricted to the rejected answer.

When we originally performed DPO, we did not apply the above procedure: we kept answers with low scores, but we also filtered out all toxic content and did not filter out answers that end with superfluous edits. 
We therefore need to repeat the DPO data cleaning and training procedure and will update this section once that has been completed.

In addition, we also have freedom about which answers to keep in the reward modeling dataset.
Recall that during the reward modeling step we typically perform a binary classification task.
Therefore, if a question receives 10 different answers we can construct 45 different chosen/rejected pairs. In general, if we have $n$ answers then we have n(n-1)/2 number of pairs. One simple option is to treat pair independently. 
However, it was pointed out in \cite{ouyang2022training} that doing this leads to overfitting since the model sees some questions much more frequently than other ones.
To avoid this issue we will explore the following options:
\begin{enumerate}
\item For each question only keep one pair of answers. We can choose to keep the top two answers, the answers with the highest/lowest number of votes, or two random answers.
By only keeping one pair of answers, we avoid overfitting, but we also throw out potentially useful data.
\item For each question, we keep multiple pairs of answers, but rescale the loss function so that question/answer pairs are weighted inversely proportional to how many answers a question received.
\end{enumerate}
The second option was used in \cite{ouyang2022training} where the loss function is:
\begin{align}
\mathcal{L}_{\theta}=-\frac{1}{{K\choose 2}}\mathbb{E}_{(x,y_w,y_l)\sim \mathcal{D}}\left[\log \sigma\left(r_{\theta}(x,y_w)-r_{\theta}(x,y_l)\right)\right].
\end{align}
To be truly efficient, one should compute the reward associated to each answer separately, combine the result in a large vector, and then compute the pairwise differences between all the possible accepted/rejected columns.
Given our memory limits and the size of the ELI5 QA pairs we cannot fit all the question/answer pairs into memory, so we will compute the loss on each pair separately.
In addition, we will only keep a maximum of 10 pairs for each answer.

\subsubsection{Simple Wikipedia}

For Simple Wikipedia the data cleaning procedure is relatively simpler than for ELI5.
Unlike for ELI5, all the QA pairs will be placed in the SFT dataset.
The reason is simply that to perform reward modeling, we need to have two answers to compare against.
At the moment we only have a single answer, text from the original Simple Wikipedia article, associated to each GPT-3.5 generated question, so we cannot perform any preference modeling, although there are ways around this we are exploring.
In addition, the Simple Wikipedia is written in an encyclopedic format, so the articles do not contain toxic content.

All that said, we still need to clean the original Simple Wikipedia dataset before we actually use GPT-3.5 to generate questions or perform any fine-tuning.
The original Simple Wikipedia dataset contains a total of 205328 articles which have not been split up into train/validation/test datasets.
We then apply the following filtering steps:
\begin{enumerate}
\item We remove any articles which are less than 50 words long. Articles shorter than 50 words tend to be fairly uninformative.
\item We define a ``truncated\_text" column which is defined to be the first few paragraphs of an article.
To create the truncated article we concatenate paragraphs from the original article until the truncated article is at least 300 words long. After it reaches 300 words, or we reach the end of the original article, we stop adding new text. The truncated text will be our ``answer" when training the question-answering model.
\item To avoid excessively long answers, we remove any answers which are more than 500 words long. This is done to ensure that the answers are both long-form and coherent.
\item We compute the Flesch Readability Ease (FRE) and Flesch-Kincaid grade score (FKG) and only keep articles which have an FRE$\geq 60$ and a FKG$<9$.
This is the same simplicity criteria as used for the Reddit data.
\item Finally, we split the resulting dataset into train/validation/test sets where the validation and test sets contain 5000 articles and the training set contains the remaining articles.
\end{enumerate}

After applying the above filtering steps, we noticed some articles contained text which was not written in natural language. 
Specifically, Wikipedia contains its own special markdown text for images and tables. 
We therefore removed any articles which contained such text to avoid confusing the LLM.
The resulting train/validation/test split of the Simple Wikipedia dataset contains 65254/4991/4997 articles.
We then used GPT-3.5 to create questions whose answers are given in the truncated articles to create the Simple Wikipedia SFT dataset.

\subsection{Exploratory Data Analysis}
This section will be filled in with more details on the features of our datasets.

\section{Inference}

\subsection{How to Define The Inference Model}
As is well-known, after training a model we have to make sure it is in ``evaluation" mode before performing inference, i.e. in PyTorch we call model.eval() to turn off dropout, prevent the accumulation of any more gradients, etc.

In this project we faced an additional subtlety because we used QLoRA to fine-tune the large language models.
In the original LoRA paper \cite{hu2021lora} they showed that one can efficiently fine-tune LLMs by adding low-rank adapter layers to the transformer.
Specifically, they added adapter weights to the query, key, value, and output matrices that are used in the attention mechanims.
In the QLoRA paper \cite{dettmers2023qlora} they extended this idea by additionally quantizing the base model weights to a new data type called 4-bit NormalFloat (NF4) while using a higher-precision, BF16, for the adapter layers.\footnote{In the QLoRA paper they also use double-quantization (quantizing the quantization constants) and paged optimizers to also save memory. In addition, in the QLoRA paper they add adapter layers to all linear layers and not just the attention weights.}
In order to perform back-propagation for a given LoRA layer, we dequantize the associated base model's weights to BF16, update the LoRA weights, and then requantize the base model's weights.
By using these methods, we can fine-tune 13B models on a single A100 GPU with 40GB of GPU RAM.

However, we now face a subtlety when defining the model at inference time because there are now two different ways to merge the LoRA layers with the base model's weights.
One option, which we call the simple merge, is to quantize the base model's FP32 weights to BF16 and then merge with the LoRA layers.
The second option, which we call the careful merge, is to first quantize the base model's weights to NF4, de-quantize to BF16, and then merge those weights with the LoRA adapter layers.
The second option is more natural since this is the procedure we actually follow during training and its important that LoRA weights see the same base weights during training and inference.
The source of the difference between these two merges is that when we quantize and de-quantize weights we inevitably produce small errors due to the intermediate loss of precision.
Given that we are studying billion-parameter models, these small deviations can build up to large effects.
For this reason, unless stated explicitly, we will always use the careful merge at inference time.

\subsection{Supervised Fine-Tuning}
In this section we will summarize the results of Llama-2 models fine-tuned using supervised fine-tuning.

\subsubsection{HuggingFace Leaderboard}
First we'll summarize the results of our fine-tuned models versus the Meta-Llama models on the Huggingface Open LLM leaderboard.
The leaderboard is a wrapper for the ``Eleuther AI Language Model Evaluation Harness". 
Specifically, the leaderboard measures the 25-shot performance of LLMs on the arc-challenge dataset, the 10-shot performance on the HellaSwag dataset, the 0-shot performance on TurthfulQA, and the 5-shot performance on MMLU. For arc-challenge and HellaSwag performance is measured using the acc$-$norm metric of EleutherAI, for TruthfulQA they use the mc2 metric, and for MMLU they average the accuracy of the model across tasks in the MMLU dataset.
The nice thing about the leaderboard is one can submit either the full model or submit just the adapter layers and give the base model separately.
\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Model &          Average & Arc & HellaSwag & MMLU & TruthfulQA \\
\hline
Llama-2-7b&                  53.40               &   53.07            & 77.74              &  43.80                &  38.98 \\
Llama-2-7b-chat  &          \textbf{56.34} &  52.90              & 78.55             &  \textbf{48.32}    &  \textbf{45.57} \\
Llama-2-7b-ELI5  &           53.92             &  53.41              & 77.90             &  43.56                 & 40.81\\
Llama-2-7b-wiki &           53.72              &  \textbf{54.35} & 78.06             &  45.35                 & 37.11\\
Llama-2-7b-ELI5-wiki&    55.46               &  53.75              & \textbf{78.76}&  46.02                   & 43.31\\
\hhline{|=|=|=|=|=|=|}
Llama-2-13b &               56.90              &   58.11             & 80.97            &  54.34            & 34.17\\
Llama-2-13b-chat&        59.93              & 59.04               & 81.94            &  54.64            & \textbf{44.12}\\
Llama-2-13b-ELI5&          \textbf{60.61} &  \textbf{60.41}  & \textbf{82.58}&  \textbf{55.86}& 43.61\\
Llama-2-13b-wiki&	        58.12             &  59.04               & 82.33            &  55.36            & 35.75\\
Llama-2-13b-ELI5-wiki&   59.43             &  59.98               & 82.43            &  55.41            & 39.90 \\
\hline
\end{tabular}
\end{center}
\caption{Results for Llama-2 models on Huggingface Open LLM Leaderboard}.
\label{table:hfleaderboard}
\end{table}
Here Llama-2-7b is the base 7 billion model and Llama-2-7b-chat is the Llama-2 model which has undergone RLHF.
The remaining models are fine-tuned versions of the base-model. 
Specifically, they are trained using SFT and QLORA on the ELI5 SFT dataset, the Simple Wikipedia Instruct dataset, or their combination. The second half of the table is the same, except for the 13B parameter model.

%All that said, to get the above results we quantized and dequantized the 7B model before merging, while for the 13B model we quantized the model to bfloat16 and then merged. 
%For the 7B model we could perform the quantization and dequantization on a 40GB A100.
%For the 13B model we directly submitted the adapter layers to the HuggingFace leaderboard and used the ungated Llama-2-7b-hf model from NousResearch (the Meta-Llama model is gated and although it can be downloaded, we were not able to use it as a base model on the leaderboard).

In Table \ref{table:hfleaderboard} we see that of the 7B models, the Meta-Llama-2-7b-chat performs the best on average with the Llama-2 model trained on ELI5 and Simple Wikipedia performing the second best.
One surprising thing is the chat model actually performs worse than the base model on the Arc-Challenge dataset, and here the model trained on just Simple Wikipedia QA pairs performs the best. 
On the HellaSwag dataset the Llama-2-7B model trained on ELI5 and Simple Wikipedia marginally outperforms the chat model, but the difference is likely too small to be statistically significant.
Finally, on MMLU and TruthfulQA the 7B chat model performs significantly better than the other models.

Once we go up to 13B parameters we see that the Llama-2 model trained on the ELI5 SFT dataset performs the best on average with the 13B chat model and 13B model fine-tuned on ELI5 + Simple Wikipedia close behind.
It is surprising that the model trained on just ELI5 model performs the best, and this result is driven primarily by its improved performance on the TruthfulQA dataset. 
On the other datasets it barely improves over the model trained on the combined ELI5 and Simple Wikipedia dataset.
We are not sure what the cause of this effect is, somehow training the model more on Reddit data makes the model more honest!
This result could also be an artifact of a poor choice of hyperparameters, and perhaps with a different learning rate and/or after averaging over initializations the difference would go away or the model trained on the combined dataset would perform better.

\subsubsection{Judging the Validation Set with GPT-4}


\subsubsection{ROUGE and BERTScore}
In this section we will investigate how supervised fine-tuning effects the models ROUGE and BERTScores.
ROUGE is a well-known automated benchmark that measures n-gam overlap between generated text and the reference text.
Since it just looks at n-gram overlaps, and does not take into account semantic content, ROUGE is effectively measuring to what extent the trained model is adopting the vocabulary of the reference text.
One advantage of BERTScore is it uses pre-trained encoder models, such as BERT or RoBERTa, to encode the generated and reference text in some high-dimensional vector space and then measures the cosine-similarity between the two vectors.
Of course, this also means that BERTScore is more computationally intensive to compute.

In this section we will only perform inference on a small subset (100 QA pairs) of the validation set, for each validation set we randomly sample 100 question and generate the answers.

In the tables below we present the ROUGE and BERTScore metrics for the Llama-2 base model, as well as our fine-tuned models, on the (small) validation sets for our three datasets: ELI5, Simple Wikipedia, and combined dataset.
We include the original Llama-2 model as a baseline to measure how much fine-tuning changes the Rouge and BERTScores.
In addition, we also include ``off-diagonal" elements, where we train a model on one dataset and measure its ROUGE and BERTScores on the validation split of a different dataset.
For example, we include cases where we train the model on the ELI5 SFT dataset and then evaluate it on the validation split of the Simple Wikipedia QA dataset.
We included these results to serve as additional baselines to see to what extend fine-tuning on \textit{any} QA dataset changes the evaluation metrics.
In all cases, the model produces at most 256 new tokens.

\begin{table}[htbp]
\centering
\begin{tabular}{cccccc}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{rouge1} & \textbf{rouge2} & \textbf{rougeL} & \textbf{rougeLsum} \\
\midrule
\multirow{4}{*}{ELI5} & Llama-2-7b & \textbf{0.3796} & \textbf{0.2432} & \textbf{0.3000} & \textbf{0.3222} \\
& Llama-2-7b-ELI5 & 0.3701 & 0.2140 & 0.2736 & 0.2821 \\
& Llama-2-7b-wiki & 0.3575 & 0.2083 & 0.2660 & 0.2762 \\
& Llama-2-7b-ELI5-wiki & 0.3702 & 0.2126 & 0.2733 & 0.2819 \\
\midrule
\multirow{4}{*}{wiki} & Llama-2-7b & 0.1923 & 0.0103 & 0.0937 & \textbf{0.1392} \\
& Llama-2-7b-ELI5 & \textbf{0.2203} & \textbf{0.0125} & \textbf{0.0964} & 0.1271 \\
& Llama-2-7b-wiki & 0.1826 & 0.0073 & 0.0879 & 0.1165 \\
& Llama-2-7b-ELI5-wiki & 0.1811 & 0.0076 & 0.0885 & 0.1153 \\
\midrule
\multirow{4}{*}{full} & Llama-2-7b & 0.1944 & 0.0087 & 0.0905 & \textbf{0.1400} \\
& Llama-2-7b-ELI5 & \textbf{0.2243} & \textbf{0.0118} & \textbf{0.0971} & 0.1312 \\
& Llama-2-7b-wiki & 0.1905 & 0.0079 & 0.0889 & 0.1198 \\
&Llama-2-7b-ELI5-wiki & 0.1907 & 0.0080 & 0.0894 & 0.1193 \\
\bottomrule
\end{tabular}
\caption{Rouge Scores}
\label{tab:rouge_scores}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{ccccc}
\toprule
\textbf{Dataset} & \textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
\multirow{4}{*}{ELI5} & Llama-2-7b & \textbf{0.8429} & 0.8754 & 0.8583 \\
& Llama-2-7b-ELI5 & 0.8372 & \textbf{0.8799} & 0.8575 \\
& Llama-2-7b-wiki & 0.8415 & 0.8796 & \textbf{0.8598} \\
& Llama-2-7b-ELI5-wiki & 0.8399 & 0.8798 & 0.8590 \\
\midrule
\multirow{4}{*}{wiki} & Llama-2-7b & \textbf{0.7879} & 0.8067 & 0.7970 \\
& Llama-2-7b-ELI5 & 0.7859 & \textbf{0.8093} & \textbf{0.7971} \\
& Llama-2-7b-wiki & 0.7782 & 0.8019 & 0.7898 \\
& Llama-2-7b-ELI5-wiki & 0.7783 & 0.8014 & 0.7896 \\
\midrule
\multirow{4}{*}{full} & Llama-2-7b & \textbf{0.7940} & 0.8092 & 0.8014 \\
& Llama-2-7b-ELI5 & 0.7933 & \textbf{0.8116} & \textbf{0.8022} \\
& Llama-2-7b-wiki  & 0.7861 & 0.8048 & 0.7952 \\
& Llama-2-7b-ELI5-wiki & 0.7841 & 0.8046 & 0.7941 \\
\bottomrule
\end{tabular}
\caption{Precision, Recall, and F1 BERTScores}
\label{tab:scores}
\end{table}

It's difficult to interpret or make sense of these results, somehow the base Llama-2 model often outperforms the fine-tuned models and the model trained on just the ELI5 dataset often performs very well on the Simple Wikipedia validation set!
It is probably best to take these numbers with a grain of salt. The BERTScore metrics tend to differ by very small amounts, e.g. the difference in F1 scores between the base model and the model trained on ELI5 for the Simple Wikipedia validation set differ by just 0.001.
In addition, we of course do not know on what data the original Llama-2 model was trained and if there is data leakage.

Finally, let's look at the Flesch readability ease (FRE) and Flesch-Kincaid grade (FKG) metrics. 
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Model} & \textbf{FRE} & \textbf{FKG} \\
\hline
\multirow{4}{*}{ELI5} & Llama-2-7b & 58.2425 & 10.075 \\
& Llama-2-7b-ELI5 & 67.6357 & 8.148  \\
& Llama-2-7b-wiki & 58.7145 & 10.526 \\
& Llama-2-7b-ELI5-wiki & 65.6862 &8.704  \\
\hline
\multirow{4}{*}{wiki} & Llama-2-7b-hf & 64.1040 & 8.172\\
& Llama-2-7b-ELI5 & 72.5682&6.988 \\
& Llama-2-7b-wiki & 66.5824 &8.055\\
& Llama-2-7b-ELI5-wiki & 65.7558 &8.239\\
\hline
\multirow{4}{*}{full} & Llama-2-7b & 65.0452&8.472 \\
& Llama-2-7b-ELI5 & 72.5278&7.092 \\
& Llama-2-7b-wiki & 66.8147&8.252 \\
& Llama-2-7b-ELI5-wiki & 65.8295 &8.417\\
\hline
\end{tabular}
\caption{Flesch Readability Metrics}
\end{table}
We see that for the most part, the fine-tuned models have a higher readability score and a lower grade level.
There are two noticeable exceptions, the model trained on Simple Wikipedia QA pairs and evaluated on the ELI5 validation set has a higher grade level than the original base model.
In addition, the model trained on ELI5 and Simple Wikipedia has a slightly higher grade level than the base model when evaluated on the Simple Wikipedia validation set. 
In the former case, the grade level went up by 0.451 while the readability also went up by 0.472.
Given that a text is considered simpler if its grade level is lower and its readability is higher, in this case our automatic metrics are inconclusive.
In the latter case, the readability also went up by about 1.6518 points while the grade level went up by 0.067.
Once again, in this case our metrics for readability are moving in opposite directions. 
Finally, from the table we also see that the model trained on just ELI5 tends to produce the simplest text. 


\subsubsection{MT(S)-Bench}

In this section we will investigate how well our models perform on a new MT-bench like dataset.
The idea of MT-bench is to use a LLM, like GPT-4, to judge the output of other, smaller LLMs. 
There are two types of completion tasks, single-turn and multi-turn.
For a single-turn problem, the model is given a prompt and asked to complete it.
For a multi-turn problem, the model is prompted with one complete turn of a conversation (i.e. question and answer) and then asked to reply to a new, follow-up question. 
Given that our models are only trained for single-turn problems, we will see that they do not perform well on multi-turn problems.

\begin{center}
\begin{tabular}{c|c|c}
    \hline
    \textbf{Model} & \textbf{Single-Turn score}& \textbf{Multi-Turn score} \\
    \hline
    Llama-2-7b-chat-hf &  9.40 & 8.80\\ 
    Llama-2-7b-ELI5-wiki-simple-merge & 6.683333 & 3.533333  \\
    Llama-2-7b-ELI5-wiki &  5.975000 &  2.966667 \\
    Llama-2-7b-wiki &  5.108333 & 1.616667  \\
    Llama-2-7b-ELI5 &  4.116667 & 1.733333  \\
    Llama-2-7b-wiki-simple-merge &  2.300000& 1.033333  \\
    Llama-2-7b-ELI5-simple-merge & 2.000000 & 1.200000  \\
    Llama-2-7b-hf & 1.066667 &1.000000 \\
    \hline
\end{tabular}
\end{center}

Next let's look at the readability scores of each model on the single prompt:

\begin{center}
\begin{tabular}{c|c|c}
    \hline
    \textbf{Model} & \textbf{FRE}& \textbf{FKG} \\
    \hline
    Llama-2-7b-chat-hf &  42.21 & 12.50 \\ 
    Llama-2-7b-ELI5-wiki-simple-merge & 60.55 & 9.60  \\
    Llama-2-7b-ELI5-wiki &  53.41 &  10.20 \\
    Llama-2-7b-wiki &  91.41 & 3.90 \\
    Llama-2-7b-ELI5 &  63.19 & 8.50 \\
    Llama-2-7b-wiki-simple-merge &  96.48& 2.00  \\
    Llama-2-7b-ELI5-simple-merge & 78.65 & 4.70  \\
    Llama-2-7b-hf & 29.86 &13.10 \\
    \hline
\end{tabular}
\end{center}


\bibliographystyle{utphys}
\bibliography{refs}


\end{document}  