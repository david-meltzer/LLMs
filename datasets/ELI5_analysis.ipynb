{"cells":[{"cell_type":"markdown","metadata":{"id":"bdp_5O98KF7-"},"source":["# Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693586877634,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"8HcMFFaHocM9","outputId":"cf17f20b-d8b2-4b1f-b32f-ec6083a0a63b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/LLMs/ELI5_dataset\n"]}],"source":["%cd drive/MyDrive/LLMs/ELI5_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xX-WlvYb3Y0c"},"outputs":[],"source":["!pip install datasets --quiet\n","!pip install textstat --quiet\n","!pip install wandb --quiet\n","!pip install redditcleaner --quiet\n","!pip install huggingface_hub --quiet\n","!pip install -U sentence-transformers --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4c92Nr3tf7p"},"outputs":[],"source":["# Import necessary libraries and modules\n","import wandb\n","import torch\n","import sys\n","import datasets\n","import os\n","import redditcleaner\n","import re\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from huggingface_hub import notebook_login\n","from sentence_transformers import SentenceTransformer\n","from textstat import flesch_reading_ease as fre\n","from textstat import flesch_kincaid_grade as fkg\n","from datasets import (\n","    load_dataset,\n","    load,\n","    load_from_disk,\n","    Dataset,\n","    concatenate_datasets,\n","    DatasetDict\n",")\n","from itertools import compress\n","from tqdm import tqdm\n","from collections import defaultdict\n","from itertools import combinations\n","import random\n","from datetime import datetime\n","\n","# Check for GPU availability and set the device accordingly\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Enable inline plotting for Jupyter Notebooks\n","%matplotlib inline\n"]},{"cell_type":"markdown","metadata":{"id":"wp8NTXOaURD7"},"source":["#Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u28w0TgXUcE1"},"outputs":[],"source":["def replace_url_i(text):\n","    \"\"\"\n","    Replace all occurrences of the pattern \"_url_i_\", where i is an arbitrary integer, with an empty string in the input text.\n","\n","    Parameters:\n","        text (str): The input text containing occurrences of the pattern to be replaced.\n","\n","    Returns:\n","        str: The modified text with all occurrences of the pattern removed.\n","\n","    Example:\n","        >>> replace_url_i(\"Check out my website: _url_123_ and _URL_456_\")\n","        'Check out my website:  and '\n","    \"\"\"\n","    # Define the regular expression patterns to match \"_url_i_\" where i is an arbitrary integer\n","    patterns = [r\"_url_\\d+_\", r\"_Url_\\d+_\", r\"_URL_\\d+_\"]\n","\n","    # Use re.sub() to replace all occurrences of the pattern with an empty string\n","    for pattern in patterns:\n","        text = re.sub(pattern, \"\", text)\n","\n","    return text\n","\n","def preprocess_example(example):\n","    \"\"\"\n","    Preprocess an example dictionary containing 'answers', 'title', and 'selftext' keys.\n","\n","    The function applies the following preprocessing steps to each element in the example:\n","    1. Cleans all answers, titles, and selftext using redditcleaner.\n","    2. Remove any quoted text starting with '>' and ending with a newline character in 'answers'.\n","    3. Removes extra whitespaces.\n","    4. Remove any occurrences of \"_url_i_\" in 'answers', 'title', and 'selftext'.\n","    5. Filter out answers with less than 20 words.\n","\n","    Parameters:\n","        example (dict): A dictionary containing 'answers', 'title', and 'selftext' keys.\n","\n","    Returns:\n","        dict: The preprocessed example dictionary with the above transformations applied.\n","\n","    Example:\n","        >>> example = {\n","                'answers': {'text': ['Visit this website: _url_123_', 'Sure, here is the link: _URL_456_']},\n","                'title': 'How to use Python?',\n","                'selftext': 'Check out this tutorial: _Url_789_ to learn Python.'\n","            }\n","        >>> preprocess_example(example)\n","        {\n","            'answers': {'text': ['visit this website:', 'sure, here is the link:']},\n","            'title': 'how to use python?',\n","            'selftext': 'check out this tutorial: to learn python.'\n","        }\n","    \"\"\"\n","    # Preprocess 'answers'\n","    answers = example['answers']['text']\n","    answers = [redditcleaner.clean(answer) for answer in answers]\n","    answers = [re.sub(r'>.*?\\n', ' ', answer) for answer in answers]\n","    answers = [' '.join(answer.split()) for answer in answers]\n","    answers = [replace_url_i(answer) for answer in answers]\n","    answers = [answer for answer in answers if len(answer.split()) >= 20]\n","    example['answers']['text'] = answers\n","\n","    # Preprocess 'title'\n","    title = example['title']\n","    title = redditcleaner.clean(title)\n","    title = ' '.join(title.split())\n","    title = replace_url_i(title)\n","    example['title'] = title\n","\n","    # Preprocess 'selftext'\n","    selftext = example['selftext']\n","    selftext = redditcleaner.clean(selftext)\n","    selftext = ' '.join(selftext.split())\n","    selftext = replace_url_i(selftext)\n","    example['selftext'] = selftext\n","\n","    return example\n","\n","class score_cutoff_wrapper:\n","    \"\"\"\n","    A wrapper class to filter answers based on a cutoff score from an example dictionary.\n","\n","    This class provides a method to filter the answers in an example based on their corresponding scores.\n","    Answers with a score greater than or equal to the specified cutoff will be retained, and others will be removed.\n","\n","    Parameters:\n","        cutoff (int or float): The cutoff score value to filter answers.\n","    \"\"\"\n","\n","    def __init__(self, cutoff):\n","        \"\"\"\n","        Initialize the score_cutoff_wrapper with the specified cutoff score.\n","\n","        Parameters:\n","            cutoff (int or float): The cutoff score value to filter answers.\n","        \"\"\"\n","        self.cutoff = cutoff\n","\n","    def score_cutoff_ex(self, example):\n","        \"\"\"\n","        Filter the answers in the example based on the cutoff score.\n","\n","        Parameters:\n","            example (dict): A dictionary containing 'answers' key with 'text' and 'score' lists.\n","\n","        Returns:\n","            dict: The modified example dictionary with answers filtered based on the cutoff score.\n","\n","        Example:\n","            >>> example = {\n","                    'answers': {\n","                        'text': ['Yes', 'No', 'Maybe'],\n","                        'score': [10, 5, 8]\n","                    }\n","                }\n","            >>> wrapper = score_cutoff_wrapper(cutoff=8)\n","            >>> filtered_example = wrapper.score_cutoff_ex(example)\n","            >>> filtered_example\n","            {\n","                'answers': {\n","                    'text': ['Yes', 'Maybe'],\n","                    'score': [10, 8]\n","                }\n","            }\n","        \"\"\"\n","        scores = example['answers']['score']\n","        # Find idxs where scores >= cutoff.\n","        idxs = list(np.array(scores) >= self.cutoff)\n","        # For each (key,value) pair in dictionary example['answers'] only\n","        # keep text and metadata for answers with a high enough score.\n","        for key, val in example['answers'].items():\n","            example['answers'][key] = list(compress(val, idxs))\n","\n","        return example\n","\n","\n","def score_cutoff(dataset,cutoff=4):\n","    \"\"\"\n","    Uses class score_cutoff_wrapper to filter a Huggingface dataset to only keep\n","    scores above a certain cutoff.\n","\n","    Parameters:\n","        dataset (Dataset): The input Huggingface dataset to be filtered.\n","        cutoff (int or float, optional): The cutoff score value to filter answers. Default is 4.\n","\n","    Returns:\n","        Dataset: The modified dataset with answers filtered based on the cutoff score.\n","    \"\"\"\n","    cutoff = score_cutoff_wrapper(cutoff)\n","    ds = dataset.map(cutoff.score_cutoff_ex)\n","    ds = ds.filter(lambda post: len(post['answers']['score'])>0)\n","\n","    return ds\n","\n","\n","def flesch_scores(example):\n","    \"\"\"\n","    Calculate Flesch Readability scores for each answer in the example.\n","\n","    This function calculates Flesch Readability scores and Flesch-Kincaid Grade levels for each answer in the example.\n","    The calculated scores are then added to the example dictionary under the 'fre' (Flesch Readability) and 'fkg'\n","    (Flesch-Kincaid Grade) keys.\n","\n","    Parameters:\n","        example (dict): A dictionary containing 'answers' key with 'text' lists for each answer.\n","\n","    Returns:\n","        dict: The modified example dictionary with Flesch Readability and Flesch-Kincaid Grade scores.\n","\n","    Example:\n","        >>> example = {\n","                'answers': {\n","                    'text': ['This is a sample answer.', 'Another answer with more words.']\n","                }\n","            }\n","        >>> flesch_scores(example)\n","        {\n","            'answers': {\n","                'text': ['This is a sample answer.', 'Another answer with more words.'],\n","                'fre': [89.1, 79.2],\n","                'fkg': [2.6, 5.5]\n","            }\n","        }\n","    \"\"\"\n","\n","    # Compute Flesch Readability score for each answer.\n","    fre_scores = [fre(text) for text in example['answers']['text']]\n","    # Compute Flesch Kincaid Grade level for each answer.\n","    fkg_scores = [fkg(text) for text in example['answers']['text']]\n","    # Add corresponding metrics to dictioanry example['answers'].\n","    example['answers']['fre'] = fre_scores\n","    example['answers']['fkg'] = fkg_scores\n","\n","    return example\n","\n","\n","class flesch_scores_filter_wrapper:\n","    \"\"\"\n","    This class provides a method to filter answers in an example based on Flesch Readability (fre) and\n","    Flesch-Kincaid Grade (fkg) scores. Answers with fre >= fre_cutoff and fkg < fkg_cutoff will be retained,\n","    and others will be removed.\n","    \"\"\"\n","\n","    def __init__(self, fre_cutoff, fkg_cutoff):\n","        \"\"\"\n","        Initialize the flesch_scores_filter_wrapper with the specified cutoff scores.\n","\n","        Parameters:\n","            fre_cutoff (float): The cutoff value for Flesch Readability score.\n","            fkg_cutoff (float): The cutoff value for Flesch-Kincaid Grade score.\n","        \"\"\"\n","        self.fre_cutoff = fre_cutoff\n","        self.fkg_cutoff = fkg_cutoff\n","\n","    def flesch_scores_filter(self, example):\n","        \"\"\"\n","        Applies filter to specific example using self.fre_cutoff and self.fkg_cutoff.\n","\n","        Parameters:\n","            example (dict): A dictionary containing 'answers' key with 'fre' and 'fkg' lists.\n","\n","        Returns:\n","            dict: The modified example dictionary with answers filtered based on the cutoff scores.\n","\n","        Example:\n","            >>> example = {\n","                    'answers': {\n","                        'text': ['This is a sample answer.', 'Another answer with more words.'],\n","                        'fre': [89.1, 79.2],\n","                        'fkg': [2.6, 5.5]\n","                    }\n","                }\n","            >>> filter = flesch_scores_filter_wrapper(fre_cutoff=80, fkg_cutoff=5)\n","            >>> filtered_example = filter.flesch_scores_filter(example)\n","            >>> filtered_example\n","            {\n","                'answers': {\n","                    'text': ['This is a sample answer.'],\n","                    'fre': [89.1],\n","                    'fkg': [2.6]\n","                }\n","            }\n","        \"\"\"\n","        fre_scores = example['answers']['fre']\n","        fkg_scores = example['answers']['fkg']\n","\n","        idxs = [True if (fre_scores[i] >= self.fre_cutoff\n","                         and fkg_scores[i] < self.fkg_cutoff) \\\n","                else False for i in range(len(fre_scores))]\n","\n","        # Use 'compress' to filter the values based on the boolean mask 'idxs'\n","        for key, val in example['answers'].items():\n","            example['answers'][key] = list(compress(val, idxs))\n","\n","        return example\n","\n","\n","def flesch_scores_cutoff(dataset, fre_cutoff=60, fkg_cutoff=9):\n","    \"\"\"\n","    This function applies flesch_scores_filter_wrapper to a Huggingface dataset.\n","    Only answers with fre >= fre_cutoff and fkg < fkg_cutoff.\n","    Posts with no qualifying answers will be removed.\n","\n","    Parameters:\n","        dataset (Dataset): Huggingface dataset to be filtered.\n","        fre_cutoff (float, optional): The cutoff value for Flesch Readability score. Default is 60.\n","        fkg_cutoff (float, optional): The cutoff value for Flesch-Kincaid Grade score. Default is 9.\n","\n","    Returns:\n","        Dataset: The modified dataset with answers filtered based on the Flesch Readability and Flesch-Kincaid Grade scores.\n","    \"\"\"\n","\n","    # Define filter function.\n","    filter = flesch_scores_filter_wrapper(fre_cutoff, fkg_cutoff)\n","    # Apply function to entire dataset.\n","    ds = dataset.map(filter.flesch_scores_filter)\n","    # Remove any posts with no valid answers.\n","    ds = ds.filter(lambda post: len(post['answers']['fre']) > 0)\n","\n","    return ds\n","\n","def preprocess_data(dataset,\n","                    output_file='./data/filtered',\n","                    save_file=True,\n","                    log_to_wandb=True,\n","                    overwrite=False):\n","    \"\"\"\n","    Preprocesses the input dataset by applying various filters and transformations.\n","\n","    Parameters:\n","        dataset (Dataset): The input Huggingface dataset to be processed.\n","        output_file (str, optional): The path to the file where the processed dataset will be saved.\n","            Default is './data/filtered'.\n","        save_file (bool, optional): If True, saves the processed dataset to the output_file.\n","            Default is True.\n","        log_to_wandb (bool, optional): If True, logs the processed dataset as a WandB artifact.\n","            Default is True.\n","        overwrite (bool, optional): If True, overwrites the output_file if it already exists.\n","            Default is False.\n","\n","    Returns:\n","        Dataset: The preprocessed dataset.\n","\n","    \"\"\"\n","\n","    if os.path.exists(output_file) and not overwrite:\n","        # If the output_file exists and overwrite is False, load the dataset from disk and return it.\n","        return load_from_disk(output_file)\n","\n","    # List of strings to filter out posts based on their titles\n","    not_qus = ['IAMA', 'AMA', 'ama:', 'megathread', 'Megathread',\n","               'Discussion Thread', 'Discussion thread',\n","               'discussion Thread', 'discussion thread',\n","               'Ask Anything Wednesday', 'Free-for-All',\n","               'Free-For-All', '[META]', 'Monday Methods',\n","               'Tuesday Trivia', 'Monday Mysteries',\n","               'Theory Thursday', 'Monday Mish-Mash',\n","               'Media Mondays', '[META]', 'Wednesday Week in History',\n","               'Saturday Popular Questions', 'Ask Anything Wednesday',\n","               'Thursday Focus Historical Fiction']\n","\n","    # List of question words used to filter out posts without meaningful questions in their titles or selftext\n","    qu_reqs = ['who', 'what', 'where', 'why', 'when', 'how', '?']\n","\n","    # Preprocess each example in the dataset using the preprocess_example function\n","    dataset = dataset.map(preprocess_example)\n","\n","    # Filter out posts with 'nsfw' in their titles\n","    dataset = dataset.filter(lambda post: 'nsfw' not in post['title'].lower())\n","\n","    # Filter out posts that do not contain meaningful questions in their titles or selftext\n","    dataset = dataset.filter(lambda post:\n","                             not (all(qu_req not in post['title'].lower() for qu_req in qu_reqs)\n","                                  and all(qu_req not in post['selftext'].lower() for qu_req in qu_reqs)))\n","\n","    # Filter out posts that do not correspond to questions.\n","    dataset = dataset.filter(lambda post: not (any(nq in post['title'] for nq in not_qus)))\n","\n","    # Map the flesch_scores function to calculate Flesch readability scores for each post\n","    dataset = dataset.map(flesch_scores)\n","\n","    # Apply score_cutoff function to remove posts with low reddit scores\n","    #dataset = score_cutoff(dataset)\n","\n","    # Apply flesch_scores_cutoff function to remove posts with scores below a certain threshold\n","    dataset = flesch_scores_cutoff(dataset)\n","\n","    if save_file:\n","        # Save the processed dataset to the output_file\n","        dataset.save_to_disk(output_file)\n","\n","        if log_to_wandb:\n","            # Log the processed dataset as a WandB artifact if log_to_wandb is True\n","            now = datetime.now()\n","            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","            with wandb.init(project='ELI5_analysis',\n","                            entity='ft-llmmm',\n","                            job_type='preprocess_data',\n","                            name=f'preprocess_data_{time_stamp}') as run:\n","                # Initialize a WandB run for logging\n","                processed_data_art = wandb.Artifact('ELI5_processed', 'dataset')\n","                processed_data_art.add_dir(output_file)\n","                run.log_artifact(processed_data_art)\n","\n","    return dataset\n","\n","def split_idxs(example):\n","    \"\"\"\n","    Splits the indices of scores from the input example's answers into two sets,\n","    pref_scores_idxs and dupl_scores_idxs.\n","\n","    pref_scores_idxs = Each index in pref_scores_idxs corresponds\n","                       to a unique score in example['answers']['score'].\n","\n","\n","    dupl_scores_idxs = List of indices of example['answers']['score']\n","                       not found in pref_scores_idxs.\n","\n","    pref_scores_idx correspond to indices of answers we will use for preference modeling\n","    since there are no ties in this set.\n","\n","    dupl_scores_idxs correponds to indices of answers we will use for supervised fine-tuning.\n","\n","\n","    Parameters:\n","        example (dict): The input example containing 'answers' as a dictionary with 'score' as a list.\n","\n","    Returns:\n","        dict: The modified input example with 'pref_idxs' and 'dupl_scores_idxs' added.\n","\n","    \"\"\"\n","\n","    # Extract the 'score' list from the 'answers' dictionary in the example.\n","    scores = example['answers']['score']\n","\n","    # Sort the unique scores in descending order.\n","    scores_unique = sorted(set(scores), reverse=True)\n","\n","    # Get the indices of the preferred scores in the 'scores' list.\n","    pref_scores_idxs = [scores.index(sc) for sc in scores_unique]\n","\n","    # Get the indices of duplicate scores in the 'scores' list.\n","    dupl_scores_idxs = [n for n in range(len(scores)) if n not in pref_scores_idxs]\n","\n","    # Add the preferred and duplicate scores indices to the input example.\n","    example['pref_idxs'] = pref_scores_idxs\n","    example['dupl_scores_idxs'] = dupl_scores_idxs\n","\n","    # Return the modified example with the added indices.\n","    return example\n","\n","def mult_ans_RM_proc(example):\n","    \"\"\"\n","    Processes posts containing multiple answers. Only retains answers that will be used for\n","    preference modelling.\n","\n","    Parameters:\n","        example (dict): The input example containing 'pref_idxs' and 'answers' as dictionary keys.\n","            'pref_idxs' is a list of indices corresponding to answers we will use for preference modelling.\n","             Value associated to the key 'answers' is a dictionary containing the text and metadata of the answers.\n","\n","    Returns:\n","        dict: The modified input example with 'answers' containing only text and metadata used for preference modeling.\n","\n","    \"\"\"\n","\n","\n","    pref_scores_idxs = example['pref_idxs']\n","\n","    # Iterate through each key-value pair in the 'answers' dictionary.\n","    for key, val in example['answers'].items():\n","        # Update the 'answers' dictionary by keeping only answers to be used for preference modeling.\n","        example['answers'][key] = [example['answers'][key][i] for i in pref_scores_idxs]\n","\n","    return example\n","\n","\n","def mult_ans_SFT_proc(example):\n","    \"\"\"\n","    Processes posts with multiple answers where we only retain answers that will be used for supervised fine-tuning.\n","\n","    Parameters:\n","        example (dict): The input example containing 'dupl_scores_idxs' and 'answers' as dictionary keys.\n","            'dupl_scores_idxs' is a list of indices of duplicate scores, and 'answers' is a dictionary\n","            with lists of the text of answers and their metadata.\n","\n","    Returns:\n","        dict: The modified input example with 'answers' containing only duplicate scores' answers.\n","\n","    \"\"\"\n","\n","    # Retrieve the list of indices of duplicate scores from the 'dupl_scores_idxs' key.\n","    dupl_scores_idxs = example['dupl_scores_idxs']\n","\n","    # Iterate through each key-value pair in the 'answers' dictionary.\n","    for key, val in example['answers'].items():\n","        # Update the 'answers' dictionary by keeping only the text and meta-data\n","        # corresponding to duplicate scores' indices.\n","        example['answers'][key] = [example['answers'][key][i] for i in dupl_scores_idxs]\n","\n","    return example\n","\n","def split_ds(ds_original,\n","             ds_filtered,\n","             output_dir='ds_split',\n","             save_file=True,\n","             log_to_wandb=True,\n","             overwrite=False):\n","    \"\"\"\n","    Splits the datasets into supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL) subsets.\n","\n","    Parameters:\n","        ds_original (Dataset): The original dataset containing all examples.\n","        ds_filtered (Dataset): The filtered dataset containing relevant examples for SFT and RM datasets.\n","        output_dir (str, optional): The directory where the split datasets will be saved.\n","            Default is 'ds_split'.\n","        save_file (bool, optional): If True, saves the split datasets to disk. Default is True.\n","        log_to_wandb (bool, optional): If True, logs the split datasets as a WandB artifact.\n","            Default is True.\n","        overwrite (bool, optional): If True, overwrites existing split datasets in the output directory.\n","            Default is False.\n","\n","    Returns:\n","        dict: A dictionary containing the split datasets for SFT, RM, and RL.\n","    \"\"\"\n","\n","    # Check if the split datasets already exist in the output directory and overwrite is False.\n","    if (all(os.path.exists(f'./data/{output_dir}/{split}') for split in ['ds_SFT', 'ds_RM', 'ds_RL'])\n","        and not overwrite):\n","\n","        ds_split = {}\n","\n","        # Load the split datasets from disk and return them.\n","        ds_split['SFT'] = load_from_disk(f'./data/{output_dir}/ds_SFT')\n","        ds_split['RM'] = load_from_disk(f'./data/{output_dir}/ds_RM')\n","        ds_split['RL'] = load_from_disk(f'./data/{output_dir}/ds_RL')\n","\n","        return ds_split\n","\n","    ds_split = {}\n","\n","    # Filter examples with multiple answers and single answers separately.\n","    ds_mult = ds_filtered.filter(lambda post: len(post['answers']['score']) >= 2)\n","    ds_sing = ds_filtered.filter(lambda post: len(post['answers']['score']) == 1)\n","\n","    # Process examples with multiple answers using the 'mult_ans_RM_proc' function to retain only answers that\n","    # will be used for preference modeling. We choose answers with unique scores to avoid ties during preference modeling.\n","    ds_mult_indexed = ds_mult.map(split_idxs)\n","    ds_split['RM'] = ds_mult_indexed.map(mult_ans_RM_proc)\n","    ds_split['RM'] = ds_split['RM'].filter(lambda x: len(x['answers']['score']) > 0)\n","\n","    # Process examples with multiple answers using the 'mult_ans_SFT_proc' function to retain only duplicate scores' answers.\n","    # These will be added to SFT dataset.\n","    ds_SFT_mult = ds_mult_indexed.map(mult_ans_SFT_proc)\n","    ds_SFT_mult = ds_SFT_mult.filter(lambda x: len(x['answers']['score']) > 0)\n","\n","    # Form SFT dataset by combining answers for posts with a unique answers and the\n","    # answers corresponding to the \"duplicate indices\" for posts with multiple answers.\n","    ds_split['SFT'] = datasets.DatasetDict()\n","\n","    for key in ['train', 'validation', 'test']:\n","        ds_split['SFT'][key] = datasets.concatenate_datasets([ds_SFT_mult[key], ds_sing[key]])\n","\n","    #Remove reddit posts with a low score from the SFT dataset.\n","    ds_split['SFT'] = score_cutoff(ds_split['SFT'])\n","\n","    # Collect the question IDs of examples used in SFT and RM to exclude them from RL.\n","    q_ids_taken = []\n","\n","    for ds_ in (ds_split['SFT'], ds_split['RM']):\n","        for split in ds_:\n","            q_ids_taken.extend(ds_[split]['q_id'])\n","\n","    q_ids_taken = set(q_ids_taken)\n","\n","    # Create the RL subset by excluding examples used in SFT and RM.\n","    ds_split['RL'] = ds_original.filter(lambda post: post['q_id'] not in q_ids_taken)\n","    ds_split['RL'] = datasets.concatenate_datasets([ds for ds in ds_split['RL'].values()])\n","\n","    # Save the split datasets to disk.\n","    if save_file:\n","\n","        for key, value in ds_split.items():\n","            value.save_to_disk(f'./data/{output_dir}/ds_{key}')\n","\n","        # Log the split datasets as a WandB artifact if log_to_wandb is True.\n","        if log_to_wandb:\n","            now = datetime.now()\n","            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","            with wandb.init(project='ELI5_analysis',\n","                            entity='ft-llmmm',\n","                            job_type='split_data',\n","                            name=f'split_data_{time_stamp}') as run:\n","\n","                split_data_art = wandb.Artifact('ELI5_split', 'dataset')\n","                split_data_art.add_dir(f'./data/{output_dir}')\n","                run.log_artifact(split_data_art)\n","\n","    # Return the dictionary containing the split datasets for SFT, RM, and RL.\n","    return ds_split\n","\n","def combine_title_body(example):\n","    \"\"\"\n","    Combines the title and body (selftext) of the input example into a single string and updates the input example.\n","\n","    Parameters:\n","        example (dict): The input example containing 'title' and 'selftext' as keys.\n","\n","    Returns:\n","        dict: The modified input example with the combined string of the title and body\n","              under the key 'title_body'.\n","\n","    \"\"\"\n","\n","    # Remove extra spaces and join the words in the 'title' string.\n","    title = ' '.join(example['title'].split())\n","\n","    # Remove extra spaces and join the words in the 'selftext' string.\n","    selftext = ' '.join(example['selftext'].split())\n","\n","    # Combine the 'title' and 'selftext' strings with a newline separator.\n","    combined = title + '\\n' + selftext\n","\n","    # Add the combined string under the key 'title_body' in the input example.\n","    example['title_body'] = combined\n","\n","    # Return the modified input example.\n","    return example\n","\n","def embed_datasets(dataset_split,\n","                   checkpoint='all-mpnet-base-v2',\n","                   output_dir='embedded',\n","                   save_file=True,\n","                   overwrite=False,\n","                   log_to_wandb=True):\n","    \"\"\"\n","    Embeds the datasets using a pre-trained SentenceTransformer model and saves the embeddings to disk.\n","\n","    Parameters:\n","        dataset_split (dict): A dictionary containing different dataset splits as values (e.g., train, validation).\n","        checkpoint (str, optional): The name of the SentenceTransformer model checkpoint to use.\n","            Default is 'all-mpnet-base-v2'.\n","        output_dir (str, optional): The directory where the embedded datasets will be saved.\n","            Default is 'embedded'.\n","        save_file (bool, optional): If True, saves the embedded datasets to disk. Default is True.\n","        overwrite (bool, optional): If True, overwrites existing embedded datasets in the output directory.\n","            Default is False.\n","        log_to_wandb (bool, optional): If True, logs the embedded datasets as a WandB artifact.\n","            Default is True.\n","\n","    Returns:\n","        dict: A dictionary containing the embedded datasets.\n","\n","    \"\"\"\n","\n","    # Check if the embedded datasets already exist in the output directory and overwrite is False.\n","    if (all(os.path.exists(f'./data/{output_dir}/ds_{subset}') for subset in ['SFT', 'RM', 'RL'])\n","        and not overwrite):\n","\n","        ds_embedded = {}\n","\n","        # Load the embedded datasets from disk and return them.\n","        for subset in ['SFT', 'RM', 'RL']:\n","            ds_embedded[subset] = load_from_disk(f'./data/{output_dir}/ds_{subset}')\n","        return ds_embedded\n","\n","    # Initialize a dictionary to store the embedded datasets.\n","    ds_embedded = {}\n","\n","    # Initialize the SentenceTransformer model.\n","    model = SentenceTransformer(checkpoint)\n","\n","    # Loop through each dataset split and embed the examples.\n","    for key in dataset_split:\n","        ds_embedded[key] = dataset_split[key].map(combine_title_body)\n","        ds_embedded[key] = ds_embedded[key].map(lambda x: {'qu_emb':\n","                                                           model.encode(x['title_body'],\n","                                                                        batch_size=64)})\n","\n","    # Save the embedded datasets to disk.\n","    if save_file:\n","        for key, value in ds_embedded.items():\n","            value.save_to_disk(f'./data/{output_dir}/ds_{key}')\n","\n","        # Log the embedded datasets as a WandB artifact if log_to_wandb is True.\n","        if log_to_wandb:\n","            now = datetime.now()\n","            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","            with wandb.init(project='ELI5_analysis',\n","                            entity='ft-llmmm',\n","                            job_type='embed_data',\n","                            name=f'embed_data_{time_stamp}') as run:\n","\n","                embed_data_art = wandb.Artifact('ELI5_embedded', 'dataset')\n","                embed_data_art.add_dir(f'./data/{output_dir}')\n","                run.log_artifact(embed_data_art)\n","\n","    # Return the dictionary containing the embedded datasets.\n","    return ds_embedded\n","\n","def make_pairs(example):\n","    \"\"\"\n","    Creates pairs of answers from the input example based on their scores and updates the example.\n","\n","    Parameters:\n","        example (dict): The input example containing 'answers' as a dictionary with 'text' and 'score' lists.\n","\n","    Returns:\n","        dict: The modified input example with the 'pairs' key containing the created pairs of answers.\n","\n","    \"\"\"\n","\n","    # Extract the 'text' and 'score' lists from the 'answers' dictionary in the example.\n","    answers = example['answers']['text']\n","    scores = example['answers']['score']\n","\n","    # Create a list of tuples with each tuple containing the score and its corresponding answer.\n","    sc_ans = tuple(zip(scores, answers))\n","\n","    # Generate pairs of tuples using combinations() from the 'sc_ans' list.\n","    sc_pairs = tuple(combinations(sc_ans, 2))\n","\n","    # If the number of pairs is greater than 10, randomly select 10 pairs from the list.\n","    if len(sc_pairs) > 10:\n","        sc_pairs = random.sample(sc_pairs, 10)\n","\n","    # Sort each pair of tuples based on their score in descending order.\n","    sc_pairs = list(map(lambda x: sorted(x, key=lambda y: y[0], reverse=True), sc_pairs))\n","\n","    # Extract the answers from the sorted pairs and create a list of answer pairs.\n","    pairs_text = [(sc_pair[0][1], sc_pair[1][1]) for sc_pair in sc_pairs]\n","\n","    # Add the 'pairs' key to the input example with the created answer pairs.\n","    example['pairs'] = pairs_text\n","\n","    # Return the modified input example.\n","    return example\n","\n","def clean_datasets(ds_embedded,\n","                   cutoff=0.6,\n","                   batch_size=5000,\n","                   output_dir='cleaned',\n","                   save_file=True,\n","                   overwrite=False,\n","                   log_to_wandb=True):\n","    \"\"\"\n","    Cleans the datasets by removing redundant examples based on the similarity of embedded vectors.\n","\n","    Parameters:\n","        ds_embedded (dict): A dictionary containing the embedded datasets for supervised fine-tuning (SFT),\n","                            reward modeling (RM), and reinforcement learning (RL).\n","        cutoff (float, optional): The similarity threshold to consider examples as redundant.\n","            Default is 0.6.\n","        batch_size (int, optional): The batch size used for processing RL dataset.\n","            Default is 5000.\n","        output_dir (str, optional): The directory where the cleaned datasets will be saved.\n","            Default is 'cleaned'.\n","        save_file (bool, optional): If True, saves the cleaned datasets to disk. Default is True.\n","        overwrite (bool, optional): If True, overwrites existing cleaned datasets in the output directory.\n","            Default is False.\n","        log_to_wandb (bool, optional): If True, logs the cleaned datasets as a WandB artifact.\n","            Default is True.\n","\n","    Returns:\n","        dict: A dictionary containing the cleaned datasets for SFT, RM, and RL.\n","\n","    \"\"\"\n","\n","    #ds_clean is a dictionary which contains DatasetDicts as values.\n","    ds_clean = {}\n","\n","    # Check if the cleaned datasets already exist in the output directory and overwrite is False.\n","    if (all(os.path.exists(f'./data/{output_dir}/ds_{subset}') for subset in ['SFT', 'RM', 'RL'])\n","        and not overwrite):\n","\n","        # Load the cleaned datasets from disk and return them.\n","        for subset in ['SFT', 'RM', 'RL']:\n","            ds_clean[subset] = load_from_disk(f'./data/{output_dir}/ds_{subset}')\n","        return ds_clean\n","\n","    # Initialize dictionaries to store normalized embedding vectors and overlaps between splits for SFT and RM datasets.\n","    embed_vecs = {}\n","    overlaps = {}\n","    idxs = {}\n","\n","    # standard splitting of data in supervised learning.\n","    splits = ['train', 'validation', 'test']\n","\n","    # Cleaning SFT and RM datasets.\n","    for subset in ['SFT', \"RM\"]:\n","        print(f'Cleaning {subset} dataset')\n","\n","        # Set the format of dataset to 'torch' to enable torch operations on the embedded vectors.\n","        ds_embedded[subset].set_format('torch')\n","        embed_vecs[subset] = {}\n","\n","        # Normalize the embedded vectors for each split.\n","        for split in splits:\n","            embed_vecs[subset][split] = ds_embedded[subset][split]['qu_emb']\n","            embed_vecs[subset][split] /= torch.sqrt(torch.sum(embed_vecs[subset][split] ** 2,\n","                                                             dim=1,\n","                                                             keepdim=True))\n","\n","        overlaps[subset] = {}\n","        idxs[subset] = {}\n","\n","        # Compute the overlaps between splits and store the indices of redundant examples.\n","        for j in range(1, 3):\n","            for i in range(j):\n","                overlaps[subset][(splits[i], splits[j])] = torch.matmul(\n","                    embed_vecs[subset][splits[i]],\n","                    embed_vecs[subset][splits[j]].T\n","                )\n","\n","                idxs[subset][(splits[i], splits[j])] = torch.where((overlaps[subset][(splits[i], splits[j])]) >= cutoff)\n","\n","        # Find indices of examples to remove from the training set due to overlap between train and validation splits.\n","        rm_tr_idxs_temp = idxs[subset]['train', 'validation'][0].numpy()\n","        rm_tr_idxs_temp = set(rm_tr_idxs_temp)\n","\n","        # Find indices of examples to remove from the training set due to overlap between train and test splits.\n","        rm_tr_idxs = idxs[subset]['train', 'test'][0].numpy()\n","        rm_tr_idxs = set(rm_tr_idxs).union(rm_tr_idxs_temp)\n","\n","        # Indices to keep in train set.\n","        keep_train = set(range(len(ds_embedded[subset]['train']))) - rm_tr_idxs\n","\n","        # Find indices of examples to remove from the test set due to overlap between validation and test splits.\n","        # Remove examples from test set because it is larger than the validation set.\n","        rm_test_idxs = idxs[subset]['validation', 'test'][1].numpy()\n","        rm_test_idxs = set(rm_test_idxs)\n","\n","        # Indices to keep in train set.\n","        keep_test = set(range(len(ds_embedded[subset]['test']))) - rm_test_idxs\n","\n","        # Create a new DatasetDict containing the cleaned subsets for SFT and RM.\n","        ds_clean[subset] = datasets.DatasetDict()\n","\n","        ds_clean[subset]['train'] = ds_embedded[subset]['train'].select(keep_train)\n","        ds_clean[subset]['validation'] = ds_embedded[subset]['validation']\n","        ds_clean[subset]['test'] = ds_embedded[subset]['test'].select(keep_test)\n","\n","    # Cleaning RL dataset.\n","    print(f'Cleaning RL dataset')\n","\n","    # Set the format of RL dataset to 'torch' to enable torch operations on the embedded vectors.\n","    ds_embedded['RL'].set_format('torch')\n","\n","    # Extract the embedded vectors for the RL dataset.\n","    embed_vecs['RL'] = ds_embedded['RL']['qu_emb']\n","\n","    # Normalize the embedded vectors by dividing them by their L2 norm.\n","    embed_vecs['RL'] /= torch.sqrt(torch.sum(embed_vecs['RL'] ** 2,\n","                                             dim=1,\n","                                             keepdim=True))\n","    # Get the size of the RL dataset (number of examples).\n","    RL_size = len(ds_embedded['RL'])\n","\n","    # Create an empty set to store the indices of redundant examples in the RL dataset.\n","    rem_RL = set()\n","\n","    # Initialize a variable to keep track of the start index of each batch.\n","    start = 0\n","\n","    # Calculate the number of batches based on the batch size.\n","    num_batches = RL_size // batch_size\n","\n","    # If the size of RL dataset is not perfectly divisible by batch_size, add one extra batch.\n","    if RL_size % batch_size != 0:\n","        num_batches += 1\n","\n","    # Loop through each batch and compute overlaps with SFT and RM datasets to find redundant examples.\n","    for k in tqdm(range(num_batches)):\n","\n","        # Calculate the start and end index of the current batch.\n","        start = k * batch_size\n","        end = (k + 1) * batch_size\n","\n","        # Get the current batch of embedded vectors.\n","        batch = embed_vecs['RL'][start:start + batch_size, :]\n","\n","        # Compute overlaps between the current batch and the SFT and RM datasets.\n","        for subset in ['SFT', 'RM']:\n","            for split in ['train', 'validation']:\n","                overlap = torch.matmul(embed_vecs[subset][split], batch.T)\n","\n","                # Find the indices of redundant examples in the current batch.\n","                rem_RL_idxs_temp = torch.where(overlap >= cutoff)[1].numpy()\n","\n","                # Update the set of indices of redundant examples in the entire RL dataset.\n","                rem_RL = rem_RL.union(set(rem_RL_idxs_temp))\n","\n","    # Create a set containing all the indices of the RL dataset.\n","    keep_RL = set(range(RL_size))\n","\n","    # Remove the indices of redundant examples from the set to get non-redundant examples.\n","    keep_RL -= set(rem_RL)\n","\n","    # Select non-redundant examples for RL dataset.\n","    ds_clean['RL'] = ds_embedded['RL'].select(keep_RL)\n","\n","    # Apply 'make_pairs' function to the RM dataset to create pairs of answers.\n","    ds_clean['RM'] = ds_clean['RM'].map(lambda x: make_pairs(x))\n","\n","    # Save the cleaned datasets to disk.\n","    if save_file:\n","        for subset in ['SFT', 'RM', 'RL']:\n","            ds_clean[subset].save_to_disk(f'./data/{output_dir}/ds_{subset}')\n","\n","        # Log the cleaned datasets as a WandB artifact if log_to_wandb is True.\n","        if log_to_wandb:\n","            now = datetime.now()\n","            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","            with wandb.init(project='ELI5_analysis',\n","                            entity='ft-llmmm',\n","                            job_type='clean_data',\n","                            name=f'clean_data_{time_stamp}') as run:\n","\n","                clean_data_art = wandb.Artifact('ELI5_cleaned', 'dataset')\n","                clean_data_art.add_dir(f'./data/{output_dir}')\n","                run.log_artifact(clean_data_art)\n","\n","    # Return the dictionary containing the cleaned datasets for SFT, RM, and RL.\n","    return ds_clean\n"]},{"cell_type":"markdown","metadata":{"id":"nPGvnBmK-l-g"},"source":["# Code"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273,"referenced_widgets":["81116b5bec39468c922d30338261fc8b","460fe9a48eb34076a857903e2c8725e8","39016a1cdaf84d8090ed746692de0338","b93db86b4ee14e64b2c502bb7f311be4","00dd4573a54543cf86e11cf7701090f3","eb2ffb68d17b48f3a303cc15120a5809","1ee22112fedc4f55aa84e01a76b5cdc9","c343600e4efe4cc180e28f1806d2ddf3"]},"executionInfo":{"elapsed":340060,"status":"ok","timestamp":1691186182474,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"N_IUO6JoKKv6","outputId":"3c4f6cd8-644b-479a-8f5b-e19aff66db0d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81116b5bec39468c922d30338261fc8b","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"460fe9a48eb34076a857903e2c8725e8","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/687M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39016a1cdaf84d8090ed746692de0338","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b93db86b4ee14e64b2c502bb7f311be4","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/37.9M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00dd4573a54543cf86e11cf7701090f3","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eb2ffb68d17b48f3a303cc15120a5809","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ee22112fedc4f55aa84e01a76b5cdc9","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c343600e4efe4cc180e28f1806d2ddf3","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Download original dataset.\n","ds_original = load_dataset(\"vblagoje/lfqa\")\n","\n","# Preprocess dataset to clean up text, remove posts with short answers, etc.\n","ds_filtered = preprocess_data(ds_original)\n","\n","# Split dataset into SFT, RM, and RL subsets.\n","ds_split = split_ds(ds_original,\n","                    ds_filtered)\n","\n","# Embed answers using sentence transformers.\n","ds_embedded = embed_datasets(ds_split)\n","\n","# Use embedded answers to detect and remove dataleakage.\n","ds_clean = clean_datasets(ds_embedded)"]},{"cell_type":"markdown","metadata":{"id":"bnwa8ii0TExb"},"source":["# Further Cleaning"]},{"cell_type":"code","source":["def remove_more_posts(cleaned_folder = 'cleaned_V3',\n","                      artifact_name = 'ELI5_cleaned'):\n","\n","    run = wandb.init()\n","    artifact = run.use_artifact('ft-llmmm/ELI5_analysis/ELI5_cleaned:v2', type='dataset')\n","    artifact_dir = artifact.download()\n","\n","    ds={}\n","\n","    for key in ['SFT','RL','RM']:\n","        ds[key] = load_from_disk(f'{artifact_dir}/ds_{key}')\n","\n","    filter_words = {'mod post','mods','moderator','meta ',\n","                '[meta]','ask me anything','meetup','floating feature','twenty-year rule',\n","                'askHistorians podcast episode','default subreddit',\n","                'state of the subreddit','Rules Roundtable'}\n","\n","    for key in ['SFT','RL','RM']:\n","        ds[key] = ds[key].filter(lambda x: not any(word in x['title_body'].lower()\n","                                            for word in filter_words))\n","\n","    for key in ['SFT','RL','RM']:\n","        ds[key].save_to_disk(f'./data/{cleaned_folder}/ds_{key}')\n","\n","    now = datetime.now()\n","    file_name = './data/{cleaned_folder}'\n","    time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","\n","    with wandb.init(project='ELI5_analysis',\n","                                entity='ft-llmmm',\n","                                job_type='log_data',\n","                                name=f'ELI5_cleaning_{time_stamp}') as run:\n","                    # Initialize a WandB run for logging\n","                    data_art = wandb.Artifact(artifact_name, 'dataset')\n","                    data_art.add_dir(file_name)\n","                    run.log_artifact(data_art)"],"metadata":{"id":"0IeJPP3u5ydL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Scratch (WIP)"],"metadata":{"id":"UBGiEjAW7bcH"}},{"cell_type":"markdown","metadata":{"id":"WzWtP6WZMmWN"},"source":["## Detoxify RM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRVtDLcSMpEm"},"outputs":[],"source":["!pip install detoxify\n","\n","from detoxify import Detoxify\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","detoxify_model = Detoxify('unbiased')\n","detoxify_model.model.to(device)"]},{"cell_type":"code","source":["ELI5_RM_ds = datasets.load_from_disk(f'./data/cleaned_V3/ds_RM')\n","\n","ELI5_RM_ds.set_format('pandas')\n","ELI5_RM_ds=ELI5_RM_ds.flatten()\n","ELI5_RM_ds.set_format(None)"],"metadata":{"id":"oV0SXgy8UY7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ELI5_RM_ds = ELI5_RM_ds.map(\n","    lambda x: {'toxicity_scores':\n","     [detoxify_model.predict(answer) for answer in x['answers.text']]},\n","    batched=True,batch_size=64)"],"metadata":{"id":"IOErrVR_VPUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics = ['identity_attack','insult',\n","           'obscene','severe_toxicity',\n","           'sexual_explicit','threat',\n","           'toxicity']"],"metadata":{"id":"qd_FCUa3-wLq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["answer_feats = [col for col in ELI5_RM_ds['train'][0].keys() if\n","                'answer' in col]\n","\n","ELI5_RM_ds_tox_scores = ELI5_RM_ds.map(lambda x:\n","                        {'non_tox_bool': [True if all(x['toxicity_scores'][metric][i]<=0.1 for metric in metrics) else False for\n","                                        i in range(len(x['answers.score']))]})\n","\n","ELI5_RM_non_tox = ELI5_RM_ds_tox_scores.map(\n","    lambda x: {answer_feat:list(compress(x[answer_feat],\n","                                    x['non_tox_bool']))\n","    for answer_feat in answer_feats}\n",")\n","\n","ELI5_RM_filt = ELI5_RM_non_tox.filter(lambda x:len(x['answers.score'])>=2)"],"metadata":{"id":"igvbG9Z3-lxi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ELI5_RM_filt.save_to_disk('./data/RM_non_toxic')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["d1175ca6066a4b18a2e47d76fc1c66ee","829a41cd62c54eeaa114de24c2e0fd50","6c3e208533664d30ade01dc9a7eaa191","7839c33efe75470ab02a8198d1cfa215","ef69745754624b3d868d3b72e2a7cb80","bf4c71ecf45b469ab65cbac72a85716e","c62f0ffe680f420f968181899c4e83fa","72a4724e7259410597ea8a8d075317cf","8cab82bc3cd74cd9917968ebc6f1c174","0b4ff40d05fb4ec8afc0b43389037348","b46dbf57e3e44db3a2d369cf33df8f4e","83e1a07d27d44429b35174ba723c448d","99ccb8b9fb574299b15571eab4227b49","f35a2e6cb5e94a269892099d3b7aeebb","59ee905462624bbda8b8c514d70a21f4","6d31cc31b60f49ce93066a85aa41d612","b01e1a7f041a44a19874e149a5fbb7ab","0a922ef7fa45481492733dac3c87b6c5","d5bb778f0157472cb6f9b62a5d8a4f9c","9f513784a23c4e86906749ea030d2e64","d73be62ed7e64c08a905cfbe47530493","2e0fb03745404e75b9dc7702b09c7c22","17d08660344847d08557578e315d14b1","f8c807daa931447da6b58c650c4f2797","c90ec34199c541d9aeedbcf022d214e4","dc2c0a2e604d48d5a4b3bcab99b66451","4d3741655d334f17a4018a9cf8a3f400","2d3214b14ab345e5b3321d3cf8fc2a2d","6c4a7a16b3eb4ee7ae5003afd5b2a540","52f52c1927674153bbc634ec1623b7cf","ad68b8cbdc1c48c2b591888fb4e9d510","43a4f027e13641d393c30bb88ccdd278","b2092001ad374960b735fbcefab94373"]},"id":"CpsiS9XGNtMw","executionInfo":{"status":"ok","timestamp":1692155474342,"user_tz":240,"elapsed":5368,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"}},"outputId":"8aedb894-1aaf-47bf-faac-8e39cd711612"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/39811 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1175ca6066a4b18a2e47d76fc1c66ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/1392 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83e1a07d27d44429b35174ba723c448d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/2914 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d08660344847d08557578e315d14b1"}},"metadata":{}}]},{"cell_type":"code","source":["now = datetime.now()\n","file_name = './data/RM_non_toxic'\n","time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","\n","with wandb.init(project='ELI5_analysis',\n","                entity='ft-llmmm',\n","                job_type='log_data',\n","                name=f'RM_non_toxic_{time_stamp}') as run:\n","\n","                data_art = wandb.Artifact('ELI5_RM_non_toxic', 'dataset')\n","                data_art.add_dir(file_name)\n","                run.log_artifact(data_art)"],"metadata":{"id":"2VTGiTC4RNQi"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["bdp_5O98KF7-","UBGiEjAW7bcH","WzWtP6WZMmWN"],"provenance":[],"mount_file_id":"1gzbxeUeFa7tMCftY9gaqYmb76hSHev9s","authorship_tag":"ABX9TyP8W5eo19Rae2jjQsK4BzN+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d1175ca6066a4b18a2e47d76fc1c66ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_829a41cd62c54eeaa114de24c2e0fd50","IPY_MODEL_6c3e208533664d30ade01dc9a7eaa191","IPY_MODEL_7839c33efe75470ab02a8198d1cfa215"],"layout":"IPY_MODEL_ef69745754624b3d868d3b72e2a7cb80"}},"829a41cd62c54eeaa114de24c2e0fd50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf4c71ecf45b469ab65cbac72a85716e","placeholder":"​","style":"IPY_MODEL_c62f0ffe680f420f968181899c4e83fa","value":"Saving the dataset (1/1 shards): 100%"}},"6c3e208533664d30ade01dc9a7eaa191":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72a4724e7259410597ea8a8d075317cf","max":39811,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8cab82bc3cd74cd9917968ebc6f1c174","value":39811}},"7839c33efe75470ab02a8198d1cfa215":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b4ff40d05fb4ec8afc0b43389037348","placeholder":"​","style":"IPY_MODEL_b46dbf57e3e44db3a2d369cf33df8f4e","value":" 39811/39811 [00:05&lt;00:00, 7072.91 examples/s]"}},"ef69745754624b3d868d3b72e2a7cb80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf4c71ecf45b469ab65cbac72a85716e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c62f0ffe680f420f968181899c4e83fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72a4724e7259410597ea8a8d075317cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cab82bc3cd74cd9917968ebc6f1c174":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b4ff40d05fb4ec8afc0b43389037348":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b46dbf57e3e44db3a2d369cf33df8f4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83e1a07d27d44429b35174ba723c448d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99ccb8b9fb574299b15571eab4227b49","IPY_MODEL_f35a2e6cb5e94a269892099d3b7aeebb","IPY_MODEL_59ee905462624bbda8b8c514d70a21f4"],"layout":"IPY_MODEL_6d31cc31b60f49ce93066a85aa41d612"}},"99ccb8b9fb574299b15571eab4227b49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b01e1a7f041a44a19874e149a5fbb7ab","placeholder":"​","style":"IPY_MODEL_0a922ef7fa45481492733dac3c87b6c5","value":"Saving the dataset (1/1 shards): 100%"}},"f35a2e6cb5e94a269892099d3b7aeebb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5bb778f0157472cb6f9b62a5d8a4f9c","max":1392,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f513784a23c4e86906749ea030d2e64","value":1392}},"59ee905462624bbda8b8c514d70a21f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d73be62ed7e64c08a905cfbe47530493","placeholder":"​","style":"IPY_MODEL_2e0fb03745404e75b9dc7702b09c7c22","value":" 1392/1392 [00:00&lt;00:00, 5979.64 examples/s]"}},"6d31cc31b60f49ce93066a85aa41d612":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b01e1a7f041a44a19874e149a5fbb7ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a922ef7fa45481492733dac3c87b6c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5bb778f0157472cb6f9b62a5d8a4f9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f513784a23c4e86906749ea030d2e64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d73be62ed7e64c08a905cfbe47530493":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e0fb03745404e75b9dc7702b09c7c22":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17d08660344847d08557578e315d14b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f8c807daa931447da6b58c650c4f2797","IPY_MODEL_c90ec34199c541d9aeedbcf022d214e4","IPY_MODEL_dc2c0a2e604d48d5a4b3bcab99b66451"],"layout":"IPY_MODEL_4d3741655d334f17a4018a9cf8a3f400"}},"f8c807daa931447da6b58c650c4f2797":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d3214b14ab345e5b3321d3cf8fc2a2d","placeholder":"​","style":"IPY_MODEL_6c4a7a16b3eb4ee7ae5003afd5b2a540","value":"Saving the dataset (1/1 shards): 100%"}},"c90ec34199c541d9aeedbcf022d214e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52f52c1927674153bbc634ec1623b7cf","max":2914,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad68b8cbdc1c48c2b591888fb4e9d510","value":2914}},"dc2c0a2e604d48d5a4b3bcab99b66451":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43a4f027e13641d393c30bb88ccdd278","placeholder":"​","style":"IPY_MODEL_b2092001ad374960b735fbcefab94373","value":" 2914/2914 [00:00&lt;00:00, 7629.61 examples/s]"}},"4d3741655d334f17a4018a9cf8a3f400":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d3214b14ab345e5b3321d3cf8fc2a2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c4a7a16b3eb4ee7ae5003afd5b2a540":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52f52c1927674153bbc634ec1623b7cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad68b8cbdc1c48c2b591888fb4e9d510":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43a4f027e13641d393c30bb88ccdd278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2092001ad374960b735fbcefab94373":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}