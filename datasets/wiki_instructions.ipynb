{"cells":[{"cell_type":"markdown","metadata":{"id":"XW-LivoDXXC1"},"source":["# Dependencies\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":217,"status":"ok","timestamp":1692720722217,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"jawN_hlOX2_a","outputId":"97045216-cc9e-4af4-df0e-854136c55dc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/LLMs/wikipedia\n"]}],"source":["%cd drive/MyDrive/LLMs/wikipedia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hD27kov2XaVw"},"outputs":[],"source":["!pip install datasets\n","!pip install textstat\n","!pip install wandb\n","!pip install apache-beam\n","!pip install transformers[torch] evaluate\n","!pip install mwparserfromhell\n","!pip install openai\n","!pip install wandb\n","!pip install tiktoken"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4ioIJs3YDdS"},"outputs":[],"source":["# Importing necessary libraries\n","\n","from datasets import load_dataset, load_from_disk, DatasetDict  # Importing functions to load datasets and dataset dictionaries\n","from textstat import flesch_reading_ease as fre, flesch_kincaid_grade as fkg  # Importing functions for text readability metrics\n","from tqdm import tqdm  # Importing tqdm for progress bar display\n","import pandas as pd  # Importing pandas for data manipulation and analysis\n","import matplotlib.pyplot as plt  # Importing matplotlib for data visualization\n","import re  # Importing the regular expression module for string pattern matching and manipulation\n","\n","import os  # Importing the os module to interact with the operating system\n","import openai  # Importing the OpenAI library for accessing AI models and APIs\n","import json  # Importing the json module for working with JSON data\n","from getpass import getpass  # Importing getpass to securely get password input from the user\n","\n","from scipy.stats import kstest  # Importing kstest from scipy.stats for the Kolmogorov-Smirnov test\n","import numpy as np  # Importing numpy for numerical operations\n","import wandb  # Importing wandb for experiment tracking and visualization\n","import datetime  # Importing datetime for handling date and time data\n","from datetime import datetime  # Importing datetime for more date and time functionality\n","from collections import defaultdict  # Importing defaultdict for creating dictionaries with default values\n","from time import time, sleep  # Importing time and sleep for measuring time duration\n","\n","from itertools import product  # Importing itertools for efficient looping and combination generation\n","import tiktoken  # Importing tiktoken for counting tokens in a text\n","\n","from tenacity import (\n","    retry,\n","    stop_after_attempt,\n","    wait_random_exponential,\n",")  # for exponential backoff\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7986,"status":"ok","timestamp":1692720852578,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"ZiYMyvVCUrZC","outputId":"d54b7401-1fe6-4477-91ce-88c5da741cd9"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["wandb.login()"]},{"cell_type":"markdown","metadata":{"id":"3P-xqCRBZf-0"},"source":["# Definitions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pAGC-Q_IZhig"},"outputs":[],"source":["def flesch_scores(example):\n","    \"\"\"\n","    Calculate Flesch Readability scores for each article.\n","\n","    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade scores for the given 'example' text.\n","    The Flesch Reading Ease score measures how easy a text is to read (higher scores are easier to read),\n","    while the Flesch-Kincaid Grade score estimates the U.S. grade level required to understand the text.\n","\n","    Parameters:\n","        example (dict): A dictionary object containing the 'text' key containing the article text.\n","\n","    Returns:\n","        dict: The 'example' dictionary updated with two additional keys: 'fre' and 'fkg', representing the respective scores.\n","    \"\"\"\n","\n","    text = example['text']\n","    # Calculate Flesch Reading Ease and Flesch-Kincaid Grade scores using the 'fre()' and 'fkg()' functions from the 'textstat' library.\n","    # They are stored in the 'fre' and 'fkg' keys of the 'example' dictionary, respectively.\n","    example['fre'] = fre(text)\n","    example['fkg'] = fkg(text)\n","\n","    # Return the updated 'example' dictionary with the Flesch scores added.\n","    return example\n","\n","\n","def remove_text_between_curly_braces(text):\n","    \"\"\"\n","    Removes text enclosed in curly braces from the input text.\n","\n","    Args:\n","        text (str): The input text containing curly braces.\n","\n","    Returns:\n","        str: The modified text with the content inside curly braces removed.\n","    \"\"\"\n","    # Define the pattern to match text between double curly braces\n","    pattern_double_curly = r\"{{(.|\\n)*?}}\"\n","    # Use regular expression substitution to remove text between double curly braces\n","    text = re.sub(pattern_double_curly, \"\", text)\n","\n","    # Define the pattern to match text between single curly braces\n","    pattern_single_curly = r\"{(.|\\n)*?}\"\n","    # Use regular expression substitution to remove text between single curly braces\n","    text = re.sub(pattern_single_curly, \"\", text)\n","\n","    return text\n","\n","\n","def extract_article_text(example):\n","    \"\"\"\n","    Extracts the text from a Wikipedia article before the \"Related pages\" and \"References\" sections.\n","\n","    Parameters:\n","        example (dict): Dictionary with key 'text' which contains entire Wikipedia article content as a string.\n","\n","    Returns:\n","        str: The article text before the \"Related pages\" and \"References\" sections.\n","    \"\"\"\n","    text = example['text']\n","\n","    # Find the positions of the \"Related pages\" and \"References\" sections\n","    txt_lower = text.lower()\n","\n","    related_pages_position = txt_lower.find('related pages')\n","    references_position = txt_lower.find('references')\n","\n","    # Extract the text before the sections\n","    article_text = \"\"\n","    if related_pages_position != -1:\n","        # If the \"Related pages\" section is found, extract the text before it\n","        article_text = text[:related_pages_position]\n","    elif references_position != -1:\n","        # If the \"References\" section is found (but not the \"Related pages\" section),\n","        # extract the text before it\n","        article_text = text[:references_position]\n","    else:\n","        # If both sections are not present, return the entire text\n","        article_text = text\n","\n","    article_text = article_text.replace('  ',' ')\n","    article_text = remove_text_between_curly_braces(article_text)\n","\n","    example['text'] = article_text\n","\n","    return example\n","\n","class TruncArticleWrapper:\n","    \"\"\"\n","    Wrapper class for truncating articles.\n","\n","    Attributes:\n","        cutoff_length (int): Truncate article once length >= cutoff_length. Default is 300.\n","    \"\"\"\n","\n","    def __init__(self, cutoff_length=300):\n","        \"\"\"\n","        Initializes the TruncArticleWrapper object.\n","\n","        Args:\n","            cutoff_length (int, optional): Cutoff target for truncated text. Default is 300.\n","                                           Stop adding to article once cutoff target is reached.\n","        \"\"\"\n","        self.cutoff_length = cutoff_length\n","\n","    def trunc_article_func(self, example):\n","        \"\"\"\n","        Truncates the text of an article.\n","\n","        Args:\n","            example (dict): A dictionary containing the article text.\n","\n","        Returns:\n","            dict: The dictionary with the truncated text stored under the key 'trunc_text'.\n","        \"\"\"\n","\n","        text = example['text']\n","        paragraphs = text.split('\\n\\n')\n","\n","        trunc_text = paragraphs[0]\n","        i = 1\n","\n","        while len(trunc_text) <= self.cutoff_length and i < len(paragraphs):\n","            if len(paragraphs[i].split()) >= 5:\n","                trunc_text += ' ' + paragraphs[i]\n","            i += 1\n","\n","        if '.' in trunc_text:\n","            idx = trunc_text[::-1].index('.')\n","            trunc_text = trunc_text[:len(trunc_text)-idx]\n","\n","        example['trunc_text'] = trunc_text\n","        return example\n","\n","\n","def trunc_article(dataset, cutoff_length=300):\n","    \"\"\"\n","    Applies TruncArticleWrapper.trunc_article_func to a dataset.\n","\n","    Args:\n","        dataset (Dataset): The dataset containing articles.\n","        cutoff_length (int, optional): Cutoff length truncated text. Default is 300.\n","\n","    Returns:\n","        Dataset: The modified dataset with truncated articles.\n","    \"\"\"\n","    # Create an instance of TruncArticleWrapper with the specified cutoff_length\n","    f = trunc_article_wrapper(cutoff_length).trunc_article_func\n","\n","    # Apply the truncation function to each article in the dataset\n","    dataset = dataset.map(lambda article: f(article))\n","\n","    return dataset\n","\n","\n","def create_prompt(prompt_start, text, prompt_end):\n","    \"\"\"\n","    Creates a prompt by combining a start phrase, main text, and an end phrase.\n","\n","    Args:\n","        prompt_start (str): The initial part of the prompt.\n","        text (str): The main text or content to be included in the prompt.\n","        prompt_end (str): The concluding part of the prompt.\n","\n","    Returns:\n","        str: The combined prompt string.\n","    \"\"\"\n","    # Combine the provided parts into a single prompt string\n","    prompt = f\"{prompt_start} {text} {prompt_end}\"\n","    return prompt\n","\n","\n","def GPT_custom_prompt(message, prompt_start, text, prompt_end, model_engine='gpt-3.5-turbo'):\n","    \"\"\"\n","    Generate a custom prompt for a GPT-based language model and retrieve its response.\n","\n","    Args:\n","        message (str): A system-level instruction or context message.\n","        prompt_start (str): The initial part of the prompt.\n","        text (str): The main text or content to be included in the prompt.\n","        prompt_end (str): The concluding part of the prompt.\n","        model_engine (str, optional): The name or ID of the GPT model to be used. Default is 'gpt-3.5-turbo'.\n","\n","    Returns:\n","        str: The generated instruction from the model.\n","    \"\"\"\n","    # Create a custom prompt using provided components\n","    prompt = create_prompt(prompt_start, text, prompt_end)\n","\n","    # Make a request to OpenAI's ChatCompletion API\n","    response = openai.ChatCompletion.create(\n","        model=model_engine,\n","        messages=[\n","            {\"role\": \"system\", \"content\": message},\n","            {\"role\": \"user\", \"content\": prompt},\n","        ],\n","        temperature=0\n","    )\n","\n","    # Extract the generated instruction from the response\n","    instruction = response['choices'][0]['message']['content']\n","\n","    return instruction\n","\n","\n","@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n","def form_question(row, text_key='trunc_text'):\n","    \"\"\"\n","    Formulate a question using a GPT-based language model with retry functionality.\n","\n","    Args:\n","        row (dict): A dictionary containing the article information.\n","        text_key (str, optional): The key to access the text in the row dictionary. Default is 'trunc_text'.\n","\n","    Returns:\n","        str: The generated question.\n","    \"\"\"\n","    # Generate a custom prompt for forming a question\n","    question = GPT_custom_prompt(message, prompt_template[0], row[text_key], prompt_template[1])\n","\n","    return question\n","\n","\n","def make_instruct_dataset(dataset,\n","                          message,\n","                          prompt_template,\n","                          split='train',\n","                          text_key='trunc_text',\n","                          length=10**3,\n","                          file_name_prefix='df_wiki_questions'):\n","    \"\"\"\n","    Create an instruction dataset by generating questions from a given dataset.\n","\n","    Args:\n","        dataset (datasets.DatasetDict): The Huggingface dataset containing articles.\n","        message (str): The system-level instruction or context message.\n","        prompt_template (list): A list of two elements representing the prompt start and end.\n","        split (str, optional): The split of the dataset to use (e.g., 'train', 'validation'). Default is 'train'.\n","        text_key (str, optional): The key to access the text in the row dictionary. Default is 'trunc_text'.\n","        length (int, optional): The desired length of the instruction dataset. Default is 1000.\n","        file_name_prefix (str, optional): The output file name prefix for the instruction dataset. Default is 'df_wiki_questions'.\n","    \"\"\"\n","    # Define the output file path\n","    output_file = f'./data/{file_name_prefix}_{split}.csv'\n","\n","    # Check if the output file already exists\n","    if os.path.exists(output_file):\n","        print('reloading file')\n","        # If it does, load the existing DataFrame\n","        df = pd.read_csv(output_file,\n","                         index_col='Unnamed: 0')\n","        # Get the previous length of the DataFrame\n","        prev_length = len(df)\n","\n","        print(f'length of dataset is {prev_length}')\n","\n","    else:\n","        # If the output file doesn't exist, create a new DataFrame\n","        df = pd.DataFrame(columns=['id',\n","                                   'system_message',\n","                                   'prompt_template',\n","                                   'question'])\n","        prev_length = 0\n","\n","    # Access the dataset split\n","    ds = dataset[split]\n","\n","    # Iterate over a range of indices\n","    for i in tqdm(range(prev_length,length)):\n","        # Get the row from the dataset\n","        row = ds[i]\n","\n","        # Check if the ID is already in the DataFrame, skip if true\n","        if int(row['id']) in set(df['id']):\n","            continue\n","\n","        # Create a temporary dictionary to store information\n","        temp_dict = {}\n","        temp_dict['id'] = row['id']\n","        temp_dict['system_message'] = message\n","        temp_dict['prompt_template'] = prompt_template\n","        temp_dict['question'] = 'pass'\n","\n","        # Generate a question using the 'form_question' function\n","        temp_dict['question'] = form_question(row, text_key='trunc_text')\n","\n","        # Add the temporary dictionary as a new row in the DataFrame\n","        df.loc[len(df)] = temp_dict\n","\n","        # Save the DataFrame to the output file\n","        df.to_csv(output_file)\n"]},{"cell_type":"markdown","metadata":{"id":"Ot6Czw-zntay"},"source":["# Filtering Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qW78yWD_oGUp"},"outputs":[],"source":["def filter_simple_wiki_ds(min_length=50,\n","                          max_length=500,\n","                          fre_cutoff=60,\n","                          fkg_cutoff=9,\n","                          val_size=5000,\n","                          test_size=5000,\n","                          output_file='./data/simple_wiki_split_long',\n","                          artifact_name='simple_wiki_split_long',\n","                          seed=42):\n","    \"\"\"\n","    Filter and process the Simple English Wikipedia dataset.\n","\n","    Args:\n","        min_length (int, optional): Minimum length of articles (in words). Default is 50.\n","        max_length (int, optional): Maximum length of articles (in words). Default is 500.\n","        fre_cutoff (int, optional): Flesch readability score cutoff. Default is 60.\n","        fkg_cutoff (int, optional): Flesch-Kincaid Grade Level score cutoff. Default is 9.\n","        val_size (int, optional): Size of the validation set. Default is 5000.\n","        test_size (int, optional): Size of the test set. Default is 5000.\n","        output_file (str, optional): Output file path for saving the processed dataset. Default is './data/simple_wiki_split_long'.\n","        artifact_name (str, optional): Name for the saved dataset artifact. Default is 'simple_wiki_split_long'.\n","        seed (int, optional): Random seed for reproducibility. Default is 42.\n","    \"\"\"\n","\n","    # Load the Simple English Wikipedia dataset\n","    simple_wiki = load_dataset(\"wikipedia\", \"20220301.simple\")\n","\n","    # Extract article text\n","    simple_wiki_articles = simple_wiki.map(extract_article_text)\n","\n","    # Truncate articles\n","    simple_wiki_trunc = trunc_article(simple_wiki_articles)\n","\n","    # Filter by length\n","    simple_wiki_trunc = simple_wiki_trunc.filter(lambda x: min_length <= len(x['trunc_text'].split()) <= max_length)\n","\n","    # Calculate Flesch scores\n","    simple_wiki_scored = simple_wiki_trunc.map(flesch_scores)\n","\n","    # Filter by readability scores\n","    simple_wiki_filt = simple_wiki_scored.filter(lambda article: article['fre'] >= fre_cutoff and article['fkg'] < fkg_cutoff)\n","\n","    # Filter out articles by specific characters to remove wiki metadata.\n","    simple_wiki_filt = simple_wiki_filt.filter(lambda article: not any(spec_chr in article['trunc_text']\n","                                                                    for spec_chr in {'{{', 'class=', 'infobox'}))\n","\n","    # Initialize a DatasetDict to store split data\n","    simple_wiki_split_long = DatasetDict()\n","\n","    # Split dataset into train, validation, and test sets\n","    split_dataset_tmp = simple_wiki_filt['train'].train_test_split(val_size, seed=seed)\n","\n","    simple_wiki_split_long = split_dataset_tmp['train'].train_test_split(test_size, seed=seed)\n","    simple_wiki_split_long['validation'] = split_dataset_tmp['test']\n","\n","    # Save the processed dataset to disk\n","    simple_wiki_split_long.save_to_disk(output_file)\n","\n","    # Log the dataset artifact using WandB\n","    now = datetime.now()\n","    time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","    with wandb.init(project='ELI5_analysis',\n","                    entity='ft-llmmm',\n","                    job_type='log_data',\n","                    name=f'wiki_long_articles_{time_stamp}') as run:\n","        data_art = wandb.Artifact(artifact_name, 'dataset')\n","        data_art.add_dir(output_file)\n","        run.log_artifact(data_art)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":192},"executionInfo":{"elapsed":10834,"status":"ok","timestamp":1691298056796,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"LUm6nqKou2o6","outputId":"77bda9eb-f082-4121-ece3-bba1244d3351"},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.15.8"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/LLMs/wikipedia/wandb/run-20230806_050045-41czo6t0</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/41czo6t0' target=\"_blank\">wiki_long_articles_08.06.23-05.00.45</a></strong> to <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/41czo6t0' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/41czo6t0</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/simple_wiki_split_long)... Done. 0.9s\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">wiki_long_articles_08.06.23-05.00.45</strong> at: <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/41czo6t0' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/41czo6t0</a><br/>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230806_050045-41czo6t0/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["filter_simple_wiki_ds()"]},{"cell_type":"markdown","metadata":{"id":"Cv82pHVeNTOa"},"source":["# Make instruction dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOb_ghyJO-BE"},"outputs":[],"source":["# Load existing filtered dataset.\n","simple_wiki_split_long = load_from_disk('./data/simple_wiki_split_long')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DSa4MbMDvk4x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692670242504,"user_tz":240,"elapsed":11862,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"}},"outputId":"8b995828-0d8e-43d4-8fdf-edcf4054159e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI key: ··········\n"]}],"source":["# Enter openai API key to use GPT-3.5.\n","openai.api_key = getpass('Enter your OpenAI key: ')"]},{"cell_type":"code","source":["def make_instruction_qa_all_splits(prompt_template,\n","                                   system_message,\n","                                   model_engine='gpt-3.5-turbo',\n","                                   file_name_prefix='df_simple_wiki_long_qa'):\n","    \"\"\"\n","    Generate instruction-based QA datasets for all splits.\n","\n","    Args:\n","        prompt_template (list): A list of two elements representing the prompt start and end.\n","        system_message (str): The system-level instruction or context message.\n","        model_engine (str, optional): The name or ID of the GPT model to be used. Default is 'gpt-3.5-turbo'.\n","        file_name_prefix (str, optional): Prefix for the output file names. Default is 'df_simple_wiki_long_qa'.\n","    \"\"\"\n","    # Loop through all splits: train, validation, and test\n","    for split in ['train', 'validation', 'test']:\n","\n","        # Get the total length of articles in the split\n","        tot_length = len(simple_wiki_split_long[split])\n","\n","        # Generate instruction-based QA dataset for the current split\n","        make_instruct_dataset(simple_wiki_split_long,\n","                              message=system_message,\n","                              prompt_template=prompt_template,\n","                              split=split,\n","                              text_key='trunc_text',\n","                              length=tot_length,\n","                              file_name_prefix=file_name_prefix)\n"],"metadata":{"id":"dtxWq0zkymvM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0CAAlajTNvz7"},"outputs":[],"source":["# Create splits for instruction dataset using below system message and prompt.\n","\n","message = 'You are a helpful assistant that generates questions from text.'\n","prompt_template = (\"Question: X\\nAnswer:\", \"\\nWhat kind of question, X, could this be an answer to?\\nX:\")\n","\n","make_instruction_qa_all_split(prompt_template,\n","                              message)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sZwZK5MJxqM"},"outputs":[],"source":["def combine_wiki_questions_answers(output_file_prefix='simple_wiki_QA_combined',\n","                                   artifact_name='simple_wiki_QA',\n","                                   qa_file_name_prefix='df_simple_wiki_long_qa'):\n","    \"\"\"\n","    Combine questions and answers datasets and log the resulting dataset as a WandB artifact.\n","\n","    Args:\n","        output_file_prefix (str, optional): Prefix for the output file names. Default is 'simple_wiki_QA_combined'.\n","        artifact_name (str, optional): Name for the saved dataset artifact. Default is 'simple_wiki_QA'.\n","        qa_file_name_prefix (str, optional): Prefix for the question-answer file names. Default is 'df_simple_wiki_long_qa'.\n","    \"\"\"\n","    # Initialize dictionaries to store DataFrames for answers, questions, and combined QA\n","    df_answers = {}\n","    df_questions = {}\n","    df_qa = {}\n","\n","    # Loop through all splits: train, validation, and test\n","    for split in ['train', 'validation', 'test']:\n","        # Load answers DataFrame for the split\n","        df_answers[split] = pd.DataFrame(simple_wiki_split_long[split])\n","\n","        # Convert 'id' column to integer type and set it as index\n","        df_answers[split]['id'] = df_answers[split]['id'].astype(int)\n","        df_answers[split].set_index('id', inplace=True)\n","\n","        # Load questions DataFrame for the split\n","        df_questions[split] = pd.read_csv(f'./data/{qa_file_name_prefix}_{split}.csv',\n","                                          index_col='Unnamed: 0')\n","\n","        # Convert 'id' column to integer type and set it as index\n","        df_questions[split]['id'] = df_questions[split]['id'].astype(int)\n","        df_questions[split].set_index('id', inplace=True)\n","\n","        # Join questions and answers DataFrames on 'id' column\n","        df_qa[split] = df_questions[split].join(df_answers[split][['trunc_text']],\n","                                               how='left', on='id', lsuffix='l', rsuffix='r')\n","\n","        # Save the combined QA DataFrame to a CSV file\n","        df_qa[split].to_csv(f'./data/{output_file_prefix}/{output_file_prefix}_{split}.csv')\n","\n","    # Get current date and time for the artifact name\n","    now = datetime.now()\n","    file_name = f'./data/{output_file_prefix}'\n","    time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","\n","    # Initialize a WandB run for logging\n","    with wandb.init(project='ELI5_analysis',\n","                    entity='ft-llmmm',\n","                    job_type='log_data',\n","                    name=f'simple_wiki_QA_{time_stamp}') as run:\n","        # Create a WandB artifact for the combined dataset\n","        data_art = wandb.Artifact(artifact_name, 'dataset')\n","        data_art.add_dir(file_name)\n","        # Log the artifact to the run\n","        run.log_artifact(data_art)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGmBb0ORX0tP"},"outputs":[],"source":["# Call function defined above to combine question and answer dataframes for simple wikipedia.\n","combine_wiki_questions_answers()"]},{"cell_type":"markdown","metadata":{"id":"SPvAXUofOXgq"},"source":["# Additional Cleaning"]},{"cell_type":"code","source":["def remove_wiki_tables(output_file_prefix='simple_wiki_QA_combined', artifact_name='simple_wiki_QA'):\n","    \"\"\"\n","    Remove rows from QA dataset containing tables based on the presence of 'colspan' in the 'trunc_text' column.\n","\n","    Args:\n","        output_file_prefix (str, optional): Prefix for the output file names. Default is 'simple_wiki_QA_combined'.\n","        artifact_name (str, optional): Name for the saved dataset artifact. Default is 'simple_wiki_QA'.\n","    \"\"\"\n","    # Initialize dictionary to store DataFrames for QA\n","    df_qa = {}\n","\n","    # Load QA DataFrames for all splits: train, validation, and test\n","    for split in ['train', 'validation', 'test']:\n","        df_qa[split] = pd.read_csv(f'./data/{output_file_prefix}/{output_file_prefix}_{split}.csv')\n","\n","    # Iterate through all splits again\n","    for split in ['train', 'validation', 'test']:\n","        # Check for 'colspan' in 'trunc_text' and get the corresponding row indices\n","        colspan_ids = df_qa[split]['trunc_text'].str.contains('colspan')\n","        remove_ids = df_qa[split][colspan_ids].index\n","\n","        # Remove rows with 'colspan' from the DataFrame\n","        df_qa[split].drop(index=remove_ids, inplace=True)\n","\n","        # Save the modified DataFrame back to the file\n","        df_qa[split].to_csv(f'./data/{output_file_prefix}/{output_file_prefix}_{split}.csv')\n","\n","    # Get current date and time for the artifact name\n","    now = datetime.now()\n","    file_name = './data/{output_file_prefix}'\n","    time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","\n","    # Initialize a WandB run for logging\n","    with wandb.init(project='ELI5_analysis',\n","                    entity='ft-llmmm',\n","                    job_type='log_data',\n","                    name=f'simple_wiki_QA_{time_stamp}') as run:\n","        # Create a WandB artifact for the modified dataset\n","        data_art = wandb.Artifact(artifact_name, 'dataset')\n","        data_art.add_dir(file_name)\n","        # Log the artifact to the run\n","        run.log_artifact(data_art)"],"metadata":{"id":"cI0IjqaE1r1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-59GAqhcmzAO","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1692721208410,"user_tz":240,"elapsed":10275,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"}},"outputId":"946d5f0a-8f3a-4422-e62c-dd7df5bad025"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.15.8"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/LLMs/wikipedia/wandb/run-20230822_161958-xoqnt4uy</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/xoqnt4uy' target=\"_blank\">simple_wiki_QA_08.22.23-16.19.58</a></strong> to <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/xoqnt4uy' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/xoqnt4uy</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/simple_wiki_QA_combined)... Done. 0.7s\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">simple_wiki_QA_08.22.23-16.19.58</strong> at: <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/xoqnt4uy' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/xoqnt4uy</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20230822_161958-xoqnt4uy/logs</code>"]},"metadata":{}}],"source":["# Call above function to remove wiki articles containing tables.\n","\n","remove_wiki_tables()"]},{"cell_type":"markdown","source":["# Scratch"],"metadata":{"id":"HJT4Xqj829rO"}},{"cell_type":"markdown","metadata":{"id":"kMuSoYWqM-QM"},"source":["## Compare with English Wiki"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["f7cdf087f5c340e7b499194cec1a1080","f008ad41d3a44c1ebf8175aef5e0dc3e","559b2baa591247028c858b8d90e5b331","cc873046250547ef9e72e23dbe3dc732","7cc0e8d9587f4dacb7887c7c164a270b"]},"executionInfo":{"elapsed":530570,"status":"ok","timestamp":1691349625037,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"mAjoYeAVNAKW","outputId":"f7cedeef-44e1-44d9-e9a0-ff3d19ab5ffa"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7cdf087f5c340e7b499194cec1a1080","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/35.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f008ad41d3a44c1ebf8175aef5e0dc3e","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/30.4k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"559b2baa591247028c858b8d90e5b331","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc873046250547ef9e72e23dbe3dc732","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cc0e8d9587f4dacb7887c7c164a270b","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["english_wiki = load_dataset(\"wikipedia\", \"20220301.en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["c9e2a2fc7e0247b1b4c117fc22be55f9"]},"executionInfo":{"elapsed":561283,"status":"ok","timestamp":1691350187059,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"TXJ_PbeyPmKy","outputId":"34560914-5ba0-4253-e85f-d15879149ab1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c9e2a2fc7e0247b1b4c117fc22be55f9","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/41 shards):   0%|          | 0/6458670 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["english_wiki.save_to_disk('./data/english_wiki')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mb0YNr1PFo6"},"outputs":[],"source":["simple_wiki_split_long = load_from_disk('./data/simple_wiki_split_long')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rAGNm6oPDU9"},"outputs":[],"source":["shared_titles = {}\n","for split in ['train','validation','test']:\n","    shared_titles[split] = set(simple_wiki_split_long[split]['title']).intersection(english_wiki['train']['title'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":681,"status":"ok","timestamp":1691350644093,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"mQ0Ej3SsWovq","outputId":"9382adfc-8aab-4530-81b9-0fec0facf382"},"outputs":[{"data":{"text/plain":["set"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["type(shared_titles['train'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":199,"referenced_widgets":["1e3f542d1ba24a738300ff1c7a5d5e5f","f5200355d0b54b4991561545f8134796","ce8c6a6c26844437ace5979e4af3dcf2","606f4d1d38d945d69b6aebf668e850ec"]},"executionInfo":{"elapsed":336727,"status":"ok","timestamp":1691351247363,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"U6ovq0_4NH2H","outputId":"7fd4e35e-c895-413d-9159-0b5ee35b2bc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["working on split train\n","working on split validation\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e3f542d1ba24a738300ff1c7a5d5e5f","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/6458670 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5200355d0b54b4991561545f8134796","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["working on split test\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce8c6a6c26844437ace5979e4af3dcf2","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/6458670 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"606f4d1d38d945d69b6aebf668e850ec","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/5000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["english_wiki_shared = DatasetDict()\n","simple_wiki_shared = DatasetDict()\n","\n","for split in ['train','validation','test']:\n","    print(f'working on split {split}')\n","\n","    shared = shared_titles[split]\n","\n","    english_wiki_shared[split] = english_wiki['train'].filter(lambda x:x['title'] in shared)\n","\n","    simple_wiki_shared[split] = simple_wiki_split_long[split].filter(lambda x:x['title'] in shared)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u4-40rZlV1qs"},"outputs":[],"source":["import os\n","\n","os.mkdir('./data/simple_en_overlap')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209,"referenced_widgets":["e5a5858e8e1c42599fa1c309121243c3","d7107d96c1c64033bfb084c4c6dc3711","b9171845ee7c4242b639369a3ff2aea0","38218bc8a9d04db2b9a17e7a1b46c5b6","feef680aedef4cb29cdfefd176c46667","01cc4779fcaf47e5b3cfd7406f9cdd75"]},"executionInfo":{"elapsed":126395,"status":"ok","timestamp":1691351373745,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"8ZSDIWEXWGQm","outputId":"e8b4c446-b7c7-4366-f15a-769e6b092e9c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5a5858e8e1c42599fa1c309121243c3","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/57518 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7107d96c1c64033bfb084c4c6dc3711","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/4403 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9171845ee7c4242b639369a3ff2aea0","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/4377 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"38218bc8a9d04db2b9a17e7a1b46c5b6","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/57518 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"feef680aedef4cb29cdfefd176c46667","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/4403 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01cc4779fcaf47e5b3cfd7406f9cdd75","version_major":2,"version_minor":0},"text/plain":["Saving the dataset (0/1 shards):   0%|          | 0/4377 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["english_wiki_shared.save_to_disk('./data/simple_en_overlap/english_shared')\n","simple_wiki_shared.save_to_disk('./data/simple_en_overlap/simple_shared')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"elapsed":31419,"status":"ok","timestamp":1691351405142,"user":{"displayName":"David Meltzer","userId":"14053325825935261635"},"user_tz":240},"id":"JD3X2VdqWWd0","outputId":"be795453-19b3-4d9b-b456-049a14115aee"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmeltzer\u001b[0m (\u001b[33mft-llmmm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.15.8"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/LLMs/wikipedia/wandb/run-20230806_194933-2j4dj56c</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/2j4dj56c' target=\"_blank\">shared_articles_en_simple_08.06.23-19.49.33</a></strong> to <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/2j4dj56c' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/2j4dj56c</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/simple_en_overlap)... Done. 6.4s\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">shared_articles_en_simple_08.06.23-19.49.33</strong> at: <a href='https://wandb.ai/ft-llmmm/ELI5_analysis/runs/2j4dj56c' target=\"_blank\">https://wandb.ai/ft-llmmm/ELI5_analysis/runs/2j4dj56c</a><br/>Synced 5 W&B file(s), 0 media file(s), 20 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20230806_194933-2j4dj56c/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["now = datetime.now()\n","file_name = './data/simple_en_overlap'\n","time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n","with wandb.init(project='ELI5_analysis',\n","                            entity='ft-llmmm',\n","                            job_type='log_data',\n","                            name=f'shared_articles_en_simple_{time_stamp}') as run:\n","                # Initialize a WandB run for logging\n","                data_art = wandb.Artifact('shared_articles_en_simple', 'dataset')\n","                data_art.add_dir(file_name)\n","                run.log_artifact(data_art)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8uPYbwmYBrJ"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["XW-LivoDXXC1","3P-xqCRBZf-0","Ot6Czw-zntay","HJT4Xqj829rO"],"provenance":[],"mount_file_id":"1hJvV1BN2dwnm5tvpUja4puSvXvs-eydj","authorship_tag":"ABX9TyMKv+GZckvXdVOpOtuFo2kF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}