{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k2C6R7ax6t0"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsL_LCARAnuU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/LLMs/Fine-tuning/SFT\n",
        "\n",
        "# installations\n",
        "\n",
        "!pip install bitsandbytes==0.41.1\n",
        "!pip install safetensors>=0.3.1\n",
        "!pip install trl\n",
        "!pip install wandb\n",
        "!pip install tokenizers>=0.13.3\n",
        "!pip install accelerate==0.21.0\n",
        "!pip install datasets\n",
        "!pip install -U torch\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install nltk\n",
        "!pip install bert_score\n",
        "!pip install huggingface_hub\n",
        "!pip install textstat --quiet\n",
        "!pip install openai\n",
        "\n",
        "\n",
        "!pip install git+https://github.com/huggingface/peft.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Oc1K_URzmBu0"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login  # Import login function from huggingface_hub\n",
        "from collections import defaultdict  # Import defaultdict from collections module\n",
        "import transformers  # Import transformers library\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForCausalLM)\n",
        "from tqdm import tqdm  # Import tqdm for progress bars\n",
        "from peft import PeftModel  # Import PeftModel from peft library\n",
        "import pickle  # Import pickle for serialization and deserialization\n",
        "import os  # Import os for file operations\n",
        "import pandas as pd  # Import pandas for data manipulation\n",
        "from transformers import pipeline  # Import pipeline from transformers library\n",
        "import numpy as np # import numpy using standard alias.\n",
        "from textstat import flesch_reading_ease as fre # fre and fkg are shorthands for the Flesch Readability Ease\n",
        "from textstat import flesch_kincaid_grade as fkg # and Flesch-Kincaid readability, respectively.\n",
        "from pprint import pprint # used to display text in a more readable format.\n",
        "import openai # Used to access GPT-4, which will judge the model's responses.\n",
        "import time\n",
        "import torch\n",
        "from getpass import getpass\n",
        "import evaluate\n",
        "import wandb\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eye6_JRiBVAZ"
      },
      "source": [
        "# Computing Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeF1RPfNxfYS"
      },
      "outputs": [],
      "source": [
        "def inference_formatting(example):\n",
        "    \"\"\"\n",
        "    Formats a given example for inference by adding conversation headers.\n",
        "\n",
        "    Args:\n",
        "        example (str): The input example.\n",
        "\n",
        "    Returns:\n",
        "        str: The formatted example with conversation headers.\n",
        "\n",
        "    Example:\n",
        "        >>> inference_formatting(\"How does photosynthesis work?\")\n",
        "        \"### Human: How does photosynthesis work?\\n ### Assistant:\"\n",
        "    \"\"\"\n",
        "    return f\"### Human: {example}\\n ### Assistant:\"\n",
        "\n",
        "def generate_examples(model,\n",
        "                      tokenizer,\n",
        "                      data,\n",
        "                      num_beams=1,\n",
        "                      do_sample=True,\n",
        "                      temperature=0.6,\n",
        "                      top_p=0.9,\n",
        "                      repetition_penalty=1.2,\n",
        "                      padding=True,\n",
        "                      max_new_tokens=512):\n",
        "    \"\"\"\n",
        "    Generates responses using a given model and tokenizer with configurable generation settings.\n",
        "\n",
        "    Args:\n",
        "        model (PreTrainedModel): The pre-trained model for generation.\n",
        "        tokenizer (PreTrainedTokenizerBase): The tokenizer for encoding and decoding text.\n",
        "        data (dict): The input data with prompts.\n",
        "        num_beams (int, optional): Number of beams for beam search. Default is 1.\n",
        "        do_sample (bool, optional): Whether to use sampling for generation. Default is True.\n",
        "        temperature (float, optional): The temperature for sampling. Default is 0.6.\n",
        "        top_p (float, optional): Top p value for nucleus sampling. Default is 0.9.\n",
        "        repetition_penalty (float, optional): Penalty for generating repeating tokens. Default is 1.2.\n",
        "        padding (bool, optional): Whether to apply padding during tokenization. Default is True.\n",
        "        max_new_tokens (int, optional): Maximum number of tokens to generate. Default is 512.\n",
        "\n",
        "    Returns:\n",
        "        list of str: The list of generated responses.\n",
        "\n",
        "    Example:\n",
        "        >>> generate_examples(model, tokenizer, {'prompt': ['How does photosynthesis work?']})\n",
        "        [\"Photosynthesis is the process by which green plants, algae, and some bacteria convert carbon dioxide and water into...\"]\n",
        "    \"\"\"\n",
        "    generation_config = transformers.GenerationConfig(\n",
        "        num_beams=num_beams,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        pad_token_id=model.config.eos_token_id\n",
        "    )\n",
        "\n",
        "    prompts = data['prompt']\n",
        "    input = tokenizer(prompts, return_tensors='pt', padding=padding).to('cuda')\n",
        "    output_ids = model.generate(\n",
        "        input_ids=input['input_ids'],\n",
        "        attention_mask=input['attention_mask'],\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "\n",
        "    predictions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def generate_df_predictions(model_ids,\n",
        "                            ds,\n",
        "                            output_dir,\n",
        "                            batch_size=16,\n",
        "                            seed=50,\n",
        "                            size=100,\n",
        "                            padding=True,\n",
        "                            predictions_dir='./inference_results'):\n",
        "    \"\"\"\n",
        "    Generates and evaluates predictions for a set of models on specified datasets.\n",
        "\n",
        "    Args:\n",
        "        model_ids (list): List of tuples containing base model and model ID.\n",
        "        ds (dict): Dictionary of datasets.\n",
        "        output_dir (str): Directory to save Pandas dataframe containing predictions.\n",
        "        batch_size (int, optional): Batch size for prediction. Default is 16.\n",
        "        seed (int, optional): Seed for random operations. Default is 50.\n",
        "        size (int, optional): Size of the dataset subset for evaluation. Default is 100.\n",
        "        padding (bool, optional): Whether to apply padding during tokenization. Default is True.\n",
        "        predictions_dir (str, optional): Directory to save pickled prediction results. Default is './inference_results'.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Example:\n",
        "        >>> generate_df_predictions(model_ids, datasets, 'output_dir')\n",
        "    \"\"\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(predictions_dir, exist_ok=True)\n",
        "    rouge = evaluate.load('rouge')  # Load ROUGE evaluation tool\n",
        "    bertscore = evaluate.load(\"bertscore\")  # Load BERTScore evaluation tool\n",
        "\n",
        "    # ds_small will contain small versions of the validation subsets.\n",
        "    ds_small = {}\n",
        "    # predictions will contain predictions associated to each element of validation set.\n",
        "    predictions = defaultdict(list)\n",
        "\n",
        "    for base_model, model_id in model_ids:\n",
        "        print(f'working on model {model_id.split(\"/\")[-1]}')\n",
        "        model_name = model_id.split('/')[-1]\n",
        "\n",
        "        # If we already ran inference of model on all datasets, continue.\n",
        "        if all(os.path.exists(f'{predictions_dir}/{model_name}_{ds_name}.pkl')\\\n",
        "               for ds_name in ds):\n",
        "\n",
        "            for ds_name in ds:\n",
        "                file_pkl = f'{predictions_dir}/{model_name}_{ds_name}.pkl'\n",
        "                with open(file_pkl, 'rb') as f:\n",
        "                    predictions[model_name, ds_name] = pickle.load(f)\n",
        "            continue\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        # Set pad_token to eos token if it does not exist.\n",
        "        if not tokenizer.pad_token:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        # Set padding side to left since we are doing inference.\n",
        "        # Setting padding to right will hurt model's performance.\n",
        "        tokenizer.padding_side = \"left\"\n",
        "\n",
        "        # if base model is given we download it and then attach adapter layers.\n",
        "        if base_model:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.bfloat16\n",
        "            )\n",
        "\n",
        "            model = PeftModel.from_pretrained(model=model,\n",
        "                                             model_id=model_id,\n",
        "                                             torch_dtype=torch.bfloat16,\n",
        "                                             is_trainable=False)\n",
        "        # If no base model given, download model directly.\n",
        "        else:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.bfloat16,\n",
        "            )\n",
        "\n",
        "        # Ensure model is in inference mode.\n",
        "        model.eval()\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        # If model can be moved, move it to device.\n",
        "        # Need try/except block since bitsandbytes models cannot be moved,\n",
        "        try:\n",
        "            model.to(device)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "        for ds_name in ds:\n",
        "            # For each dataset we turn the question into a prompt, shuffle the dataset, and only keep \"size\" rows.\n",
        "            ds_small = ds[ds_name]['validation'].map(lambda x: {'prompt': inference_formatting(x['question'])})\n",
        "            ds_small = ds_small.shuffle(seed=seed)\n",
        "            ds_small = ds_small.select(range(size))\n",
        "\n",
        "            print(f'working on dataset {ds_name}')\n",
        "\n",
        "            # file_pkl will contain results of running inference using \"model_name\" on the given dataset.\n",
        "            # Useful to save intermediate results to avoid unnecessary repeated computations.\n",
        "\n",
        "            # If file exists, load results into predictions dictionary.\n",
        "            file_pkl = f'{predictions_dir}/{model_name}_{ds_name}.pkl'\n",
        "            if os.path.exists(file_pkl):\n",
        "                with open(file_pkl, 'rb') as f:\n",
        "                    predictions[model_name, ds_name] = pickle.load(f)\n",
        "            # Else, run inference over the dataset and save results.\n",
        "            # Use batched inference to speed up computations.\n",
        "            else:\n",
        "                for k in tqdm(range(0, len(ds_small), batch_size)):\n",
        "                    prediction = generate_examples(model, tokenizer, ds_small[k:k + batch_size], padding=padding)\n",
        "                    predictions[model_name, ds_name].extend(prediction)\n",
        "\n",
        "                    with open(f'{predictions_dir}/{model_name}_{ds_name}.pkl', 'wb') as f:\n",
        "                        pickle.dump(predictions[model_name, ds_name], f)\n",
        "\n",
        "            rouge_scores = {}\n",
        "            bert_scores = {}\n",
        "\n",
        "        del model\n",
        "\n",
        "    # For each model and dataset, we compute the ROUGE and BERTScore.\n",
        "    for model_name, ds_name in predictions:\n",
        "        print(f'computing predictions for {(model_name, ds_name)}')\n",
        "\n",
        "        # load predictions\n",
        "        preds = predictions[(model_name, ds_name)]\n",
        "\n",
        "        # Output for predictions, ROUGE scores and BERTScores.\n",
        "        preds_file = output_dir + f'/{model_name}_{ds_name}_predictions.csv'\n",
        "        rouge_file = output_dir + f'/{model_name}_{ds_name}_rouge.csv'\n",
        "        bertscore_file = output_dir + f'/{model_name}_{ds_name}_bertscore.csv'\n",
        "\n",
        "        # If file does not already exist, compute the ROUGE scores.\n",
        "        if not os.path.exists(rouge_file):\n",
        "\n",
        "            rouge_scores[(model_name, ds_name)] = rouge.compute(\n",
        "                predictions=preds,\n",
        "                references=ds_small['QA']\n",
        "            )\n",
        "            df_rouge = pd.DataFrame(rouge_scores[(model_name, ds_name)],\n",
        "                                    index=[0])\n",
        "            df_rouge.to_csv(rouge_file)\n",
        "\n",
        "        # Same as above but for BERTScore\n",
        "        if not os.path.exists(bertscore_file):\n",
        "\n",
        "            bert_scores[(model_name, ds_name)] = bertscore.compute(\n",
        "                predictions=preds,\n",
        "                references=ds_small['QA'],\n",
        "                lang='en')\n",
        "\n",
        "            df_bert = pd.DataFrame(bert_scores[(model_name, ds_name)],\n",
        "                                   )\n",
        "            df_bert.to_csv(bertscore_file)\n",
        "\n",
        "        # Convert predictions to Pandas Dataframe and save.\n",
        "        df_preds = pd.DataFrame(preds)\n",
        "        df_preds.to_csv(preds_file, index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjhM0rZJsLsJ"
      },
      "outputs": [],
      "source": [
        "# Load artifacts containing datasets.\n",
        "\n",
        "with wandb.init(project='SFT_training_DM',\n",
        "                entity='ft-llmmm',\n",
        "                job_type='download_data',\n",
        "                name=f'download_combined_data') as run:\n",
        "\n",
        "    artifact = run.use_artifact('ft-llmmm/ELI5_analysis/llama_QA_tokenized_1024:v1', type='dataset')\n",
        "    artifact_dir = artifact.download()\n",
        "\n",
        "# We have three datasets, the combined SFT dataset, and two datasets containing just questions from the\n",
        "# ELI5 and Simple Wikipedia datasets.\n",
        "ds = {}\n",
        "ds['full'] = datasets.load_from_disk(artifact_dir)\n",
        "ds['wiki'] = ds['full'].filter(lambda x: x['source']=='simple_wiki')\n",
        "ds['eli5'] = ds['full'].filter(lambda x: x['source']!='simple_wiki')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-iWExc1GKrg"
      },
      "outputs": [],
      "source": [
        "# Here we run inference over the original Llama-2-7B model as well as our 5 fine-tuned models.\n",
        "# Note: models with \"eli5-cleaned\" in them were trained on the ELI5 dataset where we removed posts that contained \"edit:\" in them.\n",
        "# Models with just \"eli5\" were trained on the dataset with those posts left in.\n",
        "# Models trained on the uncleaned dataset are more likely to end their answers with \"edit: caught typos\", which we want to avoid.\n",
        "\n",
        "model_ids = []\n",
        "model_ids.append((None,'meta-llama/Llama-2-7b-hf'))\n",
        "\n",
        "model_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged'))\n",
        "model_ids.append((None,'dhmeltzer/llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged'))\n",
        "model_ids.append((None,'dhmeltzer/llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged'))\n",
        "\n",
        "model_ids.append((None,'dhmeltzer/Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged'))\n",
        "model_ids.append((None,'dhmeltzer/Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged'))\n",
        "\n",
        "# Run inference for all models.\n",
        "generate_df_predictions(model_ids,\n",
        "                        ds,\n",
        "                        './llama-2-inference-512',\n",
        "                        batch_size=8,\n",
        "                        padding=True,\n",
        "                        predictions_dir = './val_results_512')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLO4Xh4shHHV"
      },
      "outputs": [],
      "source": [
        "# Same as above, but now for the 13B models.\n",
        "\n",
        "model_ids = []\n",
        "model_ids.append((None,'meta-llama/Llama-2-13b-hf'))\n",
        "\n",
        "model_ids.append((None,'dhmeltzer/Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged'))\n",
        "model_ids.append((None,'dhmeltzer/Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged'))\n",
        "model_ids.append((None,'dhmeltzer/Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged'))\n",
        "\n",
        "model_ids.append((None,'dhmeltzer/Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged'))\n",
        "model_ids.append((None,'dhmeltzer/Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged'))\n",
        "\n",
        "generate_df_predictions(model_ids,\n",
        "                        ds,\n",
        "                        './llama-2-inference-512',\n",
        "                        batch_size=4,\n",
        "                        padding=True,\n",
        "                        predictions_dir = './val_results_512')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RTD1vqG3cW-"
      },
      "source": [
        "# Analyzing Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONuyni9CuTTm"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_fdZg5SuCDg"
      },
      "source": [
        "The functions defined below are used to clean up the Pandas dataframes so they are easier to read and understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "nmgUV22zuRvJ"
      },
      "outputs": [],
      "source": [
        "def fix_df_predictions(model_names,\n",
        "                       pred_directory='llama-2-inference-512'):\n",
        "    \"\"\"\n",
        "    Concatenates and formats prediction data from multiple models and datasets.\n",
        "\n",
        "    Args:\n",
        "        model_names (list): List of model names.\n",
        "        pred_directory (str, optional): Directory containing prediction files. Default is 'llama-2-inference-512'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Concatenated and formatted prediction data.\n",
        "\n",
        "    Example:\n",
        "        >>> fix_df_predictions(['model1', 'model2'])\n",
        "    \"\"\"\n",
        "    df_predictions = pd.DataFrame()  # Initialize an empty DataFrame to store the formatted predictions.\n",
        "\n",
        "    for model_name in model_names:\n",
        "        for ds_name in ['full', 'wiki', 'eli5']:\n",
        "\n",
        "            predictions_file = f'./{pred_directory}/{model_name}_{ds_name}_predictions.csv'  # Define the path to the prediction file.\n",
        "\n",
        "             # temporary dataframe containing prediction data for given model and dataset.\n",
        "            temp = pd.read_csv(predictions_file, index_col='Unnamed: 0')\n",
        "\n",
        "            # Transpose the DataFrame for proper formatting.\n",
        "            temp = temp.T\n",
        "            # Add a column for the model name.\n",
        "            temp['model_name'] = model_name\n",
        "            # Add a column for the dataset name.\n",
        "            temp['dataset'] = ds_name\n",
        "\n",
        "            # Concatenate the formatted data with the existing DataFrame.\n",
        "            df_predictions = pd.concat([df_predictions, temp])\n",
        "\n",
        "    # Set the model name and dataset as the multi-index.\n",
        "    df_predictions = df_predictions.set_index(['model_name', 'dataset'])\n",
        "\n",
        "    return df_predictions\n",
        "\n",
        "def fix_df_metric(model_names, metric, pred_directory='llama-2-inference-512'):\n",
        "    \"\"\"\n",
        "    Concatenates and formats metric data from multiple models and datasets.\n",
        "\n",
        "    Args:\n",
        "        model_names (list): List of model names.\n",
        "        metric (str): Name of the metric (either 'bertscore' or 'rouge').\n",
        "        pred_directory (str, optional): Directory containing metric files. Default is 'llama-2-inference-512'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Concatenated and formatted metric data.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If metric is not 'bertscore' or 'rouge'.\n",
        "\n",
        "    Example:\n",
        "        >>> fix_df_metric(['model1', 'model2'], 'bertscore')\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame()  # Initialize an empty DataFrame to store the formatted metric data.\n",
        "\n",
        "    if metric not in ['bertscore', 'rouge']:\n",
        "        raise ValueError('metric must be either bertscore or rouge')  # Raise an error if the metric is not valid.\n",
        "\n",
        "    for model_name in model_names:\n",
        "        for ds_name in ['full', 'wiki', 'eli5']:\n",
        "            # Define the path to the metric file for each model and dataset.\n",
        "            file_name = f'./{pred_directory}/{model_name}_{ds_name}_{metric}.csv'\n",
        "\n",
        "            # Load the metric data into a temporary dataframe.\n",
        "            temp = pd.read_csv(file_name, index_col='Unnamed: 0')\n",
        "\n",
        "            # Calculate the mean of precision, recall, and f1 score for BERTScore.\n",
        "            if metric == 'bertscore':\n",
        "                temp = pd.DataFrame(temp[['precision', 'recall', 'f1']].mean()).T\n",
        "\n",
        "            temp['model_name'] = model_name  # Add a column for the model name.\n",
        "            temp['dataset'] = ds_name  # Add a column for the dataset name.\n",
        "\n",
        "            df = pd.concat([df, temp])  # Concatenate the formatted data with the main DataFrame.\n",
        "\n",
        "    df = df.set_index(['model_name', 'dataset'])  # Set the model name and dataset as the index.\n",
        "\n",
        "    return df  # Return the formatted DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhCD_6FRspgb"
      },
      "source": [
        "## Automatic Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfNePG3tsy4n"
      },
      "source": [
        "In this section we will use automatic metrics to measure our models. Specifically, we will use ROUGE and BERTScore to evaluate how similar the model's answers are to the original human's answer. We will also use the Flesch readability metrics to see which answers are \"simpler\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "L_8nK6Krg83D"
      },
      "outputs": [],
      "source": [
        "model_names_7B = []\n",
        "model_names_7B.append('Llama-2-7b-hf')\n",
        "\n",
        "model_names_7B.append('llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged')\n",
        "model_names_7B.append('llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged')\n",
        "model_names_7B.append('llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged')\n",
        "\n",
        "model_names_7B.append('Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged')\n",
        "model_names_7B.append('Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged')\n",
        "\n",
        "df_predictions_7B = fix_df_predictions(model_names_7B)\n",
        "df_bertscore_7B = fix_df_metric(model_names_7B,'bertscore')\n",
        "df_bertscore_7B = fix_df_metric(model_names_7B,'rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dt34A2WM7Hv2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "jx-27DWQDgYB",
        "outputId": "bca21c10-c7f5-438c-de52-9c8308ac96f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2b274c37-2e97-40d6-bf93-7ca6dfe5554f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>fkg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th>model_name</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">eli5</th>\n",
              "      <th>Llama-2-7b-hf</th>\n",
              "      <td>10.799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>11.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>9.635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>8.509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>13.777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>9.237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">full</th>\n",
              "      <th>Llama-2-7b-hf</th>\n",
              "      <td>11.636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>11.270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>10.054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>7.855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>10.681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>10.314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">wiki</th>\n",
              "      <th>Llama-2-7b-hf</th>\n",
              "      <td>11.013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>10.037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>10.148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>7.606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>10.695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>9.982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b274c37-2e97-40d6-bf93-7ca6dfe5554f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2b274c37-2e97-40d6-bf93-7ca6dfe5554f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2b274c37-2e97-40d6-bf93-7ca6dfe5554f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9e95879b-b1dd-47cb-b686-199169c56364\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9e95879b-b1dd-47cb-b686-199169c56364')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9e95879b-b1dd-47cb-b686-199169c56364 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                               fkg\n",
              "dataset model_name                                                \n",
              "eli5    Llama-2-7b-hf                                       10.799\n",
              "        Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged        11.015\n",
              "        Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_m...   9.635\n",
              "        llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged       8.509\n",
              "        llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged   13.777\n",
              "        llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_me...   9.237\n",
              "full    Llama-2-7b-hf                                       11.636\n",
              "        Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged        11.270\n",
              "        Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_m...  10.054\n",
              "        llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged       7.855\n",
              "        llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged   10.681\n",
              "        llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_me...  10.314\n",
              "wiki    Llama-2-7b-hf                                       11.013\n",
              "        Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged        10.037\n",
              "        Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_m...  10.148\n",
              "        llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged       7.606\n",
              "        llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged   10.695\n",
              "        llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_me...   9.982"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Below we see that the trained models generally have a lower Flesch-Kincaid grade level than the original model.\n",
        "# The two exceptions are the models trained on the ELI5-cleaned and wikipedia dataset evaluated on the ELI5 dataset.\n",
        "df_predictions_7B.applymap(fkg).mean(axis=1).to_frame('fkg').swaplevel('model_name','dataset').sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "lXKAtirzC9ZO",
        "outputId": "822d1908-d6bb-4d7c-c122-3130c3a4eda1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-69c27d75-140b-4661-8980-2f50c54c0a48\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>fre</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th>model_name</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">eli5</th>\n",
              "      <th>Llama-2-7b-hf</th>\n",
              "      <td>52.6770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>55.2385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>61.0496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>65.0010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>44.7725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>61.9205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">full</th>\n",
              "      <th>Llama-2-7b-hf</th>\n",
              "      <td>48.9020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>54.0539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>57.9690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>68.6513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>55.5700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>56.8602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">wiki</th>\n",
              "      <th>Llama-2-7b-hf</th>\n",
              "      <td>51.4365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>60.3884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>58.3621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>69.1257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>55.8206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged</th>\n",
              "      <td>57.5070</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69c27d75-140b-4661-8980-2f50c54c0a48')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69c27d75-140b-4661-8980-2f50c54c0a48 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69c27d75-140b-4661-8980-2f50c54c0a48');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9116439a-41b3-40c7-83b3-a73ddadddabc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9116439a-41b3-40c7-83b3-a73ddadddabc')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9116439a-41b3-40c7-83b3-a73ddadddabc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                fre\n",
              "dataset model_name                                                 \n",
              "eli5    Llama-2-7b-hf                                       52.6770\n",
              "        Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged        55.2385\n",
              "        Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_m...  61.0496\n",
              "        llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged      65.0010\n",
              "        llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged   44.7725\n",
              "        llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_me...  61.9205\n",
              "full    Llama-2-7b-hf                                       48.9020\n",
              "        Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged        54.0539\n",
              "        Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_m...  57.9690\n",
              "        llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged      68.6513\n",
              "        llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged   55.5700\n",
              "        llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_me...  56.8602\n",
              "wiki    Llama-2-7b-hf                                       51.4365\n",
              "        Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged        60.3884\n",
              "        Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_m...  58.3621\n",
              "        llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged      69.1257\n",
              "        llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged   55.8206\n",
              "        llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_me...  57.5070"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Below we see that the trained models a higher Flesch readability level than the original model.\n",
        "# The one exception is that the model trained on just wikipedia has a lower score on the ELI5 dataset.\n",
        "\n",
        "df_predictions_7B.applymap(fre).mean(axis=1).to_frame('fre').swaplevel('model_name','dataset').sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "UudlF8BE3SwD"
      },
      "outputs": [],
      "source": [
        "model_names_13B = ['Llama-2-13b-hf',\n",
        " 'Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged',\n",
        " 'Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged',\n",
        " 'Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged',\n",
        " 'Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged',\n",
        " 'Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged']\n",
        "\n",
        "df_predictions_13B = fix_df_predictions(model_names_13B)\n",
        "df_bertscore_13B = fix_df_metric(model_names_13B,'bertscore')\n",
        "df_bertscore_13B = fix_df_metric(model_names_13B,'rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "7jj1Xq2g76_G",
        "outputId": "2e112979-7006-4e08-86b6-71eba6a6f38c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-6fdb5685-f7a5-41b9-9196-51148bfb79b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>fkg</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th>model_name</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">eli5</th>\n",
              "      <th>Llama-2-13b-hf</th>\n",
              "      <td>9.932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>10.142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</th>\n",
              "      <td>13.763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>8.907</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>9.298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</th>\n",
              "      <td>8.433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">full</th>\n",
              "      <th>Llama-2-13b-hf</th>\n",
              "      <td>10.028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>8.895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</th>\n",
              "      <td>11.296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>9.339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>10.218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</th>\n",
              "      <td>9.501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">wiki</th>\n",
              "      <th>Llama-2-13b-hf</th>\n",
              "      <td>9.999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>7.994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</th>\n",
              "      <td>11.595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>7.885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>9.913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</th>\n",
              "      <td>9.324</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fdb5685-f7a5-41b9-9196-51148bfb79b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6fdb5685-f7a5-41b9-9196-51148bfb79b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6fdb5685-f7a5-41b9-9196-51148bfb79b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-aa16778b-d19e-453e-bbb3-a6e21408f83e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aa16778b-d19e-453e-bbb3-a6e21408f83e')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-aa16778b-d19e-453e-bbb3-a6e21408f83e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                               fkg\n",
              "dataset model_name                                                \n",
              "eli5    Llama-2-13b-hf                                       9.932\n",
              "        Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged    10.142\n",
              "        Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_...  13.763\n",
              "        Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged        8.907\n",
              "        Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_...   9.298\n",
              "        Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged   8.433\n",
              "full    Llama-2-13b-hf                                      10.028\n",
              "        Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged     8.895\n",
              "        Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_...  11.296\n",
              "        Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged        9.339\n",
              "        Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_...  10.218\n",
              "        Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged   9.501\n",
              "wiki    Llama-2-13b-hf                                       9.999\n",
              "        Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged     7.994\n",
              "        Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_...  11.595\n",
              "        Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged        7.885\n",
              "        Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_...   9.913\n",
              "        Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged   9.324"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# For the 13B mdoels we see more examples where the fine-tuned models have a higher Flesch-Kincaid grade level.\n",
        "# The exceptions are:\n",
        "#   1) The models trained on just ELI5 or just simple wikipedia evaluated on the ELI5 validation set.\n",
        "#   2) The models trained on just simple wikipedia or ELI5-cleaned and evalauted on the full dataset.\n",
        "#   3) The model trained on just simple wikipedia and evaluated on the simple wikipedia validation set.\n",
        "\n",
        "df_predictions_13B.applymap(fkg).mean(axis=1).to_frame('fkg').swaplevel('model_name','dataset').sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "mz8R-Wn73ymn",
        "outputId": "8dbea890-2efb-4913-adf8-05549c5d3b82"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f49092c5-d619-46c9-aedb-3ffb73cdb9cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>fre</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th>model_name</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">eli5</th>\n",
              "      <th>Llama-2-13b-hf</th>\n",
              "      <td>55.9093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>55.9616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</th>\n",
              "      <td>43.5825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>62.4666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>61.6679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</th>\n",
              "      <td>64.5636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">full</th>\n",
              "      <th>Llama-2-13b-hf</th>\n",
              "      <td>55.5964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>63.1428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</th>\n",
              "      <td>52.3647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>61.2361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>56.9492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</th>\n",
              "      <td>59.5139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"6\" valign=\"top\">wiki</th>\n",
              "      <th>Llama-2-13b-hf</th>\n",
              "      <td>55.7939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged</th>\n",
              "      <td>67.1027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged</th>\n",
              "      <td>51.8992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged</th>\n",
              "      <td>67.8129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged</th>\n",
              "      <td>57.7417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged</th>\n",
              "      <td>60.8368</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f49092c5-d619-46c9-aedb-3ffb73cdb9cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f49092c5-d619-46c9-aedb-3ffb73cdb9cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f49092c5-d619-46c9-aedb-3ffb73cdb9cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d9ec4fd2-aa3f-4d73-840d-2b4ee30a62d1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d9ec4fd2-aa3f-4d73-840d-2b4ee30a62d1')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d9ec4fd2-aa3f-4d73-840d-2b4ee30a62d1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                fre\n",
              "dataset model_name                                                 \n",
              "eli5    Llama-2-13b-hf                                      55.9093\n",
              "        Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged    55.9616\n",
              "        Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_...  43.5825\n",
              "        Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged       62.4666\n",
              "        Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_...  61.6679\n",
              "        Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged  64.5636\n",
              "full    Llama-2-13b-hf                                      55.5964\n",
              "        Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged    63.1428\n",
              "        Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_...  52.3647\n",
              "        Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged       61.2361\n",
              "        Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_...  56.9492\n",
              "        Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged  59.5139\n",
              "wiki    Llama-2-13b-hf                                      55.7939\n",
              "        Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged    67.1027\n",
              "        Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_...  51.8992\n",
              "        Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged       67.8129\n",
              "        Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_...  57.7417\n",
              "        Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged  60.8368"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Here we see that most 13B models have a higher Flesch readability scores.\n",
        "# The exception is the model trained on just the Simple Wikipedia dataset and evaluated on all three datasets.\n",
        "\n",
        "df_predictions_13B.applymap(fre).mean(axis=1).to_frame('fre').swaplevel('model_name','dataset').sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IVRxlHGHnD8"
      },
      "source": [
        "## GPT-4 As Judge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxQRSxgqxG1L"
      },
      "source": [
        "In this section we will use GPT-4 as a judge to score each model's answer. Inspired by [MT-bench](https://arxiv.org/abs/2306.05685) we use the following prompt:\n",
        "\n",
        "\n",
        "\n",
        "> [Instruction]\\nPlease act as an impartial judge and\n",
        "    evaluate the quality of the response provided by an AI assistant\n",
        "    to the user question displayed below. Your evaluation should consider\n",
        "    factors such as the following:\\n\\n- **Simplicity**: Is the response\n",
        "    clear and straightforward enough for middle or high school students to\n",
        "    understand?\\n- **Helpfulness**: Does the response effectively address\n",
        "    the query?\\n- **Relevance**: Does the response directly pertain to the\n",
        "    question?\\n- **Accuracy**: Is the response factually correct?\\n-\n",
        "    **Depth & Creativity**: Assess richness without favoring excessive\n",
        "    detail.\\n\\nAim for a holistic assessment. Begin your evaluation by\n",
        "    providing a short explanation. Be as objective as possible. After\n",
        "    providing your explanation, you must rate the response on a scale\n",
        "    of 1 to 10 by strictly following this format: \\'[[rating]]\\',\n",
        "    for example: \\'Rating: [[5]]\\'.\\n\\n[Question]\\n{question}\\n\\n\n",
        "    [The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's\n",
        "    Answer]\n",
        "\n",
        "This prompt was chosen to emphasize both helpfulness, i.e. that the model actually answers the question accurately, while also being simple enough for students to understand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I2M1aroL3Ft",
        "outputId": "a249f63d-d3f0-4160-a8b9-4a889b09b1f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ],
      "source": [
        "# To access GPT-4, we need an OpenAI access token.\n",
        "os.environ['OPENAI_API_KEY'] = getpass('Enter OpenAI access token: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "kkSw3uG-HpHP"
      },
      "outputs": [],
      "source": [
        "def find_rating(string):\n",
        "    \"\"\"\n",
        "    Find the rating value from a string.\n",
        "\n",
        "    Args:\n",
        "        string (str): Input string containing the rating.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted rating.\n",
        "\n",
        "    Example:\n",
        "        >>> find_rating(\"Rating: [[5]]\")\n",
        "        '5'\n",
        "    \"\"\"\n",
        "    idx = string.index('[[')\n",
        "    return string[idx+2]\n",
        "\n",
        "def get_GPT_judgement(QA_pair,\n",
        "                      model='gpt-4',\n",
        "                      temperature=0,\n",
        "                      max_tokens=2048,\n",
        "                      system_message='You are a helpful assistant.',\n",
        "                      API_MAX_RETRY=16,\n",
        "                      API_RETRY_SLEEP=10,\n",
        "                      API_ERROR_OUTPUT=\"$ERROR$\"):\n",
        "    \"\"\"\n",
        "    Get a GPT-based judgment on a given QA pair.\n",
        "\n",
        "    Args:\n",
        "        QA_pair (str): Input QA pair formatted as '### Human: ... ### Assistant: ...'.\n",
        "        model (str, optional): OpenAI GPT model name. Defaults to 'gpt-4'.\n",
        "        temperature (int, optional): Temperature parameter for generating responses. Defaults to 0.\n",
        "        max_tokens (int, optional): Maximum number of tokens in the generated response. Defaults to 2048.\n",
        "        system_message (str, optional): System message for the conversation. Defaults to 'You are a helpful assistant.'.\n",
        "        API_MAX_RETRY (int, optional): Maximum number of API retries in case of error. Defaults to 16.\n",
        "        API_RETRY_SLEEP (int, optional): Sleep duration (in seconds) between API retries. Defaults to 10.\n",
        "        API_ERROR_OUTPUT (str, optional): Output string in case of API error. Defaults to \"$ERROR$\".\n",
        "\n",
        "    Returns:\n",
        "        str: GPT-generated judgment on the given QA pair.\n",
        "\n",
        "    Example:\n",
        "        >>> get_GPT_judgement(\"### Human: What is the capital of France? ### Assistant: Paris.\",\n",
        "        ...                   model='gpt-4', temperature=0.6)\n",
        "        'A good answer, clear and helpful. [Rating: [[8]]]'\n",
        "    \"\"\"\n",
        "    QA_pair = QA_pair.split('### Human:')[1].split('### Assistant:')\n",
        "    QA_pair = list(map(lambda x:x.strip(),QA_pair))\n",
        "\n",
        "    question = QA_pair[0]\n",
        "    answer = QA_pair[1]\n",
        "\n",
        "    prompt = f\"\"\"[Instruction]\\nPlease act as an impartial judge and\n",
        "    evaluate the quality of the response provided by an AI assistant\n",
        "    to the user question displayed below. Your evaluation should consider\n",
        "    factors such as the following:\\n\\n- **Simplicity**: Is the response\n",
        "    clear and straightforward enough for middle or high school students to\n",
        "    understand?\\n- **Helpfulness**: Does the response effectively address\n",
        "    the query?\\n- **Relevance**: Does the response directly pertain to the\n",
        "    question?\\n- **Accuracy**: Is the response factually correct?\\n-\n",
        "    **Depth & Creativity**: Assess richness without favoring excessive\n",
        "    detail.\\n\\nAim for a holistic assessment. Begin your evaluation by\n",
        "    providing a short explanation. Be as objective as possible. After\n",
        "    providing your explanation, you must rate the response on a scale\n",
        "    of 1 to 10 by strictly following this format: \\'[[rating]]\\',\n",
        "    for example: \\'Rating: [[5]]\\'.\\n\\n[Question]\\n{question}\\n\\n\n",
        "    [The Start of Assistant's Answer]\\n{answer}\\n[The End of Assistant's\n",
        "    Answer]\"\"\"\n",
        "\n",
        "    output = API_ERROR_OUTPUT\n",
        "\n",
        "    # code block below queries GPT model to complete the prompt a fixed number of times, API_MAX_RETRY.\n",
        "    # If not successful, the code returns API_ERROR_OUTPUT.\n",
        "    for _ in range(API_MAX_RETRY):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": system_message},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                n=1,\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "            )\n",
        "            output = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "            break\n",
        "        except openai.error.OpenAIError as e:\n",
        "            print(type(e), e)\n",
        "            time.sleep(API_RETRY_SLEEP)\n",
        "\n",
        "    return output\n",
        "\n",
        "def GPT_judgement_on_df(df_predictions, output_file, model_engine='gpt-4'):\n",
        "    \"\"\"\n",
        "    Generate GPT-based judgments on a DataFrame of predictions.\n",
        "\n",
        "    Args:\n",
        "        df_predictions (pd.DataFrame): DataFrame containing predictions.\n",
        "        output_file (str): Path to the output file for storing judgments.\n",
        "        model_engine (str, optional): OpenAI GPT model name. Defaults to 'gpt-4'.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Example:\n",
        "        >>> GPT_judgement_on_df(df_predictions, 'output_judgements.csv', model_engine='gpt-4')\n",
        "    \"\"\"\n",
        "    # Check if output file exists\n",
        "    if os.path.exists(output_file):\n",
        "        df_GPT4_judgements = pd.read_csv(output_file, index_col='Unnamed: 0')\n",
        "    else:\n",
        "        df_GPT4_judgements = defaultdict(list)\n",
        "\n",
        "    # Get unique model names from the predictions\n",
        "    model_names = sorted(list(set([idx[0] for idx in df_predictions.index])))\n",
        "\n",
        "    # Iterate over model names\n",
        "    for model_name in model_names:\n",
        "        print(f'working on model {model_name}')\n",
        "        # Iterate over samples\n",
        "        for j in tqdm(range(df_predictions.shape[1])):\n",
        "            QA_pair = df_predictions.loc[(model_name, 'full')][j]\n",
        "            # Check if judgment already exists for this sample\n",
        "            if j not in df_GPT4_judgements[model_name]:\n",
        "                # Get GPT-based judgment\n",
        "                df_GPT4_judgements[model_name][j] = get_GPT_judgement(QA_pair, model=model_engine)\n",
        "\n",
        "    # Save judgments to output file\n",
        "    df_GPT4_judgements.to_csv(output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiaYgHA45ET-",
        "outputId": "9b3cc840-cdd0-446e-9453-49ce2884c6b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-7b-hf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 4361.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 4945.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 5081.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 4030.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 4085.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 4382.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-13b-hf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 4526.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 3881.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 2495.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 1918.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 2182.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "working on model Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 1923.41it/s]\n"
          ]
        }
      ],
      "source": [
        "GPT_judgement_on_df(df_predictions_7B,\n",
        "                        './llama-2-inference-512/GPT4_7B_judgements.csv')\n",
        "GPT_judgement_on_df(df_predictions_13B,\n",
        "                        './llama-2-inference-512/GPT4_13B_judgements.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "lULZjju5zQQO"
      },
      "outputs": [],
      "source": [
        "df_GPT_4_judgements_7B = pd.read_csv('./llama-2-inference-512/GPT4_7B_judgements.csv',\n",
        "                                  index_col='Unnamed: 0')\n",
        "df_GPT_4_judgements_13B = pd.read_csv('./llama-2-inference-512/GPT4_13B_judgements.csv',\n",
        "                                  index_col='Unnamed: 0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eRQnAF3mu0s",
        "outputId": "dda3b7ce-ce0e-40fb-c061-715d66e2dae9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Llama-2-7b-hf-eli5-cleaned-wiki65k-1024_qlora_merged', 3.89),\n",
              " ('llama-7b-SFT_ds_wiki65k_1024_r_64_alpha_16_merged', 3.58),\n",
              " ('llama-7b-SFT_eli5_wiki65k_1024_r_64_alpha_16_merged', 3.45),\n",
              " ('Llama-2-7b-hf', 2.92),\n",
              " ('Llama-2-7b-hf-eli5-cleaned-1024_qlora_merged', 2.58),\n",
              " ('llama-7b-SFT_ds_eli5_1024_r_64_alpha_16_merged', 2.43)]"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# At 7B we see the best performing model s the one trained on the combined ELI5-clenaed + simple wikipedia dataset.\n",
        "\n",
        "scores_GPT4_7B = df_GPT_4_judgements_7B.T.applymap(find_rating).applymap(int).mean(axis=1)\n",
        "sorted(list(scores_GPT4_7B.items()),key = lambda x:x[1],reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRFGIqjgnrTC",
        "outputId": "06de660a-74fb-4522-9f52-71f610c89562"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Llama-2-13b-hf-eli5-wiki-1024_r_64_alpha_16_merged', 4.91),\n",
              " ('Llama-2-13b-hf-eli5-cleaned-wiki65k-1024_qlora_merged', 4.77),\n",
              " ('Llama-2-13b-hf-ds_wiki_1024_full_r_64_alpha_16_merged', 4.53),\n",
              " ('Llama-2-13b-hf', 3.43),\n",
              " ('Llama-2-13b-hf-eli5-cleaned-1024_qlora_merged', 3.27),\n",
              " ('Llama-2-13b-hf-ds_eli5_1024_r_64_alpha_16_merged', 3.17)]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# At 13B we see the best performing model s the one trained on the original ELI5 + simple wikipedia dataset.\n",
        "# The model trained on ELI-5 Cleaned + Simple Wikipedia is not far behind though.\n",
        "\n",
        "scores_GPT4_13B = df_GPT_4_judgements_13B.T.applymap(find_rating).applymap(int).mean(axis=1)\n",
        "sorted(list(scores_GPT4_13B.items()),key = lambda x:x[1],reverse=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
