{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1gzbxeUeFa7tMCftY9gaqYmb76hSHev9s",
      "authorship_tag": "ABX9TyPklauyzX1+z7fGJnKuzCOm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-meltzer/LLMs/blob/main/data_analysis/ELI5_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "bdp_5O98KF7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/LLMs/ELI5_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HcMFFaHocM9",
        "outputId": "17ab8d78-faf4-4784-a1cc-9f0ed4a9af17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/LLMs/ELI5_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX-WlvYb3Y0c"
      },
      "outputs": [],
      "source": [
        "%cd drive/MyDrive/LLMs/ELI5_dataset\n",
        "\n",
        "!pip install datasets --quiet\n",
        "!pip install textstat --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install redditcleaner --quiet\n",
        "!pip install huggingface_hub --quiet\n",
        "!pip install -U sentence-transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb, torch\n",
        "import sys\n",
        "import datasets\n",
        "import os\n",
        "import redditcleaner\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from huggingface_hub import notebook_login\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from textstat import flesch_reading_ease as fre\n",
        "from textstat import flesch_kincaid_grade as fkg\n",
        "from datasets import (load_dataset,\n",
        "                      load,\n",
        "                      load_from_disk,\n",
        "                      Dataset,\n",
        "                      concatenate_datasets,\n",
        "                      DatasetDict)\n",
        "from itertools import compress\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import random\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M4c92Nr3tf7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Definitions"
      ],
      "metadata": {
        "id": "wp8NTXOaURD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_url_i(text):\n",
        "    # Define the regular expression pattern to match \"_url_i_\" where i is an arbitrary integer\n",
        "\n",
        "    patterns = [r\"_url_\\d+_\",r\"_Url_\\d+_\",r\"_URL_\\d+_\"]\n",
        "\n",
        "    # Use re.sub() to replace all occurrences of the pattern with an empty string\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, \"\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_example(example):\n",
        "\n",
        "    answers = example['answers']['text']\n",
        "    answers = [redditcleaner.clean(answer) for answer in answers]\n",
        "    answers = [re.sub('>.*?\\n',' ',answer) for answer in answers]\n",
        "    answers = [' '.join(answer.lower().split()) for answer in answers]\n",
        "    answers = [replace_url_i(answer) for answer in answers]\n",
        "    answers = [answer for answer in answers if len(answer.split())>=20]\n",
        "    example['answers']['text'] = answers\n",
        "\n",
        "    title = example['title']\n",
        "    title = redditcleaner.clean(title)\n",
        "    title = ' '.join(title.split())\n",
        "    title = replace_url_i(title)\n",
        "    example['title'] = title\n",
        "\n",
        "    selftext = example['selftext']\n",
        "    selftext = redditcleaner.clean(selftext)\n",
        "    selftext = ' '.join(selftext.lower().split())\n",
        "    selftext = replace_url_i(selftext)\n",
        "    example['selftext'] = selftext\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "class score_cutoff_wrapper:\n",
        "    def __init__(self,cutoff):\n",
        "        self.cutoff = cutoff\n",
        "\n",
        "    def score_cutoff_ex(self,example):\n",
        "        scores = example['answers']['score']\n",
        "        idxs = list(np.array(scores) >= self.cutoff)\n",
        "        for key, val in example['answers'].items():\n",
        "            example['answers'][key] = list(compress(val,idxs))\n",
        "\n",
        "        return example\n",
        "\n",
        "\n",
        "def score_cutoff(dataset,cutoff=4):\n",
        "    cutoff = score_cutoff_wrapper(cutoff)\n",
        "    ds = dataset.map(cutoff.score_cutoff_ex)\n",
        "    ds = ds.filter(lambda post: len(post['answers']['score'])>0)\n",
        "\n",
        "    return ds\n",
        "\n",
        "def flesch_scores(example):\n",
        "\n",
        "    fre_scores = [fre(text) for text in example['answers']['text']]\n",
        "    fkg_scores = [fkg(text) for text in example['answers']['text']]\n",
        "    example['answers']['fre'] = fre_scores\n",
        "    example['answers']['fkg'] = fkg_scores\n",
        "\n",
        "    return example\n",
        "\n",
        "class flesch_scores_filter_wrapper:\n",
        "    def __init__(self,fre_cutoff, fkg_cutoff):\n",
        "        self.fre_cutoff = fre_cutoff\n",
        "        self.fkg_cutoff = fkg_cutoff\n",
        "\n",
        "    def flesch_scores_filter(self,example):\n",
        "\n",
        "        fre_scores = example['answers']['fre']\n",
        "        fkg_scores = example['answers']['fkg']\n",
        "\n",
        "        idxs = [True if (fre_scores[i]>=self.fre_cutoff\n",
        "                         and fkg_scores[i]<self.fkg_cutoff) else False\n",
        "                for i in range(len(fre_scores))]\n",
        "\n",
        "        for key, val in example['answers'].items():\n",
        "            example['answers'][key] = list(compress(val,idxs))\n",
        "\n",
        "        return example\n",
        "\n",
        "def flesch_scores_cutoff(dataset,fre_cutoff=60,fkg_cutoff=9):\n",
        "    filter = flesch_scores_filter_wrapper(fre_cutoff, fkg_cutoff)\n",
        "    ds = dataset.map(filter.flesch_scores_filter)\n",
        "    ds = ds.filter(lambda post: len(post['answers']['score'])>0)\n",
        "\n",
        "    return ds\n",
        "\n",
        "def preprocess_data(dataset,\n",
        "                    output_file = './data/filtered',\n",
        "                    save_file = True,\n",
        "                    log_to_wandb = True,\n",
        "                    overwrite = False):\n",
        "\n",
        "    if os.path.exists(output_file) and not overwrite:\n",
        "        return load_from_disk(output_file)\n",
        "\n",
        "    not_qus = ['IAMA','AMA','ama:','megathread','Megathread',\n",
        "           'Discussion Thread','Discussion thread',\n",
        "           'discussion Thread','discussion thread',\n",
        "           'Ask Anything Wednesday','Free-for-All',\n",
        "           'Free-For-All','[META]','Monday Methods',\n",
        "           'Tuesday Trivia','Monday Mysteries',\n",
        "           'Theory Thursday','Monday Mish-Mash',\n",
        "           'Media Mondays','[META]','Wednesday Week in History',\n",
        "           'Saturday Popular Questions','Ask Anything Wednesday',\n",
        "           'Thursday Focus Historical Fiction']\n",
        "\n",
        "    qu_reqs = ['who','what','where','why','when','how','?']\n",
        "\n",
        "    dataset = dataset.map(preprocess_example)\n",
        "    dataset = dataset.filter(lambda post: 'nsfw' not in post['title'].lower())\n",
        "\n",
        "    dataset = dataset.filter(lambda post:\n",
        "                             not (all(qu_req not in post['title'].lower() for qu_req in qu_reqs)\n",
        "                             and all(qu_req not in post['selftext'].lower() for qu_req in qu_reqs)))\n",
        "\n",
        "    dataset = dataset.filter(lambda post:\n",
        "                             not (any(nq in post['title'] for nq in not_qus)))\n",
        "\n",
        "    dataset = dataset.map(flesch_scores)\n",
        "\n",
        "    dataset = score_cutoff(dataset)\n",
        "    dataset = flesch_scores_cutoff(dataset)\n",
        "\n",
        "    if save_file:\n",
        "        dataset.save_to_disk(output_file)\n",
        "\n",
        "        if log_to_wandb:\n",
        "            now = datetime.now()\n",
        "            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "            with wandb.init(project='ELI5_analysis',\n",
        "                            entity='ft-llmmm',\n",
        "                            job_type='preprocess_data',\n",
        "                            name=f'preprocess_data_{time_stamp}') as run:\n",
        "\n",
        "\n",
        "                processed_data_art=wandb.Artifact('ELI5_processed','dataset')\n",
        "                processed_data_art.add_dir(output_file)\n",
        "                run.log_artifact(processed_data_art)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def split_idxs(example):\n",
        "    scores = example['answers']['score']\n",
        "    scores_unique = sorted(set(scores),reverse=True)\n",
        "    pref_scores_idxs = [scores.index(sc)for sc in scores_unique]\n",
        "    dupl_scores_idxs = [n for n in range(len(scores)) if n not in pref_scores_idxs]\n",
        "\n",
        "    example['pref_idxs'] = pref_scores_idxs\n",
        "    example['dupl_scores_idxs'] = dupl_scores_idxs\n",
        "\n",
        "    return example\n",
        "\n",
        "def mult_ans_RM_proc(example):\n",
        "    pref_scores_idxs = example['pref_idxs']\n",
        "    for key, val in example['answers'].items():\n",
        "        example['answers'][key] = [example['answers'][key][i] for i in pref_scores_idxs]\n",
        "    return example\n",
        "\n",
        "def mult_ans_SFT_proc(example):\n",
        "    dupl_scores_idxs = example['dupl_scores_idxs']\n",
        "    for key, val in example['answers'].items():\n",
        "        example['answers'][key] = [example['answers'][key][i] for i in dupl_scores_idxs]\n",
        "    return example\n",
        "\n",
        "def split_ds(ds_original,\n",
        "             ds_filtered,\n",
        "             output_dir='ds_split',\n",
        "             save_file=True,\n",
        "             log_to_wandb = True,\n",
        "             overwrite = False):\n",
        "\n",
        "    if (all(os.path.exists(f'./data/{output_dir}/{split}') for split in ['ds_SFT','ds_RM','ds_RL'])\n",
        "        and not overwrite):\n",
        "\n",
        "        ds_split = {}\n",
        "\n",
        "        ds_split['SFT'] = load_from_disk(f'./data/{output_dir}/ds_SFT')\n",
        "        ds_split['RM'] = load_from_disk(f'./data/{output_dir}/ds_RM')\n",
        "        ds_split['RL'] = load_from_disk(f'./data/{output_dir}/ds_RL')\n",
        "\n",
        "        return ds_split\n",
        "\n",
        "    ds_split = {}\n",
        "\n",
        "    ds_mult = ds_filtered.filter(lambda post : len(post['answers']['score'])>=2)\n",
        "    ds_sing = ds_filtered.filter(lambda post : len(post['answers']['score'])==1)\n",
        "\n",
        "    ds_mult_indexed = ds_mult.map(split_idxs)\n",
        "\n",
        "    ds_split['RM'] = ds_mult_indexed.map(mult_ans_RM_proc)\n",
        "    ds_split['RM'] = ds_split['RM'].filter(lambda x: len(x['answers']['score'])>0)\n",
        "\n",
        "    ds_SFT_mult = ds_mult_indexed.map(mult_ans_SFT_proc)\n",
        "    ds_SFT_mult = ds_SFT_mult.filter(lambda x: len(x['answers']['score'])>0)\n",
        "\n",
        "    ds_split['SFT'] = datasets.DatasetDict()\n",
        "\n",
        "    for key in ['train','validation','test']:\n",
        "        ds_split['SFT'][key] = datasets.concatenate_datasets([ds_SFT_mult[key],\n",
        "                                                     ds_sing[key]])\n",
        "\n",
        "    q_ids_taken = []\n",
        "\n",
        "    for ds_ in (ds_split['SFT'],ds_split['RM']):\n",
        "        for split in ds_:\n",
        "            q_ids_taken.extend(ds_[split]['q_id'])\n",
        "\n",
        "    q_ids_taken = set(q_ids_taken)\n",
        "\n",
        "    ds_split['RL'] = ds_original.filter(lambda post: post['q_id'] not in q_ids_taken)\n",
        "    ds_split['RL'] = concatenate_datasets([ds for ds in ds_split['RL'].values()])\n",
        "\n",
        "    if save_file:\n",
        "\n",
        "        for key,value in ds_split.items():\n",
        "            value.save_to_disk(f'./data/{output_dir}/ds_{key}')\n",
        "\n",
        "        if log_to_wandb:\n",
        "            now = datetime.now()\n",
        "            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "            with wandb.init(project='ELI5_analysis',\n",
        "                            entity='ft-llmmm',\n",
        "                            job_type='split_data',\n",
        "                            name=f'split_data_{time_stamp}') as run:\n",
        "\n",
        "\n",
        "                split_data_art=wandb.Artifact('ELI5_split','dataset')\n",
        "\n",
        "                split_data_art.add_dir(f'./data/{output_dir}')\n",
        "                run.log_artifact(split_data_art)\n",
        "\n",
        "    return ds_split\n",
        "\n",
        "def combine_title_body(example):\n",
        "    title = ' '.join(example['title'].split())\n",
        "    selftext = ' '.join(example['selftext'].split())\n",
        "\n",
        "    combined = title +'\\n'+selftext\n",
        "\n",
        "    return {'title_body':combined}\n",
        "\n",
        "def embed_datasets(dataset_split,\n",
        "                   checkpoint ='all-mpnet-base-v2',\n",
        "                   output_dir = 'embedded',\n",
        "                   save_file = True,\n",
        "                   overwrite = False,\n",
        "                   log_to_wandb = True):\n",
        "\n",
        "    if (all(os.path.exists(f'./data/{output_dir}/ds_{subset}') for subset in ['SFT','RM','RL'])\n",
        "        and not overwrite):\n",
        "\n",
        "        ds_embedded = {}\n",
        "\n",
        "        for subset in ['SFT','RM','RL']:\n",
        "            ds_embedded[subset] = load_from_disk(f'./data/{output_dir}/ds_{subset}')\n",
        "        return ds_embedded\n",
        "\n",
        "    ds_embedded = {}\n",
        "    model = SentenceTransformer(checkpoint)\n",
        "\n",
        "    for key in dataset_split:\n",
        "        ds_embedded[key] = dataset_split[key].map(combine_title_body)\n",
        "        ds_embedded[key] = ds_embedded[key].map(lambda x:{'qu_emb':\n",
        "                                                          model.encode(x['title_body'],\n",
        "                                                                       batch_size=64)})\n",
        "    if save_file:\n",
        "\n",
        "        for key,value in ds_embedded.items():\n",
        "            value.save_to_disk(f'./data/{output_dir}/ds_{key}')\n",
        "\n",
        "        if log_to_wandb:\n",
        "            now = datetime.now()\n",
        "            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "            with wandb.init(project='ELI5_analysis',\n",
        "                            entity='ft-llmmm',\n",
        "                            job_type='embed_data',\n",
        "                            name=f'embed_data_{time_stamp}') as run:\n",
        "\n",
        "\n",
        "                embed_data_art=wandb.Artifact('ELI5_embedded','dataset')\n",
        "\n",
        "                embed_data_art.add_dir(f'./data/{output_dir}')\n",
        "                run.log_artifact(embed_data_art)\n",
        "\n",
        "    return ds_embedded\n",
        "\n",
        "def make_pairs(example):\n",
        "    answers = example['answers']['text']\n",
        "    scores = example['answers']['score']\n",
        "\n",
        "    sc_ans = tuple(zip(scores,answers))\n",
        "    sc_pairs = tuple(combinations(sc_ans,2))\n",
        "\n",
        "    if len(sc_pairs)>10:\n",
        "        sc_pairs = random.sample(sc_pairs,10)\n",
        "\n",
        "    sc_pairs = list(map(lambda x: sorted(x,key=lambda y:y[0],\n",
        "                                 reverse=True),sc_pairs))\n",
        "\n",
        "    pairs_text = [(sc_pair[0][1],sc_pair[1][1]) for sc_pair in sc_pairs]\n",
        "\n",
        "    example['pairs'] = pairs_text\n",
        "\n",
        "    return example\n",
        "\n",
        "def clean_datasets(ds_embedded,\n",
        "                   cutoff = 0.6,\n",
        "                   batch_size = 5000,\n",
        "                   output_dir = 'cleaned',\n",
        "                   save_file=True,\n",
        "                   overwrite = False,\n",
        "                   log_to_wandb = True\n",
        "                   ):\n",
        "\n",
        "    if (all(os.path.exists(f'./data/{output_dir}/ds_{subset}') for subset in ['SFT','RM','RL'])\n",
        "        and not overwrite):\n",
        "\n",
        "        ds_clean = {}\n",
        "\n",
        "        for subset in ['SFT','RM','RL']:\n",
        "            ds_clean[subset] = load_from_disk(f'./data/{output_dir}/ds_{subset}')\n",
        "        return ds_clean\n",
        "\n",
        "    embed_vecs = {}\n",
        "    overlaps = {}\n",
        "    idxs = {}\n",
        "    splits = ['train','validation','test']\n",
        "    keep_train = {}\n",
        "    keep_test = {}\n",
        "    ds_clean = {}\n",
        "\n",
        "\n",
        "\n",
        "    for subset in ['SFT',\"RM\"]:\n",
        "        print(f'Cleaning {subset} dataset')\n",
        "\n",
        "        ds_embedded[subset].set_format('torch')\n",
        "        embed_vecs[subset]={}\n",
        "\n",
        "        for split in splits:\n",
        "            embed_vecs[subset][split] = ds_embedded[subset][split]['qu_emb']\n",
        "            embed_vecs[subset][split] /= torch.sqrt(torch.sum(embed_vecs[subset][split]**2,\n",
        "                                                           dim=1,\n",
        "                                                           keepdim=True))\n",
        "\n",
        "        overlaps[subset] = {}\n",
        "        idxs[subset] = {}\n",
        "        for j in range(1,3):\n",
        "            for i in range(j):\n",
        "\n",
        "                overlaps[subset][(splits[i],splits[j])] = torch.matmul(\n",
        "                    embed_vecs[subset][splits[i]],\n",
        "                    embed_vecs[subset][splits[j]].T\n",
        "                )\n",
        "\n",
        "                idxs[subset][(splits[i],splits[j])] = torch.where((overlaps[subset][(splits[i],splits[j])])>=cutoff)\n",
        "\n",
        "        rm_tr_idxs_temp = idxs[subset]['train','validation'][0].numpy()\n",
        "        rm_tr_idxs_temp = set(rm_tr_idxs_temp)\n",
        "\n",
        "        rm_tr_idxs = idxs[subset]['train','test'][0].numpy()\n",
        "        rm_tr_idxs = set(rm_tr_idxs).union(rm_tr_idxs_temp)\n",
        "\n",
        "        keep_train = set(range(len(ds_embedded[subset]['train'])))-rm_tr_idxs\n",
        "\n",
        "        rm_test_idxs = idxs[subset]['validation','test'][1].numpy()\n",
        "        rm_test_idxs = set(rm_test_idxs)\n",
        "\n",
        "        keep_test = set(range(len(ds_embedded[subset]['test'])))-rm_test_idxs\n",
        "\n",
        "        ds_clean[subset] = DatasetDict()\n",
        "\n",
        "        ds_clean[subset]['train'] = ds_embedded[subset]['train'].select(keep_train)\n",
        "        ds_clean[subset]['validation'] = ds_embedded[subset]['validation']\n",
        "        ds_clean[subset]['test'] = ds_embedded[subset]['test'].select(keep_test)\n",
        "\n",
        "\n",
        "    print(f'Cleaning RL dataset')\n",
        "    ds_embedded['RL'].set_format('torch')\n",
        "    embed_vecs['RL'] = ds_embedded['RL']['qu_emb']\n",
        "    embed_vecs['RL'] /= torch.sqrt(torch.sum(embed_vecs['RL']**2,\n",
        "                                        dim = 1,\n",
        "                                        keepdim = True))\n",
        "\n",
        "    RL_size = len(ds_embedded['RL'])\n",
        "    rem_RL = set()\n",
        "    start = 0\n",
        "    i=0\n",
        "\n",
        "    num_batches = RL_size//batch_size\n",
        "\n",
        "    if RL_size%batch_size != 0:\n",
        "        num_batches += 1\n",
        "\n",
        "    for k in tqdm(range(num_batches)):\n",
        "\n",
        "        start = k*batch_size\n",
        "        end = (k+1)*batch_size\n",
        "\n",
        "        batch = embed_vecs['RL'][start:start+batch_size,:]\n",
        "\n",
        "        for subset in ['SFT','RM']:\n",
        "            for split in ['train','validation']:\n",
        "                overlap = torch.matmul(embed_vecs[subset][split],\n",
        "                                       batch.T)\n",
        "                rem_RL_idxs_temp = torch.where(overlap>=cutoff)[1].numpy()\n",
        "                rem_RL = rem_RL.union(set(rem_RL_idxs_temp))\n",
        "\n",
        "    keep_RL = set(range(RL_size))\n",
        "    keep_RL -= set(rem_RL)\n",
        "\n",
        "    ds_clean['RL'] = ds_embedded['RL'].select(keep_RL)\n",
        "\n",
        "    ds_clean['RM'] = ds_clean['RM'].map(lambda x:make_pairs(x))\n",
        "\n",
        "    if save_file:\n",
        "        for subset in ['SFT','RM','RL']:\n",
        "            ds_clean[subset].save_to_disk(f'./data/{output_dir}/ds_{subset}')\n",
        "\n",
        "        if log_to_wandb:\n",
        "            now = datetime.now()\n",
        "            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "            with wandb.init(project='ELI5_analysis',\n",
        "                            entity='ft-llmmm',\n",
        "                            job_type='clean_data',\n",
        "                            name=f'clean_data_{time_stamp}') as run:\n",
        "\n",
        "\n",
        "                clean_data_art=wandb.Artifact('ELI5_cleaned','dataset')\n",
        "                clean_data_art.add_dir(f'./data/{output_dir}')\n",
        "                run.log_artifact(clean_data_art)\n",
        "\n",
        "\n",
        "    return ds_clean"
      ],
      "metadata": {
        "id": "u28w0TgXUcE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "nPGvnBmK-l-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_original = load_dataset(\"vblagoje/lfqa\")"
      ],
      "metadata": {
        "id": "N_IUO6JoKKv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_filtered = preprocess_data(ds_original)"
      ],
      "metadata": {
        "id": "1fS-RqXhIYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_split = split_ds(ds_original,\n",
        "                    ds_filtered)"
      ],
      "metadata": {
        "id": "xwCBBzwYUlEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_embedded = embed_datasets(ds_split)"
      ],
      "metadata": {
        "id": "JOTZWH7Ihy1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_clean = clean_datasets(ds_embedded)"
      ],
      "metadata": {
        "id": "ZJTPzUg4DFFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}