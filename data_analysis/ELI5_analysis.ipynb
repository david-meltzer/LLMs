{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1gzbxeUeFa7tMCftY9gaqYmb76hSHev9s",
      "authorship_tag": "ABX9TyM1+sZ8oX7JC61LgTyoO/dE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david-meltzer/LLMs/blob/main/data_cleaning/ELI5_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "bdp_5O98KF7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/LLMs/ELI5_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HcMFFaHocM9",
        "outputId": "2137aac3-6263-44ad-f016-4a0a10410d2d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/LLMs/ELI5_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xX-WlvYb3Y0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15cc7837-6ead-479c-9a04-7ab8c337510d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets --quiet\n",
        "!pip install textstat --quiet\n",
        "!pip install wandb --quiet\n",
        "!pip install redditcleaner --quiet\n",
        "!pip install huggingface_hub --quiet\n",
        "!pip install -U sentence-transformers --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb, torch\n",
        "import sys\n",
        "import datasets\n",
        "import os\n",
        "import redditcleaner\n",
        "import re\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from huggingface_hub import notebook_login\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from textstat import flesch_reading_ease as fre\n",
        "from textstat import flesch_kincaid_grade as fkg\n",
        "from datasets import (load_dataset,\n",
        "                      load,\n",
        "                      load_from_disk,\n",
        "                      Dataset,\n",
        "                      concatenate_datasets,\n",
        "                      DatasetDict)\n",
        "from itertools import compress\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from itertools import combinations\n",
        "import random\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "M4c92Nr3tf7p"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Definitions"
      ],
      "metadata": {
        "id": "wp8NTXOaURD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_url_i(text):\n",
        "    \"\"\"\n",
        "    Replace all occurrences of the pattern \"_url_i_\", where i is an arbitrary integer, with an empty string in the input text.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The input text containing occurrences of the pattern to be replaced.\n",
        "\n",
        "    Returns:\n",
        "        str: The modified text with all occurrences of the pattern removed.\n",
        "\n",
        "    Example:\n",
        "        >>> replace_url_i(\"Check out my website: _url_123_ and _URL_456_\")\n",
        "        'Check out my website:  and '\n",
        "    \"\"\"\n",
        "    # Define the regular expression patterns to match \"_url_i_\" where i is an arbitrary integer\n",
        "    patterns = [r\"_url_\\d+_\", r\"_Url_\\d+_\", r\"_URL_\\d+_\"]\n",
        "\n",
        "    # Use re.sub() to replace all occurrences of the pattern with an empty string\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, \"\", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_example(example):\n",
        "    \"\"\"\n",
        "    Preprocess an example dictionary containing 'answers', 'title', and 'selftext' keys.\n",
        "\n",
        "    The function applies the following preprocessing steps to each element in the example:\n",
        "    1. Cleans all answers, titles, and selftext using redditcleaner.\n",
        "    2. Remove any quoted text starting with '>' and ending with a newline character in 'answers'.\n",
        "    3. Convert all characters in 'answers' to lowercase and remove extra whitespaces.\n",
        "    4. Remove any occurrences of \"_url_i_\" in 'answers', 'title', and 'selftext'.\n",
        "    5. Filter out answers with less than 20 words.\n",
        "    6. Convert the 'selftext' to lowercase and remove extra whitespaces.\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): A dictionary containing 'answers', 'title', and 'selftext' keys.\n",
        "\n",
        "    Returns:\n",
        "        dict: The preprocessed example dictionary with the above transformations applied.\n",
        "\n",
        "    Example:\n",
        "        >>> example = {\n",
        "                'answers': {'text': ['Visit this website: _url_123_', 'Sure, here is the link: _URL_456_']},\n",
        "                'title': 'How to use Python?',\n",
        "                'selftext': 'Check out this tutorial: _Url_789_ to learn Python.'\n",
        "            }\n",
        "        >>> preprocess_example(example)\n",
        "        {\n",
        "            'answers': {'text': ['visit this website:', 'sure, here is the link:']},\n",
        "            'title': 'how to use python?',\n",
        "            'selftext': 'check out this tutorial: to learn python.'\n",
        "        }\n",
        "    \"\"\"\n",
        "    # Preprocess 'answers'\n",
        "    answers = example['answers']['text']\n",
        "    answers = [redditcleaner.clean(answer) for answer in answers]\n",
        "    answers = [re.sub(r'>.*?\\n', ' ', answer) for answer in answers]\n",
        "    answers = [' '.join(answer.lower().split()) for answer in answers]\n",
        "    answers = [replace_url_i(answer) for answer in answers]\n",
        "    answers = [answer for answer in answers if len(answer.split()) >= 20]\n",
        "    example['answers']['text'] = answers\n",
        "\n",
        "    # Preprocess 'title'\n",
        "    title = example['title']\n",
        "    title = redditcleaner.clean(title)\n",
        "    title = ' '.join(title.split())\n",
        "    title = replace_url_i(title)\n",
        "    example['title'] = title\n",
        "\n",
        "    # Preprocess 'selftext'\n",
        "    selftext = example['selftext']\n",
        "    selftext = redditcleaner.clean(selftext)\n",
        "    selftext = ' '.join(selftext.lower().split())\n",
        "    selftext = replace_url_i(selftext)\n",
        "    example['selftext'] = selftext\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "\n",
        "class score_cutoff_wrapper:\n",
        "    \"\"\"\n",
        "    A wrapper class to filter answers based on a cutoff score from an example dictionary.\n",
        "\n",
        "    This class provides a method to filter the answers in an example based on their corresponding scores.\n",
        "    Answers with a score greater than or equal to the specified cutoff will be retained, and others will be removed.\n",
        "\n",
        "    Parameters:\n",
        "        cutoff (int or float): The cutoff score value to filter answers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cutoff):\n",
        "        \"\"\"\n",
        "        Initialize the score_cutoff_wrapper with the specified cutoff score.\n",
        "\n",
        "        Parameters:\n",
        "            cutoff (int or float): The cutoff score value to filter answers.\n",
        "        \"\"\"\n",
        "        self.cutoff = cutoff\n",
        "\n",
        "    def score_cutoff_ex(self, example):\n",
        "        \"\"\"\n",
        "        Filter the answers in the example based on the cutoff score.\n",
        "\n",
        "        Parameters:\n",
        "            example (dict): A dictionary containing 'answers' key with 'text' and 'score' lists.\n",
        "\n",
        "        Returns:\n",
        "            dict: The modified example dictionary with answers filtered based on the cutoff score.\n",
        "\n",
        "        Example:\n",
        "            >>> example = {\n",
        "                    'answers': {\n",
        "                        'text': ['Yes', 'No', 'Maybe'],\n",
        "                        'score': [10, 5, 8]\n",
        "                    }\n",
        "                }\n",
        "            >>> wrapper = score_cutoff_wrapper(cutoff=8)\n",
        "            >>> filtered_example = wrapper.score_cutoff_ex(example)\n",
        "            >>> filtered_example\n",
        "            {\n",
        "                'answers': {\n",
        "                    'text': ['Yes', 'Maybe'],\n",
        "                    'score': [10, 8]\n",
        "                }\n",
        "            }\n",
        "        \"\"\"\n",
        "        scores = example['answers']['score']\n",
        "        # Find idxs where scores >= cutoff.\n",
        "        idxs = list(np.array(scores) >= self.cutoff)\n",
        "        # For each (key,value) pair in dictionary example['answers'] only\n",
        "        # keep text and metadata for answers with a high enough score.\n",
        "        for key, val in example['answers'].items():\n",
        "            example['answers'][key] = list(compress(val, idxs))\n",
        "\n",
        "        return example\n",
        "\n",
        "\n",
        "def score_cutoff(dataset,cutoff=4):\n",
        "    \"\"\"\n",
        "    Uses class score_cutoff_wrapper to filter a Huggingface dataset to only keep\n",
        "    scores above a certain cutoff.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (Dataset): The input Huggingface dataset to be filtered.\n",
        "        cutoff (int or float, optional): The cutoff score value to filter answers. Default is 4.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: The modified dataset with answers filtered based on the cutoff score.\n",
        "    \"\"\"\n",
        "    cutoff = score_cutoff_wrapper(cutoff)\n",
        "    ds = dataset.map(cutoff.score_cutoff_ex)\n",
        "    ds = ds.filter(lambda post: len(post['answers']['score'])>0)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "def flesch_scores(example):\n",
        "    \"\"\"\n",
        "    Calculate Flesch Readability scores for each answer in the example.\n",
        "\n",
        "    This function calculates Flesch Readability scores and Flesch-Kincaid Grade levels for each answer in the example.\n",
        "    The calculated scores are then added to the example dictionary under the 'fre' (Flesch Readability) and 'fkg'\n",
        "    (Flesch-Kincaid Grade) keys.\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): A dictionary containing 'answers' key with 'text' lists for each answer.\n",
        "\n",
        "    Returns:\n",
        "        dict: The modified example dictionary with Flesch Readability and Flesch-Kincaid Grade scores.\n",
        "\n",
        "    Example:\n",
        "        >>> example = {\n",
        "                'answers': {\n",
        "                    'text': ['This is a sample answer.', 'Another answer with more words.']\n",
        "                }\n",
        "            }\n",
        "        >>> flesch_scores(example)\n",
        "        {\n",
        "            'answers': {\n",
        "                'text': ['This is a sample answer.', 'Another answer with more words.'],\n",
        "                'fre': [89.1, 79.2],\n",
        "                'fkg': [2.6, 5.5]\n",
        "            }\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute Flesch Readability score for each answer.\n",
        "    fre_scores = [flesch(text) for text in example['answers']['text']]\n",
        "    # Compute Flesch Kincaid Grade level for each answer.\n",
        "    fkg_scores = [flesch_kincaid(text) for text in example['answers']['text']]\n",
        "    # Add corresponding metrics to dictioanry example['answers'].\n",
        "    example['answers']['fre'] = fre_scores\n",
        "    example['answers']['fkg'] = fkg_scores\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "class flesch_scores_filter_wrapper:\n",
        "    \"\"\"\n",
        "    This class provides a method to filter answers in an example based on Flesch Readability (fre) and\n",
        "    Flesch-Kincaid Grade (fkg) scores. Answers with fre >= fre_cutoff and fkg < fkg_cutoff will be retained,\n",
        "    and others will be removed.\n",
        "\n",
        "    Parameters:\n",
        "        fre_cutoff (float): The cutoff value for Flesch Readability score.\n",
        "        fkg_cutoff (float): The cutoff value for Flesch-Kincaid Grade score.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fre_cutoff, fkg_cutoff):\n",
        "        \"\"\"\n",
        "        Initialize the flesch_scores_filter_wrapper with the specified cutoff scores.\n",
        "\n",
        "        Parameters:\n",
        "            fre_cutoff (float): The cutoff value for Flesch Readability score.\n",
        "            fkg_cutoff (float): The cutoff value for Flesch-Kincaid Grade score.\n",
        "        \"\"\"\n",
        "        self.fre_cutoff = fre_cutoff\n",
        "        self.fkg_cutoff = fkg_cutoff\n",
        "\n",
        "    def flesch_scores_filter(self, example):\n",
        "        \"\"\"\n",
        "        Applies filter to specific example using self.fre_cutoff and self.fkg_cutoff.\n",
        "\n",
        "        Parameters:\n",
        "            example (dict): A dictionary containing 'answers' key with 'fre' and 'fkg' lists.\n",
        "\n",
        "        Returns:\n",
        "            dict: The modified example dictionary with answers filtered based on the cutoff scores.\n",
        "\n",
        "        Example:\n",
        "            >>> example = {\n",
        "                    'answers': {\n",
        "                        'text': ['This is a sample answer.', 'Another answer with more words.'],\n",
        "                        'fre': [89.1, 79.2],\n",
        "                        'fkg': [2.6, 5.5]\n",
        "                    }\n",
        "                }\n",
        "            >>> filter = flesch_scores_filter_wrapper(fre_cutoff=80, fkg_cutoff=5)\n",
        "            >>> filtered_example = filter.flesch_scores_filter(example)\n",
        "            >>> filtered_example\n",
        "            {\n",
        "                'answers': {\n",
        "                    'text': ['This is a sample answer.'],\n",
        "                    'fre': [89.1],\n",
        "                    'fkg': [2.6]\n",
        "                }\n",
        "            }\n",
        "        \"\"\"\n",
        "        fre_scores = example['answers']['fre']\n",
        "        fkg_scores = example['answers']['fkg']\n",
        "\n",
        "        idxs = [True if (fre_scores[i] >= self.fre_cutoff\n",
        "                         and fkg_scores[i] < self.fkg_cutoff) \\\n",
        "                else False for i in range(len(fre_scores))]\n",
        "\n",
        "        # Use 'compress' to filter the values based on the boolean mask 'idxs'\n",
        "        for key, val in example['answers'].items():\n",
        "            example['answers'][key] = list(compress(val, idxs))\n",
        "\n",
        "        return example\n",
        "\n",
        "\n",
        "def flesch_scores_cutoff(dataset, fre_cutoff=60, fkg_cutoff=9):\n",
        "    \"\"\"\n",
        "    This function applies flesch_scores_filter_wrapper to a Huggingface dataset.\n",
        "    Only answers with fre >= fre_cutoff and fkg < fkg_cutoff.\n",
        "    Posts with no qualifying answers will be removed.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (Dataset): Huggingface dataset to be filtered.\n",
        "        fre_cutoff (float, optional): The cutoff value for Flesch Readability score. Default is 60.\n",
        "        fkg_cutoff (float, optional): The cutoff value for Flesch-Kincaid Grade score. Default is 9.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: The modified dataset with answers filtered based on the Flesch Readability and Flesch-Kincaid Grade scores.\n",
        "\n",
        "    Example:\n",
        "        >>> dataset = Dataset.from_dict({\n",
        "                'answers': {\n",
        "                    'text': ['This is a sample answer.', 'Another answer with more words.'],\n",
        "                    'fre': [89.1, 79.2],\n",
        "                    'fkg': [2.6, 5.5]\n",
        "                }\n",
        "            })\n",
        "        >>> filtered_dataset = flesch_scores_cutoff(dataset, fre_cutoff=80, fkg_cutoff=5)\n",
        "        >>> len(filtered_dataset)\n",
        "        1\n",
        "        >>> filtered_dataset[0]['answers']['text']\n",
        "        ['This is a sample answer.']\n",
        "        >>> filtered_dataset[0]['answers']['fre']\n",
        "        [89.1]\n",
        "        >>> filtered_dataset[0]['answers']['fkg']\n",
        "        [2.6]\n",
        "    \"\"\"\n",
        "\n",
        "    # Define filter function.\n",
        "    filter = flesch_scores_filter_wrapper(fre_cutoff, fkg_cutoff)\n",
        "    # Apply function to entire dataset.\n",
        "    ds = dataset.map(filter.flesch_scores_filter)\n",
        "    # Remove any posts with no valid answers.\n",
        "    ds = ds.filter(lambda post: len(post['answers']['fre']) > 0)\n",
        "\n",
        "    return ds\n",
        "\n",
        "def preprocess_data(dataset,\n",
        "                    output_file='./data/filtered',\n",
        "                    save_file=True,\n",
        "                    log_to_wandb=True,\n",
        "                    overwrite=False):\n",
        "    \"\"\"\n",
        "    Preprocesses the input dataset by applying various filters and transformations.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (Dataset): The input Huggingface dataset to be processed.\n",
        "        output_file (str, optional): The path to the file where the processed dataset will be saved.\n",
        "            Default is './data/filtered'.\n",
        "        save_file (bool, optional): If True, saves the processed dataset to the output_file.\n",
        "            Default is True.\n",
        "        log_to_wandb (bool, optional): If True, logs the processed dataset as a WandB artifact.\n",
        "            Default is True.\n",
        "        overwrite (bool, optional): If True, overwrites the output_file if it already exists.\n",
        "            Default is False.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: The preprocessed dataset.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.exists(output_file) and not overwrite:\n",
        "        # If the output_file exists and overwrite is False, load the dataset from disk and return it.\n",
        "        return load_from_disk(output_file)\n",
        "\n",
        "    # List of strings to filter out posts based on their titles\n",
        "    not_qus = ['IAMA', 'AMA', 'ama:', 'megathread', 'Megathread',\n",
        "               'Discussion Thread', 'Discussion thread',\n",
        "               'discussion Thread', 'discussion thread',\n",
        "               'Ask Anything Wednesday', 'Free-for-All',\n",
        "               'Free-For-All', '[META]', 'Monday Methods',\n",
        "               'Tuesday Trivia', 'Monday Mysteries',\n",
        "               'Theory Thursday', 'Monday Mish-Mash',\n",
        "               'Media Mondays', '[META]', 'Wednesday Week in History',\n",
        "               'Saturday Popular Questions', 'Ask Anything Wednesday',\n",
        "               'Thursday Focus Historical Fiction']\n",
        "\n",
        "    # List of question words used to filter out posts without meaningful questions in their titles or selftext\n",
        "    qu_reqs = ['who', 'what', 'where', 'why', 'when', 'how', '?']\n",
        "\n",
        "    # Preprocess each example in the dataset using the preprocess_example function\n",
        "    dataset = dataset.map(preprocess_example)\n",
        "\n",
        "    # Filter out posts with 'nsfw' in their titles\n",
        "    dataset = dataset.filter(lambda post: 'nsfw' not in post['title'].lower())\n",
        "\n",
        "    # Filter out posts that do not contain meaningful questions in their titles or selftext\n",
        "    dataset = dataset.filter(lambda post:\n",
        "                             not (all(qu_req not in post['title'].lower() for qu_req in qu_reqs)\n",
        "                                  and all(qu_req not in post['selftext'].lower() for qu_req in qu_reqs)))\n",
        "\n",
        "    # Filter out posts that do not correspond to questions.\n",
        "    dataset = dataset.filter(lambda post: not (any(nq in post['title'] for nq in not_qus)))\n",
        "\n",
        "    # Map the flesch_scores function to calculate Flesch readability scores for each post\n",
        "    dataset = dataset.map(flesch_scores)\n",
        "\n",
        "    # Apply score_cutoff function to remove posts with low Flesch scores\n",
        "    dataset = score_cutoff(dataset)\n",
        "\n",
        "    # Apply flesch_scores_cutoff function to remove posts with scores below a certain threshold\n",
        "    dataset = flesch_scores_cutoff(dataset)\n",
        "\n",
        "    if save_file:\n",
        "        # Save the processed dataset to the output_file\n",
        "        dataset.save_to_disk(output_file)\n",
        "\n",
        "        if log_to_wandb:\n",
        "            # Log the processed dataset as a WandB artifact if log_to_wandb is True\n",
        "            now = datetime.now()\n",
        "            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "            with wandb.init(project='ELI5_analysis',\n",
        "                            entity='ft-llmmm',\n",
        "                            job_type='preprocess_data',\n",
        "                            name=f'preprocess_data_{time_stamp}') as run:\n",
        "                # Initialize a WandB run for logging\n",
        "                processed_data_art = wandb.Artifact('ELI5_processed', 'dataset')\n",
        "                processed_data_art.add_dir(output_file)\n",
        "                run.log_artifact(processed_data_art)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def split_idxs(example):\n",
        "    \"\"\"\n",
        "    Splits the indices of scores from the input example's answers into two sets,\n",
        "    pref_scores_idxs and dupl_scores_idxs.\n",
        "\n",
        "    pref_scores_idxs = Each index in pref_scores_idxs corresponds\n",
        "                       to a unique score in example['answers']['score'].\n",
        "\n",
        "\n",
        "    dupl_scores_idxs = List of indices of example['answers']['score']\n",
        "                       not found in pref_scores_idxs.\n",
        "\n",
        "    pref_scores_idx correspond to indices of answers we will use for preference modeling\n",
        "    since there are no ties in this set.\n",
        "\n",
        "    dupl_scores_idxs correponds to indices of answers we will use for supervised fine-tuning.\n",
        "\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): The input example containing 'answers' as a dictionary with 'score' as a list.\n",
        "\n",
        "    Returns:\n",
        "        dict: The modified input example with 'pref_idxs' and 'dupl_scores_idxs' added.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the 'score' list from the 'answers' dictionary in the example.\n",
        "    scores = example['answers']['score']\n",
        "\n",
        "    # Sort the unique scores in descending order.\n",
        "    scores_unique = sorted(set(scores), reverse=True)\n",
        "\n",
        "    # Get the indices of the preferred scores in the 'scores' list.\n",
        "    pref_scores_idxs = [scores.index(sc) for sc in scores_unique]\n",
        "\n",
        "    # Get the indices of duplicate scores in the 'scores' list.\n",
        "    dupl_scores_idxs = [n for n in range(len(scores)) if n not in pref_scores_idxs]\n",
        "\n",
        "    # Add the preferred and duplicate scores indices to the input example.\n",
        "    example['pref_idxs'] = pref_scores_idxs\n",
        "    example['dupl_scores_idxs'] = dupl_scores_idxs\n",
        "\n",
        "    # Return the modified example with the added indices.\n",
        "    return example\n",
        "\n",
        "def mult_ans_RM_proc(example):\n",
        "    \"\"\"\n",
        "    Processes posts containing multiple answers. Only retains answers that will be used for\n",
        "    preference modelling.\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): The input example containing 'pref_idxs' and 'answers' as dictionary keys.\n",
        "            'pref_idxs' is a list of indices corresponding to answers we will use for preference modelling.\n",
        "             Value associated to the key 'answers' is a dictionary containing the text and metadata of the answers.\n",
        "\n",
        "    Returns:\n",
        "        dict: The modified input example with 'answers' containing only text and metadata used for preference modeling.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    pref_scores_idxs = example['pref_idxs']\n",
        "\n",
        "    # Iterate through each key-value pair in the 'answers' dictionary.\n",
        "    for key, val in example['answers'].items():\n",
        "        # Update the 'answers' dictionary by keeping only answers to be used for preference modeling.\n",
        "        example['answers'][key] = [example['answers'][key][i] for i in pref_scores_idxs]\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "def mult_ans_SFT_proc(example):\n",
        "    \"\"\"\n",
        "    Processes posts with multiple answers where we only retain answers that will be used for supervised fine-tuning.\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): The input example containing 'dupl_scores_idxs' and 'answers' as dictionary keys.\n",
        "            'dupl_scores_idxs' is a list of indices of duplicate scores, and 'answers' is a dictionary\n",
        "            with lists of the text of answers and their metadata.\n",
        "\n",
        "    Returns:\n",
        "        dict: The modified input example with 'answers' containing only duplicate scores' answers.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve the list of indices of duplicate scores from the 'dupl_scores_idxs' key.\n",
        "    dupl_scores_idxs = example['dupl_scores_idxs']\n",
        "\n",
        "    # Iterate through each key-value pair in the 'answers' dictionary.\n",
        "    for key, val in example['answers'].items():\n",
        "        # Update the 'answers' dictionary by keeping only the text and meta-data\n",
        "        # corresponding to duplicate scores' indices.\n",
        "        example['answers'][key] = [example['answers'][key][i] for i in dupl_scores_idxs]\n",
        "\n",
        "    return example\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "def split_ds(ds_original,\n",
        "             ds_filtered,\n",
        "             output_dir='ds_split',\n",
        "             save_file=True,\n",
        "             log_to_wandb=True,\n",
        "             overwrite=False):\n",
        "    \"\"\"\n",
        "    Splits the datasets into supervised fine-tuning (SFT), reward modeling (RM), and reinforcement learning (RL) subsets.\n",
        "\n",
        "    Parameters:\n",
        "        ds_original (Dataset): The original dataset containing all examples.\n",
        "        ds_filtered (Dataset): The filtered dataset containing relevant examples for SFT and RM datasets.\n",
        "        output_dir (str, optional): The directory where the split datasets will be saved.\n",
        "            Default is 'ds_split'.\n",
        "        save_file (bool, optional): If True, saves the split datasets to disk. Default is True.\n",
        "        log_to_wandb (bool, optional): If True, logs the split datasets as a WandB artifact.\n",
        "            Default is True.\n",
        "        overwrite (bool, optional): If True, overwrites existing split datasets in the output directory.\n",
        "            Default is False.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the split datasets for SFT, RM, and RL.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the split datasets already exist in the output directory and overwrite is False.\n",
        "    if (all(os.path.exists(f'./data/{output_dir}/{split}') for split in ['ds_SFT', 'ds_RM', 'ds_RL'])\n",
        "        and not overwrite):\n",
        "\n",
        "        ds_split = {}\n",
        "\n",
        "        # Load the split datasets from disk and return them.\n",
        "        ds_split['SFT'] = load_from_disk(f'./data/{output_dir}/ds_SFT')\n",
        "        ds_split['RM'] = load_from_disk(f'./data/{output_dir}/ds_RM')\n",
        "        ds_split['RL'] = load_from_disk(f'./data/{output_dir}/ds_RL')\n",
        "\n",
        "        return ds_split\n",
        "\n",
        "    ds_split = {}\n",
        "\n",
        "    # Filter examples with multiple answers and single answers separately.\n",
        "    ds_mult = ds_filtered.filter(lambda post: len(post['answers']['score']) >= 2)\n",
        "    ds_sing = ds_filtered.filter(lambda post: len(post['answers']['score']) == 1)\n",
        "\n",
        "    # Process examples with multiple answers using the 'mult_ans_RM_proc' function to retain only answers that\n",
        "    # will be used for preference modeling. We choose answers with unique scores to avoid ties during preference modeling.\n",
        "    ds_mult_indexed = ds_mult.map(split_idxs)\n",
        "    ds_split['RM'] = ds_mult_indexed.map(mult_ans_RM_proc)\n",
        "    ds_split['RM'] = ds_split['RM'].filter(lambda x: len(x['answers']['score']) > 0)\n",
        "\n",
        "    # Process examples with multiple answers using the 'mult_ans_SFT_proc' function to retain only duplicate scores' answers.\n",
        "    # These will be added to SFT dataset.\n",
        "    ds_SFT_mult = ds_mult_indexed.map(mult_ans_SFT_proc)\n",
        "    ds_SFT_mult = ds_SFT_mult.filter(lambda x: len(x['answers']['score']) > 0)\n",
        "\n",
        "    # Form SFT dataset by combining answers for posts with a unique answers and the\n",
        "    # answers corresponding to the \"duplicate indices\" for posts with multiple answers.\n",
        "    ds_split['SFT'] = datasets.DatasetDict()\n",
        "\n",
        "    for key in ['train', 'validation', 'test']:\n",
        "        ds_split['SFT'][key] = datasets.concatenate_datasets([ds_SFT_mult[key], ds_sing[key]])\n",
        "\n",
        "    # Collect the question IDs of examples used in SFT and RM to exclude them from RL.\n",
        "    q_ids_taken = []\n",
        "\n",
        "    for ds_ in (ds_split['SFT'], ds_split['RM']):\n",
        "        for split in ds_:\n",
        "            q_ids_taken.extend(ds_[split]['q_id'])\n",
        "\n",
        "    q_ids_taken = set(q_ids_taken)\n",
        "\n",
        "    # Create the RL subset by excluding examples used in SFT and RM.\n",
        "    ds_split['RL'] = ds_original.filter(lambda post: post['q_id'] not in q_ids_taken)\n",
        "    ds_split['RL'] = datasets.concatenate_datasets([ds for ds in ds_split['RL'].values()])\n",
        "\n",
        "    # Save the split datasets to disk.\n",
        "    if save_file:\n",
        "\n",
        "        for key, value in ds_split.items():\n",
        "            value.save_to_disk(f'./data/{output_dir}/ds_{key}')\n",
        "\n",
        "        # Log the split datasets as a WandB artifact if log_to_wandb is True.\n",
        "        if log_to_wandb:\n",
        "            now = datetime.now()\n",
        "            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "            with wandb.init(project='ELI5_analysis',\n",
        "                            entity='ft-llmmm',\n",
        "                            job_type='split_data',\n",
        "                            name=f'split_data_{time_stamp}') as run:\n",
        "\n",
        "                split_data_art = wandb.Artifact('ELI5_split', 'dataset')\n",
        "                split_data_art.add_dir(f'./data/{output_dir}')\n",
        "                run.log_artifact(split_data_art)\n",
        "\n",
        "    # Return the dictionary containing the split datasets for SFT, RM, and RL.\n",
        "    return ds_split\n",
        "\n",
        "\n",
        "\n",
        "def combine_title_body(example):\n",
        "    \"\"\"\n",
        "    Combines the title and body (selftext) of the input example into a single string and updates the input example.\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): The input example containing 'title' and 'selftext' as keys.\n",
        "\n",
        "    Returns:\n",
        "        dict: The modified input example with the combined string of the title and body\n",
        "              under the key 'title_body'.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove extra spaces and join the words in the 'title' string.\n",
        "    title = ' '.join(example['title'].split())\n",
        "\n",
        "    # Remove extra spaces and join the words in the 'selftext' string.\n",
        "    selftext = ' '.join(example['selftext'].split())\n",
        "\n",
        "    # Combine the 'title' and 'selftext' strings with a newline separator.\n",
        "    combined = title + '\\n' + selftext\n",
        "\n",
        "    # Add the combined string under the key 'title_body' in the input example.\n",
        "    example['title_body'] = combined\n",
        "\n",
        "    # Return the modified input example.\n",
        "    return example\n",
        "\n",
        "def embed_datasets(dataset_split,\n",
        "                   checkpoint='all-mpnet-base-v2',\n",
        "                   output_dir='embedded',\n",
        "                   save_file=True,\n",
        "                   overwrite=False,\n",
        "                   log_to_wandb=True):\n",
        "    \"\"\"\n",
        "    Embeds the datasets using a pre-trained SentenceTransformer model and saves the embeddings to disk.\n",
        "\n",
        "    Parameters:\n",
        "        dataset_split (dict): A dictionary containing different dataset splits as values (e.g., train, validation).\n",
        "        checkpoint (str, optional): The name of the SentenceTransformer model checkpoint to use.\n",
        "            Default is 'all-mpnet-base-v2'.\n",
        "        output_dir (str, optional): The directory where the embedded datasets will be saved.\n",
        "            Default is 'embedded'.\n",
        "        save_file (bool, optional): If True, saves the embedded datasets to disk. Default is True.\n",
        "        overwrite (bool, optional): If True, overwrites existing embedded datasets in the output directory.\n",
        "            Default is False.\n",
        "        log_to_wandb (bool, optional): If True, logs the embedded datasets as a WandB artifact.\n",
        "            Default is True.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the embedded datasets.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if the embedded datasets already exist in the output directory and overwrite is False.\n",
        "    if (all(os.path.exists(f'./data/{output_dir}/ds_{subset}') for subset in ['SFT', 'RM', 'RL'])\n",
        "        and not overwrite):\n",
        "\n",
        "        ds_embedded = {}\n",
        "\n",
        "        # Load the embedded datasets from disk and return them.\n",
        "        for subset in ['SFT', 'RM', 'RL']:\n",
        "            ds_embedded[subset] = load_from_disk(f'./data/{output_dir}/ds_{subset}')\n",
        "        return ds_embedded\n",
        "\n",
        "    # Initialize a dictionary to store the embedded datasets.\n",
        "    ds_embedded = {}\n",
        "\n",
        "    # Initialize the SentenceTransformer model.\n",
        "    model = SentenceTransformer(checkpoint)\n",
        "\n",
        "    # Loop through each dataset split and embed the examples.\n",
        "    for key in dataset_split:\n",
        "        ds_embedded[key] = dataset_split[key].map(combine_title_body)\n",
        "        ds_embedded[key] = ds_embedded[key].map(lambda x: {'qu_emb': model.encode(x['title_body'], batch_size=64)})\n",
        "\n",
        "    # Save the embedded datasets to disk.\n",
        "    if save_file:\n",
        "        for key, value in ds_embedded.items():\n",
        "            value.save_to_disk(f'./data/{output_dir}/ds_{key}')\n",
        "\n",
        "        # Log the embedded datasets as a WandB artifact if log_to_wandb is True.\n",
        "        if log_to_wandb:\n",
        "            now = datetime.now()\n",
        "            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "            with wandb.init(project='ELI5_analysis',\n",
        "                            entity='ft-llmmm',\n",
        "                            job_type='embed_data',\n",
        "                            name=f'embed_data_{time_stamp}') as run:\n",
        "\n",
        "                embed_data_art = wandb.Artifact('ELI5_embedded', 'dataset')\n",
        "                embed_data_art.add_dir(f'./data/{output_dir}')\n",
        "                run.log_artifact(embed_data_art)\n",
        "\n",
        "    # Return the dictionary containing the embedded datasets.\n",
        "    return ds_embedded\n",
        "\n",
        "\n",
        "def make_pairs(example):\n",
        "    \"\"\"\n",
        "    Creates pairs of answers from the input example based on their scores and updates the example.\n",
        "\n",
        "    Parameters:\n",
        "        example (dict): The input example containing 'answers' as a dictionary with 'text' and 'score' lists.\n",
        "\n",
        "    Returns:\n",
        "        dict: The modified input example with the 'pairs' key containing the created pairs of answers.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the 'text' and 'score' lists from the 'answers' dictionary in the example.\n",
        "    answers = example['answers']['text']\n",
        "    scores = example['answers']['score']\n",
        "\n",
        "    # Create a list of tuples with each tuple containing the score and its corresponding answer.\n",
        "    sc_ans = tuple(zip(scores, answers))\n",
        "\n",
        "    # Generate pairs of tuples using combinations() from the 'sc_ans' list.\n",
        "    sc_pairs = tuple(combinations(sc_ans, 2))\n",
        "\n",
        "    # If the number of pairs is greater than 10, randomly select 10 pairs from the list.\n",
        "    if len(sc_pairs) > 10:\n",
        "        sc_pairs = random.sample(sc_pairs, 10)\n",
        "\n",
        "    # Sort each pair of tuples based on their score in descending order.\n",
        "    sc_pairs = list(map(lambda x: sorted(x, key=lambda y: y[0], reverse=True), sc_pairs))\n",
        "\n",
        "    # Extract the answers from the sorted pairs and create a list of answer pairs.\n",
        "    pairs_text = [(sc_pair[0][1], sc_pair[1][1]) for sc_pair in sc_pairs]\n",
        "\n",
        "    # Add the 'pairs' key to the input example with the created answer pairs.\n",
        "    example['pairs'] = pairs_text\n",
        "\n",
        "    # Return the modified input example.\n",
        "    return example\n",
        "\n",
        "def clean_datasets(ds_embedded,\n",
        "                   cutoff=0.6,\n",
        "                   batch_size=5000,\n",
        "                   output_dir='cleaned',\n",
        "                   save_file=True,\n",
        "                   overwrite=False,\n",
        "                   log_to_wandb=True):\n",
        "    \"\"\"\n",
        "    Cleans the datasets by removing redundant examples based on the similarity of embedded vectors.\n",
        "\n",
        "    Parameters:\n",
        "        ds_embedded (dict): A dictionary containing the embedded datasets for supervised fine-tuning (SFT),\n",
        "                            reward modeling (RM), and reinforcement learning (RL).\n",
        "        cutoff (float, optional): The similarity threshold to consider examples as redundant.\n",
        "            Default is 0.6.\n",
        "        batch_size (int, optional): The batch size used for processing RL dataset.\n",
        "            Default is 5000.\n",
        "        output_dir (str, optional): The directory where the cleaned datasets will be saved.\n",
        "            Default is 'cleaned'.\n",
        "        save_file (bool, optional): If True, saves the cleaned datasets to disk. Default is True.\n",
        "        overwrite (bool, optional): If True, overwrites existing cleaned datasets in the output directory.\n",
        "            Default is False.\n",
        "        log_to_wandb (bool, optional): If True, logs the cleaned datasets as a WandB artifact.\n",
        "            Default is True.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the cleaned datasets for SFT, RM, and RL.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #ds_clean is a dictionary which contains DatasetDicts as values.\n",
        "    ds_clean = {}\n",
        "\n",
        "    # Check if the cleaned datasets already exist in the output directory and overwrite is False.\n",
        "    if (all(os.path.exists(f'./data/{output_dir}/ds_{subset}') for subset in ['SFT', 'RM', 'RL'])\n",
        "        and not overwrite):\n",
        "\n",
        "        # Load the cleaned datasets from disk and return them.\n",
        "        for subset in ['SFT', 'RM', 'RL']:\n",
        "            ds_clean[subset] = load_from_disk(f'./data/{output_dir}/ds_{subset}')\n",
        "        return ds_clean\n",
        "\n",
        "    # Initialize dictionaries to store normalized embedding vectors and overlaps between splits for SFT and RM datasets.\n",
        "    embed_vecs = {}\n",
        "    overlaps = {}\n",
        "    idxs = {}\n",
        "\n",
        "    # standard splitting of data in supervised learning.\n",
        "    splits = ['train', 'validation', 'test']\n",
        "\n",
        "    # Cleaning SFT and RM datasets.\n",
        "    for subset in ['SFT', \"RM\"]:\n",
        "        print(f'Cleaning {subset} dataset')\n",
        "\n",
        "        # Set the format of dataset to 'torch' to enable torch operations on the embedded vectors.\n",
        "        ds_embedded[subset].set_format('torch')\n",
        "        embed_vecs[subset] = {}\n",
        "\n",
        "        # Normalize the embedded vectors for each split.\n",
        "        for split in splits:\n",
        "            embed_vecs[subset][split] = ds_embedded[subset][split]['qu_emb']\n",
        "            embed_vecs[subset][split] /= torch.sqrt(torch.sum(embed_vecs[subset][split] ** 2,\n",
        "                                                             dim=1,\n",
        "                                                             keepdim=True))\n",
        "\n",
        "        overlaps[subset] = {}\n",
        "        idxs[subset] = {}\n",
        "\n",
        "        # Compute the overlaps between splits and store the indices of redundant examples.\n",
        "        for j in range(1, 3):\n",
        "            for i in range(j):\n",
        "                overlaps[subset][(splits[i], splits[j])] = torch.matmul(\n",
        "                    embed_vecs[subset][splits[i]],\n",
        "                    embed_vecs[subset][splits[j]].T\n",
        "                )\n",
        "\n",
        "                idxs[subset][(splits[i], splits[j])] = torch.where((overlaps[subset][(splits[i], splits[j])]) >= cutoff)\n",
        "\n",
        "        # Find indices of examples to remove from the training set due to overlap between train and validation splits.\n",
        "        rm_tr_idxs_temp = idxs[subset]['train', 'validation'][0].numpy()\n",
        "        rm_tr_idxs_temp = set(rm_tr_idxs_temp)\n",
        "\n",
        "        # Find indices of examples to remove from the training set due to overlap between train and test splits.\n",
        "        rm_tr_idxs = idxs[subset]['train', 'test'][0].numpy()\n",
        "        rm_tr_idxs = set(rm_tr_idxs).union(rm_tr_idxs_temp)\n",
        "\n",
        "        # Indices to keep in train set.\n",
        "        keep_train = set(range(len(ds_embedded[subset]['train']))) - rm_tr_idxs\n",
        "\n",
        "        # Find indices of examples to remove from the test set due to overlap between validation and test splits.\n",
        "        # Remove examples from test set because it is larger than the validation set.\n",
        "        rm_test_idxs = idxs[subset]['validation', 'test'][1].numpy()\n",
        "        rm_test_idxs = set(rm_test_idxs)\n",
        "\n",
        "        # Indices to keep in train set.\n",
        "        keep_test = set(range(len(ds_embedded[subset]['test']))) - rm_test_idxs\n",
        "\n",
        "        # Create a new DatasetDict containing the cleaned subsets for SFT and RM.\n",
        "        ds_clean[subset] = datasets.DatasetDict()\n",
        "\n",
        "        ds_clean[subset]['train'] = ds_embedded[subset]['train'].select(keep_train)\n",
        "        ds_clean[subset]['validation'] = ds_embedded[subset]['validation']\n",
        "        ds_clean[subset]['test'] = ds_embedded[subset]['test'].select(keep_test)\n",
        "\n",
        "    # Cleaning RL dataset.\n",
        "    print(f'Cleaning RL dataset')\n",
        "\n",
        "    # Set the format of RL dataset to 'torch' to enable torch operations on the embedded vectors.\n",
        "    ds_embedded['RL'].set_format('torch')\n",
        "\n",
        "    # Extract the embedded vectors for the RL dataset.\n",
        "    embed_vecs['RL'] = ds_embedded['RL']['qu_emb']\n",
        "\n",
        "    # Normalize the embedded vectors by dividing them by their L2 norm.\n",
        "    embed_vecs['RL'] /= torch.sqrt(torch.sum(embed_vecs['RL'] ** 2,\n",
        "                                             dim=1,\n",
        "                                             keepdim=True))\n",
        "    # Get the size of the RL dataset (number of examples).\n",
        "    RL_size = len(ds_embedded['RL'])\n",
        "\n",
        "    # Create an empty set to store the indices of redundant examples in the RL dataset.\n",
        "    rem_RL = set()\n",
        "\n",
        "    # Initialize a variable to keep track of the start index of each batch.\n",
        "    start = 0\n",
        "\n",
        "    # Calculate the number of batches based on the batch size.\n",
        "    num_batches = RL_size // batch_size\n",
        "\n",
        "    # If the size of RL dataset is not perfectly divisible by batch_size, add one extra batch.\n",
        "    if RL_size % batch_size != 0:\n",
        "        num_batches += 1\n",
        "\n",
        "    # Loop through each batch and compute overlaps with SFT and RM datasets to find redundant examples.\n",
        "    for k in tqdm(range(num_batches)):\n",
        "\n",
        "        # Calculate the start and end index of the current batch.\n",
        "        start = k * batch_size\n",
        "        end = (k + 1) * batch_size\n",
        "\n",
        "        # Get the current batch of embedded vectors.\n",
        "        batch = embed_vecs['RL'][start:start + batch_size, :]\n",
        "\n",
        "        # Compute overlaps between the current batch and the SFT and RM datasets.\n",
        "        for subset in ['SFT', 'RM']:\n",
        "            for split in ['train', 'validation']:\n",
        "                overlap = torch.matmul(embed_vecs[subset][split], batch.T)\n",
        "\n",
        "                # Find the indices of redundant examples in the current batch.\n",
        "                rem_RL_idxs_temp = torch.where(overlap >= cutoff)[1].numpy()\n",
        "\n",
        "                # Update the set of indices of redundant examples in the entire RL dataset.\n",
        "                rem_RL = rem_RL.union(set(rem_RL_idxs_temp))\n",
        "\n",
        "    # Create a set containing all the indices of the RL dataset.\n",
        "    keep_RL = set(range(RL_size))\n",
        "\n",
        "    # Remove the indices of redundant examples from the set to get non-redundant examples.\n",
        "    keep_RL -= set(rem_RL)\n",
        "\n",
        "    # Select non-redundant examples for RL dataset.\n",
        "    ds_clean['RL'] = ds_embedded['RL'].select(keep_RL)\n",
        "\n",
        "    # Apply 'make_pairs' function to the RM dataset to create pairs of answers.\n",
        "    ds_clean['RM'] = ds_clean['RM'].map(lambda x: make_pairs(x))\n",
        "\n",
        "    # Save the cleaned datasets to disk.\n",
        "    if save_file:\n",
        "        for subset in ['SFT', 'RM', 'RL']:\n",
        "            ds_clean[subset].save_to_disk(f'./data/{output_dir}/ds_{subset}')\n",
        "\n",
        "        # Log the cleaned datasets as a WandB artifact if log_to_wandb is True.\n",
        "        if log_to_wandb:\n",
        "            now = datetime.now()\n",
        "            time_stamp = now.strftime(\"%m.%d.%y-%H.%M.%S\")\n",
        "            with wandb.init(project='ELI5_analysis',\n",
        "                            entity='ft-llmmm',\n",
        "                            job_type='clean_data',\n",
        "                            name=f'clean_data_{time_stamp}') as run:\n",
        "\n",
        "                clean_data_art = wandb.Artifact('ELI5_cleaned', 'dataset')\n",
        "                clean_data_art.add_dir(f'./data/{output_dir}')\n",
        "                run.log_artifact(clean_data_art)\n",
        "\n",
        "    # Return the dictionary containing the cleaned datasets for SFT, RM, and RL.\n",
        "    return ds_clean\n"
      ],
      "metadata": {
        "id": "u28w0TgXUcE1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code"
      ],
      "metadata": {
        "id": "nPGvnBmK-l-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_original = load_dataset(\"vblagoje/lfqa\")"
      ],
      "metadata": {
        "id": "N_IUO6JoKKv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_filtered = preprocess_data(ds_original)"
      ],
      "metadata": {
        "id": "1fS-RqXhIYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_split = split_ds(ds_original,\n",
        "                    ds_filtered)"
      ],
      "metadata": {
        "id": "xwCBBzwYUlEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_embedded = embed_datasets(ds_split)"
      ],
      "metadata": {
        "id": "JOTZWH7Ihy1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_clean = clean_datasets(ds_embedded)"
      ],
      "metadata": {
        "id": "ZJTPzUg4DFFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}